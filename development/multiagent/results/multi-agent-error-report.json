{
  "status": "FAILED",
  "errors": [
    "è®°å¿†ç³»ç»ŸéªŒè¯å¤±è´¥ï¼šå¤ªå¤šæ–‡ä»¶ç¼ºå¤±æˆ–æ— æ•ˆ"
  ],
  "completedPhases": [],
  "failedPhase": 1,
  "results": [
    [
      "legacy-code-analyzer",
      {
        "success": true,
        "duration": 172058,
        "output": {
          "stdout": "## ğŸ”¥ ç«‹å³å¯æ‰§è¡Œçš„ä¿®å¤ä»£ç \n\n### 1ï¸âƒ£ **ç´§æ€¥æ—¥å¿—ä¿®å¤è„šæœ¬** (fix_logs_immediately.py)\n\n```python\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nç´§æ€¥æ—¥å¿—ä¿®å¤è„šæœ¬ - ç«‹å³æ‰§è¡Œé˜²æ­¢ç£ç›˜æ»¡\nä¿å­˜ä¸º: fix_logs_immediately.py\næ‰§è¡Œ: python fix_logs_immediately.py\n\"\"\"\n\nimport os\nimport shutil\nfrom pathlib import Path\nfrom datetime import datetime\n\ndef backup_and_rotate_logs():\n    \"\"\"å¤‡ä»½ç°æœ‰æ—¥å¿—å¹¶å¯ç”¨è½®è½¬\"\"\"\n    log_dir = Path(__file__).parent / 'logs'\n    archive_dir = log_dir / 'archive'\n    archive_dir.mkdir(exist_ok=True)\n    \n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    \n    # å¤‡ä»½å¤§æ—¥å¿—æ–‡ä»¶\n    for log_file in log_dir.glob('*.log'):\n        if log_file.stat().st_size > 1024*1024:  # å¤§äº1MB\n            backup_path = archive_dir / f\"{log_file.stem}_{timestamp}{log_file.suffix}\"\n            print(f\"å¤‡ä»½å¤§æ–‡ä»¶: {log_file.name} ({log_file.stat().st_size//1024}KB)\")\n            shutil.move(str(log_file), str(backup_path))\n            # åˆ›å»ºæ–°çš„ç©ºæ—¥å¿—æ–‡ä»¶\n            log_file.touch()\n    \n    print(f\"âœ… æ—¥å¿—å¤‡ä»½å®Œæˆ\")\n\ndef update_logger_with_rotation():\n    \"\"\"æ›´æ–°logger.pyæ·»åŠ è½®è½¬åŠŸèƒ½\"\"\"\n    logger_file = Path(__file__).parent / 'app' / 'utils' / 'logger.py'\n    \n    # åœ¨ç¬¬8è¡Œåæ’å…¥\n    insert_code = \"\"\"from logging.handlers import RotatingFileHandler\"\"\"\n    \n    # æ›¿æ¢ç¬¬38è¡Œçš„FileHandler\n    replace_from = \"file_handler = logging.FileHandler(log_path, encoding='utf-8')\"\n    replace_to = \"\"\"file_handler = RotatingFileHandler(\n        log_path, \n        maxBytes=10*1024*1024,  # 10MB\n        backupCount=5,\n        encoding='utf-8'\n    )\"\"\"\n    \n    if logger_file.exists():\n        content = logger_file.read_text(encoding='utf-8')\n        # æ·»åŠ import\n        if 'RotatingFileHandler' not in content:\n            lines = content.split('\\n')\n            lines.insert(5, insert_code)\n            content = '\\n'.join(lines)\n        # æ›¿æ¢FileHandler\n        content = content.replace(replace_from, replace_to)\n        logger_file.write_text(content, encoding='utf-8')\n        print(\"âœ… å·²æ›´æ–°logger.pyï¼Œå¯ç”¨æ—¥å¿—è½®è½¬\")\n\nif __name__ == '__main__':\n    print(\"ğŸš€ æ‰§è¡Œç´§æ€¥æ—¥å¿—ä¿®å¤...\")\n    backup_and_rotate_logs()\n    update_logger_with_rotation()\n    print(\"âœ… å®Œæˆ! é‡å¯ç³»ç»Ÿç”Ÿæ•ˆ\")\n```\n\n### 2ï¸âƒ£ **APIé™æµæ™ºèƒ½ä¿®å¤** (fix_api_rate_limit.py)\n\n```python\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nä¿®å¤APIé™æµé—®é¢˜ - æ™ºèƒ½è¯·æ±‚ç®¡ç†\nä¿å­˜ä¸º: fix_api_rate_limit.py\næ‰§è¡Œ: python fix_api_rate_limit.py\n\"\"\"\n\nimport time\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\ndef patch_publisher():\n    \"\"\"ä¿®è¡¥publisher.pyæ·»åŠ æ™ºèƒ½å»¶è¿Ÿ\"\"\"\n    publisher_file = Path(__file__).parent / 'app' / 'core' / 'publisher.py'\n    \n    # åœ¨post_tweet_with_videoå‡½æ•°å¼€å§‹å¤„æ·»åŠ \n    smart_delay_code = \"\"\"\n        # æ™ºèƒ½APIé™æµå»¶è¿Ÿ\n        import random\n        base_delay = 3  # åŸºç¡€å»¶è¿Ÿ3ç§’\n        jitter = random.uniform(0, 2)  # æ·»åŠ 0-2ç§’éšæœºæŠ–åŠ¨\n        smart_delay = base_delay + jitter\n        \n        # æ£€æŸ¥ä¸Šæ¬¡è¯·æ±‚æ—¶é—´(å­˜å‚¨åœ¨ç±»å±æ€§ä¸­)\n        if hasattr(self, '_last_request_time'):\n            elapsed = time.time() - self._last_request_time\n            if elapsed < smart_delay:\n                wait_time = smart_delay - elapsed\n                logger.info(f\"[RATE_LIMIT] æ™ºèƒ½å»¶è¿Ÿ {wait_time:.1f} ç§’é¿å…é™æµ\")\n                time.sleep(wait_time)\n        \n        self._last_request_time = time.time()\n\"\"\"\n    \n    if publisher_file.exists():\n        content = publisher_file.read_text(encoding='utf-8')\n        \n        # åœ¨å‡½æ•°å¼€å§‹å¤„æ’å…¥æ™ºèƒ½å»¶è¿Ÿ\n        insertion_point = 'def post_tweet_with_video(self, text: str, video_path: str)'\n        if insertion_point in content and '[RATE_LIMIT]' not in content:\n            lines = content.split('\\n')\n            for i, line in enumerate(lines):\n                if insertion_point in line:\n                    # æ‰¾åˆ°å‡½æ•°å®šä¹‰åçš„ç¬¬ä¸€è¡Œä»£ç ä½ç½®\n                    j = i + 1\n                    while j < len(lines) and (lines[j].strip().startswith('\"\"\"') or not lines[j].strip()):\n                        j += 1\n                    # æ’å…¥æ™ºèƒ½å»¶è¿Ÿä»£ç \n                    lines.insert(j, smart_delay_code)\n                    break\n            \n            content = '\\n'.join(lines)\n            publisher_file.write_text(content, encoding='utf-8')\n            print(\"âœ… å·²ä¿®è¡¥publisher.pyï¼Œæ·»åŠ æ™ºèƒ½APIå»¶è¿Ÿ\")\n    \n    # åŒæ—¶æ›´æ–°é…ç½®\n    update_config_for_rate_limit()\n\ndef update_config_for_rate_limit():\n    \"\"\"æ›´æ–°é…ç½®æ–‡ä»¶ä¼˜åŒ–é™æµå‚æ•°\"\"\"\n    config_file = Path(__file__).parent / 'config' / 'enhanced_config.yaml'\n    \n    if config_file.exists():\n        content = config_file.read_text(encoding='utf-8')\n        \n        # æ›´æ–°è°ƒåº¦å‚æ•°\n        updates = [\n            ('batch_size: 1', 'batch_size: 1  # ä¿æŒå•ä¸ªå‘å¸ƒé¿å…é™æµ'),\n            ('interval_hours: 4', 'interval_hours: 1  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡'),\n            ('min_publish_interval: 14400', 'min_publish_interval: 180  # æœ€å°3åˆ†é’Ÿé—´éš”'),\n        ]\n        \n        for old, new in updates:\n            if old in content:\n                content = content.replace(old, new)\n        \n        config_file.write_text(content, encoding='utf-8')\n        print(\"âœ… å·²ä¼˜åŒ–é…ç½®æ–‡ä»¶é™æµå‚æ•°\")\n\nif __name__ == '__main__':\n    print(\"ğŸ”§ ä¿®å¤APIé™æµé—®é¢˜...\")\n    patch_publisher()\n    print(\"âœ… å®Œæˆ! é‡å¯ç³»ç»Ÿç”Ÿæ•ˆ\")\n    print(\"\\nå»ºè®®:\")\n    print(\"1. æ¯æ¬¡å‘å¸ƒé—´éš”è‡³å°‘3åˆ†é’Ÿ\")\n    print(\"2. ä½¿ç”¨batch_size=1é¿å…å¹¶å‘\")\n    print(\"3. ç›‘æ§logs/main.logæŸ¥çœ‹æ•ˆæœ\")\n```\n\n### 3ï¸âƒ£ **ä¿®å¤å¡æ­»ä»»åŠ¡** (fix_stuck_tasks.py)\n\n```python\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nä¿®å¤å¡æ­»ä»»åŠ¡ - æ¸…ç†runningçŠ¶æ€\nä¿å­˜ä¸º: fix_stuck_tasks.py\næ‰§è¡Œ: python fix_stuck_tasks.py\n\"\"\"\n\nimport sqlite3\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\ndef fix_stuck_tasks():\n    \"\"\"ä¿®å¤æ‰€æœ‰å¡æ­»çš„ä»»åŠ¡\"\"\"\n    db_path = Path(__file__).parent / 'data' / 'enhanced_twitter_automation.db'\n    \n    if not db_path.exists():\n        # å°è¯•å…¶ä»–æ•°æ®åº“\n        db_path = Path(__file__).parent / 'data' / 'twitter_publisher.db'\n    \n    if not db_path.exists():\n        print(\"âŒ æ‰¾ä¸åˆ°æ•°æ®åº“æ–‡ä»¶\")\n        return\n    \n    conn = sqlite3.connect(str(db_path))\n    cursor = conn.cursor()\n    \n    # æŸ¥æ‰¾å¡æ­»çš„ä»»åŠ¡\n    stuck_threshold = datetime.now() - timedelta(hours=2)\n    stuck_time = stuck_threshold.strftime('%Y-%m-%d %H:%M:%S')\n    \n    # æŸ¥è¯¢å¡æ­»ä»»åŠ¡\n    cursor.execute(\"\"\"\n        SELECT id, status, updated_at \n        FROM publishing_tasks \n        WHERE status = 'running' \n        AND updated_at < ",
          "stderr": "",
          "exitCode": 0
        },
        "timestamp": "2025-08-16T20:59:48.762Z"
      }
    ]
  ],
  "timestamp": "2025-08-16T20:59:48.763Z",
  "diagnostics": {
    "platform": "darwin",
    "projectType": {
      "type": "Node.js",
      "indicator": "package.json"
    },
    "discoveredConfigs": 3,
    "nodeVersion": "v22.14.0"
  }
}