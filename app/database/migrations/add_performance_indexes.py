#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“æ€§èƒ½ç´¢å¼•ä¼˜åŒ–è¿ç§»
è§£å†³"ä»»åŠ¡æŸ¥è¯¢é¢‘ç¹è¿›è¡Œå…¨è¡¨æ‰«æ"çš„é‡ç‚¹é—®é¢˜

æ ¹æ®TWITTER_OPTIMIZATION_PLAN.mdç¬¬ä¸€é˜¶æ®µè¦æ±‚ï¼š
1. æ·»åŠ å¤åˆç´¢å¼•ï¼š(status, scheduled_at, priority) ç”¨äºä»»åŠ¡æŸ¥è¯¢
2. åˆ›å»ºé¡¹ç›®-çŠ¶æ€å¤åˆç´¢å¼•ï¼š(project_id, status)
3. ä¼˜åŒ–æ—¶é—´èŒƒå›´æŸ¥è¯¢ç´¢å¼•ï¼š(scheduled_at, status)
"""

import sqlite3
import os
from pathlib import Path
from datetime import datetime
from app.utils.logger import get_logger
from app.utils.enhanced_config import get_enhanced_config

logger = get_logger(__name__)

class PerformanceIndexMigration:
    """æ€§èƒ½ç´¢å¼•è¿ç§»ç±»"""
    
    def __init__(self):
        self.config = get_enhanced_config()
        self.db_path = self.config.get('database', {}).get('path', './data/twitter_publisher.db')
        
    def get_database_path(self):
        """è·å–æ•°æ®åº“è·¯å¾„"""
        # ç¡®ä¿æ•°æ®åº“è·¯å¾„å­˜åœ¨
        db_path = Path(self.db_path)
        db_path.parent.mkdir(parents=True, exist_ok=True)
        return str(db_path)
        
    def execute_migration(self):
        """æ‰§è¡Œç´¢å¼•è¿ç§»"""
        db_path = self.get_database_path()
        
        if not os.path.exists(db_path):
            logger.error(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
            return False
            
        try:
            with sqlite3.connect(db_path) as conn:
                cursor = conn.cursor()
                
                logger.info("å¼€å§‹åˆ›å»ºæ€§èƒ½ä¼˜åŒ–ç´¢å¼•...")
                
                # 1. æ ¸å¿ƒä»»åŠ¡æŸ¥è¯¢ç´¢å¼• - è§£å†³å…¨è¡¨æ‰«æé—®é¢˜
                logger.info("åˆ›å»ºæ ¸å¿ƒä»»åŠ¡æŸ¥è¯¢ç´¢å¼•: idx_tasks_status_scheduled_priority")
                cursor.execute("""
                    CREATE INDEX IF NOT EXISTS idx_tasks_status_scheduled_priority 
                    ON publishing_tasks(status, scheduled_at, priority);
                """)
                
                # 2. é¡¹ç›®-çŠ¶æ€å¤åˆç´¢å¼•
                logger.info("åˆ›å»ºé¡¹ç›®çŠ¶æ€ç´¢å¼•: idx_tasks_project_status")
                cursor.execute("""
                    CREATE INDEX IF NOT EXISTS idx_tasks_project_status 
                    ON publishing_tasks(project_id, status);
                """)
                
                # 3. æ—¶é—´èŒƒå›´æŸ¥è¯¢ç´¢å¼•
                logger.info("åˆ›å»ºæ—¶é—´èŒƒå›´æŸ¥è¯¢ç´¢å¼•: idx_tasks_scheduled_status")
                cursor.execute("""
                    CREATE INDEX IF NOT EXISTS idx_tasks_scheduled_status 
                    ON publishing_tasks(scheduled_at, status);
                """)
                
                # 4. æ—¥å¿—æŸ¥è¯¢ä¼˜åŒ–ç´¢å¼•
                logger.info("åˆ›å»ºæ—¥å¿—æŸ¥è¯¢ç´¢å¼•: idx_logs_task_published")
                cursor.execute("""
                    CREATE INDEX IF NOT EXISTS idx_logs_task_published 
                    ON publishing_logs(task_id, published_at);
                """)
                
                # 5. åˆ†æç»Ÿè®¡ç´¢å¼•
                logger.info("åˆ›å»ºåˆ†æç»Ÿè®¡ç´¢å¼•: idx_analytics_hour_project")
                cursor.execute("""
                    CREATE INDEX IF NOT EXISTS idx_analytics_hour_project 
                    ON analytics_hourly(hour_timestamp, project_id);
                """)
                
                conn.commit()
                logger.info("âœ… æ‰€æœ‰æ€§èƒ½ç´¢å¼•åˆ›å»ºå®Œæˆï¼")
                
                # éªŒè¯ç´¢å¼•åˆ›å»º
                self._verify_indexes(cursor)
                
                # åˆ†ææŸ¥è¯¢è®¡åˆ’
                self._analyze_query_plans(cursor)
                
                return True
                
        except Exception as e:
            logger.error(f"åˆ›å»ºç´¢å¼•å¤±è´¥: {e}", exc_info=True)
            return False
            
    def _verify_indexes(self, cursor):
        """éªŒè¯ç´¢å¼•æ˜¯å¦åˆ›å»ºæˆåŠŸ"""
        logger.info("éªŒè¯ç´¢å¼•åˆ›å»ºçŠ¶æ€...")
        
        cursor.execute("SELECT name FROM sqlite_master WHERE type='index' AND name LIKE 'idx_%';")
        indexes = cursor.fetchall()
        
        expected_indexes = [
            'idx_tasks_status_scheduled_priority',
            'idx_tasks_project_status', 
            'idx_tasks_scheduled_status',
            'idx_logs_task_published',
            'idx_analytics_hour_project'
        ]
        
        created_indexes = [idx[0] for idx in indexes]
        
        for expected_idx in expected_indexes:
            if expected_idx in created_indexes:
                logger.info(f"âœ… ç´¢å¼• {expected_idx} åˆ›å»ºæˆåŠŸ")
            else:
                logger.error(f"âŒ ç´¢å¼• {expected_idx} åˆ›å»ºå¤±è´¥")
                
    def _analyze_query_plans(self, cursor):
        """åˆ†æå…³é”®æŸ¥è¯¢çš„æ‰§è¡Œè®¡åˆ’ï¼ŒéªŒè¯æ˜¯å¦ä½¿ç”¨ç´¢å¼•"""
        logger.info("åˆ†æå…³é”®æŸ¥è¯¢æ‰§è¡Œè®¡åˆ’...")
        
        # æµ‹è¯•pendingä»»åŠ¡æŸ¥è¯¢ - è¿™æ˜¯æœ€é¢‘ç¹çš„æŸ¥è¯¢
        test_queries = [
            {
                'name': 'get_pending_tasks',
                'query': """
                    EXPLAIN QUERY PLAN 
                    SELECT * FROM publishing_tasks 
                    WHERE status IN ('pending', 'retry') 
                    ORDER BY priority DESC, scheduled_at ASC 
                    LIMIT 10;
                """
            },
            {
                'name': 'get_project_tasks',
                'query': """
                    EXPLAIN QUERY PLAN 
                    SELECT * FROM publishing_tasks 
                    WHERE project_id = 1 AND status = 'pending';
                """
            },
            {
                'name': 'get_tasks_by_time',
                'query': """
                    EXPLAIN QUERY PLAN 
                    SELECT * FROM publishing_tasks 
                    WHERE scheduled_at <= datetime('now') AND status = 'pending';
                """
            }
        ]
        
        for test in test_queries:
            logger.info(f"\nåˆ†ææŸ¥è¯¢: {test['name']}")
            try:
                cursor.execute(test['query'])
                plan = cursor.fetchall()
                
                uses_index = False
                for step in plan:
                    plan_detail = ' '.join(map(str, step))
                    logger.info(f"  {plan_detail}")
                    
                    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†ç´¢å¼•
                    if 'USING INDEX' in plan_detail.upper():
                        uses_index = True
                        
                if uses_index:
                    logger.info(f"âœ… æŸ¥è¯¢ {test['name']} æ­£åœ¨ä½¿ç”¨ç´¢å¼•")
                else:
                    logger.warning(f"âš ï¸ æŸ¥è¯¢ {test['name']} å¯èƒ½ä»åœ¨è¿›è¡Œè¡¨æ‰«æ")
                    
            except Exception as e:
                logger.error(f"åˆ†ææŸ¥è¯¢ {test['name']} å¤±è´¥: {e}")
                
    def rollback_migration(self):
        """å›æ»šè¿ç§» - åˆ é™¤åˆ›å»ºçš„ç´¢å¼•"""
        db_path = self.get_database_path()
        
        try:
            with sqlite3.connect(db_path) as conn:
                cursor = conn.cursor()
                
                logger.info("å¼€å§‹å›æ»šç´¢å¼•è¿ç§»...")
                
                indexes_to_drop = [
                    'idx_tasks_status_scheduled_priority',
                    'idx_tasks_project_status',
                    'idx_tasks_scheduled_status', 
                    'idx_logs_task_published',
                    'idx_analytics_hour_project'
                ]
                
                for index_name in indexes_to_drop:
                    try:
                        cursor.execute(f"DROP INDEX IF EXISTS {index_name};")
                        logger.info(f"åˆ é™¤ç´¢å¼•: {index_name}")
                    except Exception as e:
                        logger.error(f"åˆ é™¤ç´¢å¼• {index_name} å¤±è´¥: {e}")
                        
                conn.commit()
                logger.info("ç´¢å¼•å›æ»šå®Œæˆ")
                return True
                
        except Exception as e:
            logger.error(f"å›æ»šè¿ç§»å¤±è´¥: {e}")
            return False

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    migration = PerformanceIndexMigration()
    
    logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ•°æ®åº“æ€§èƒ½ç´¢å¼•ä¼˜åŒ–è¿ç§»...")
    logger.info("ç›®æ ‡: è§£å†³'ä»»åŠ¡æŸ¥è¯¢é¢‘ç¹è¿›è¡Œå…¨è¡¨æ‰«æ'é—®é¢˜")
    
    success = migration.execute_migration()
    
    if success:
        logger.info("ğŸ‰ æ•°æ®åº“æ€§èƒ½ç´¢å¼•ä¼˜åŒ–å®Œæˆï¼")
        logger.info("ğŸ“Š é¢„æœŸæ•ˆæœ:")
        logger.info("  - ä»»åŠ¡æŸ¥è¯¢æ€§èƒ½æå‡ 50-300%")
        logger.info("  - æ¶ˆé™¤å…¨è¡¨æ‰«æé—®é¢˜")
        logger.info("  - æ”¯æŒæ›´é«˜å¹¶å‘çš„ä»»åŠ¡å¤„ç†")
    else:
        logger.error("âŒ æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–å¤±è´¥")
        
    return success

if __name__ == "__main__":
    main()