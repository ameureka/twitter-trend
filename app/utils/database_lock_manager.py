#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”’ æ•°æ®åº“é”è¶…æ—¶å¤„ç†ç®¡ç†å™¨ - Phase 4.2
æ ¹æ®TWITTER_OPTIMIZATION_PLAN.mdå®ç°é«˜çº§æ•°æ®åº“é”ç®¡ç†æœºåˆ¶

ä¸»è¦åŠŸèƒ½:
1. æ™ºèƒ½é”æ£€æµ‹ä¸ç›‘æ§
2. æ­»é”é¢„é˜²ä¸è§£å†³
3. é”ç­‰å¾…é˜Ÿåˆ—ç®¡ç†
4. è‡ªé€‚åº”é‡è¯•ç­–ç•¥
5. è¿æ¥æ± ä¼˜åŒ–
6. é”ç»Ÿè®¡ä¸åˆ†æ
"""

import sqlite3
import threading
import time
import queue
import hashlib
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable, Tuple
from dataclasses import dataclass, field
from enum import Enum
from contextlib import contextmanager
from collections import defaultdict, deque
import random
import psutil

from app.utils.logger import get_logger

logger = get_logger(__name__)

class LockType(Enum):
    """é”ç±»å‹æšä¸¾"""
    SHARED = "shared"          # å…±äº«é”ï¼ˆè¯»é”ï¼‰
    EXCLUSIVE = "exclusive"    # æ’ä»–é”ï¼ˆå†™é”ï¼‰
    RESERVED = "reserved"      # ä¿ç•™é”
    PENDING = "pending"        # å¾…å®šé”
    DEFERRED = "deferred"      # å»¶è¿Ÿé”

class LockState(Enum):
    """é”çŠ¶æ€æšä¸¾"""
    WAITING = "waiting"        # ç­‰å¾…ä¸­
    ACQUIRED = "acquired"      # å·²è·å–
    TIMEOUT = "timeout"        # è¶…æ—¶
    DEADLOCK = "deadlock"      # æ­»é”
    RELEASED = "released"      # å·²é‡Šæ”¾
    FAILED = "failed"          # å¤±è´¥

@dataclass
class LockRequest:
    """é”è¯·æ±‚ä¿¡æ¯"""
    request_id: str
    connection_id: str
    lock_type: LockType
    table_name: Optional[str]
    request_time: datetime
    timeout_seconds: float
    priority: int = 1
    retry_count: int = 0
    max_retries: int = 3
    
    def is_expired(self) -> bool:
        """æ£€æŸ¥è¯·æ±‚æ˜¯å¦è¶…æ—¶"""
        elapsed = (datetime.now() - self.request_time).total_seconds()
        return elapsed > self.timeout_seconds
    
    def __hash__(self):
        return hash(self.request_id)

@dataclass
class LockStatistics:
    """é”ç»Ÿè®¡ä¿¡æ¯"""
    total_requests: int = 0
    successful_acquisitions: int = 0
    timeouts: int = 0
    deadlocks: int = 0
    retries: int = 0
    average_wait_time: float = 0.0
    peak_concurrent_locks: int = 0
    lock_type_distribution: Dict[str, int] = field(default_factory=dict)
    table_lock_distribution: Dict[str, int] = field(default_factory=dict)

@dataclass
class ConnectionInfo:
    """æ•°æ®åº“è¿æ¥ä¿¡æ¯"""
    connection_id: str
    connection: sqlite3.Connection
    created_at: datetime
    last_used: datetime
    in_use: bool = False
    transaction_active: bool = False
    lock_count: int = 0
    
    def is_idle(self, idle_timeout_seconds: int = 300) -> bool:
        """æ£€æŸ¥è¿æ¥æ˜¯å¦ç©ºé—²"""
        if self.in_use:
            return False
        idle_time = (datetime.now() - self.last_used).total_seconds()
        return idle_time > idle_timeout_seconds

class DatabaseLockManager:
    """ğŸ”’ é«˜çº§æ•°æ®åº“é”ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str, config: Optional[Dict[str, Any]] = None):
        self.db_path = db_path
        self.config = config or {}
        
        # é…ç½®å‚æ•°
        self.max_connections = self.config.get('max_connections', 10)
        self.min_connections = self.config.get('min_connections', 2)
        self.default_timeout = self.config.get('default_timeout', 30)
        self.max_wait_time = self.config.get('max_wait_time', 60)
        self.deadlock_check_interval = self.config.get('deadlock_check_interval', 5)
        self.connection_idle_timeout = self.config.get('connection_idle_timeout', 300)
        
        # é‡è¯•ç­–ç•¥å‚æ•°
        self.base_retry_delay = self.config.get('base_retry_delay', 0.1)
        self.max_retry_delay = self.config.get('max_retry_delay', 5.0)
        self.retry_jitter = self.config.get('retry_jitter', 0.2)
        
        # è¿æ¥æ± 
        self.connection_pool: Dict[str, ConnectionInfo] = {}
        self.available_connections: queue.Queue = queue.Queue()
        
        # é”ç®¡ç†
        self.active_locks: Dict[str, LockRequest] = {}
        self.waiting_queue: queue.PriorityQueue = queue.PriorityQueue()
        self.lock_wait_graph: Dict[str, List[str]] = defaultdict(list)  # ç”¨äºæ­»é”æ£€æµ‹
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.statistics = LockStatistics()
        self.lock_history: deque = deque(maxlen=1000)  # ä¿ç•™æœ€è¿‘1000æ¡é”è®°å½•
        
        # çº¿ç¨‹åŒæ­¥
        self.manager_lock = threading.RLock()
        self.condition = threading.Condition(self.manager_lock)
        
        # ç›‘æ§çº¿ç¨‹
        self.monitoring = False
        self.monitor_thread = None
        
        # åˆå§‹åŒ–è¿æ¥æ± 
        self._initialize_connection_pool()
        
        logger.info("ğŸ”’ æ•°æ®åº“é”ç®¡ç†å™¨å·²åˆå§‹åŒ–")
        logger.info(f"  - æ•°æ®åº“è·¯å¾„: {self.db_path}")
        logger.info(f"  - æœ€å¤§è¿æ¥æ•°: {self.max_connections}")
        logger.info(f"  - é»˜è®¤è¶…æ—¶: {self.default_timeout}ç§’")
    
    def _initialize_connection_pool(self):
        """åˆå§‹åŒ–è¿æ¥æ± """
        try:
            for i in range(self.min_connections):
                conn_info = self._create_connection()
                if conn_info:
                    self.connection_pool[conn_info.connection_id] = conn_info
                    self.available_connections.put(conn_info.connection_id)
                    
            logger.info(f"ğŸ”’ è¿æ¥æ± åˆå§‹åŒ–å®Œæˆï¼Œåˆ›å»ºäº† {self.min_connections} ä¸ªè¿æ¥")
            
        except Exception as e:
            logger.error(f"ğŸ”’ è¿æ¥æ± åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _create_connection(self) -> Optional[ConnectionInfo]:
        """åˆ›å»ºæ–°çš„æ•°æ®åº“è¿æ¥"""
        try:
            connection_id = self._generate_connection_id()
            
            # åˆ›å»ºè¿æ¥å¹¶è®¾ç½®ä¼˜åŒ–å‚æ•°
            conn = sqlite3.connect(self.db_path, timeout=self.default_timeout, check_same_thread=False)
            
            # è®¾ç½®SQLiteä¼˜åŒ–å‚æ•°
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA cache_size=10000")
            conn.execute("PRAGMA temp_store=MEMORY")
            conn.execute("PRAGMA mmap_size=268435456")  # 256MB
            conn.execute(f"PRAGMA busy_timeout={self.default_timeout * 1000}")  # æ¯«ç§’
            
            # å¯ç”¨å¤–é”®çº¦æŸ
            conn.execute("PRAGMA foreign_keys=ON")
            
            conn_info = ConnectionInfo(
                connection_id=connection_id,
                connection=conn,
                created_at=datetime.now(),
                last_used=datetime.now()
            )
            
            logger.debug(f"ğŸ”’ åˆ›å»ºæ–°è¿æ¥: {connection_id}")
            return conn_info
            
        except Exception as e:
            logger.error(f"ğŸ”’ åˆ›å»ºè¿æ¥å¤±è´¥: {e}")
            return None
    
    def _generate_connection_id(self) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„è¿æ¥ID"""
        timestamp = datetime.now().isoformat()
        random_str = str(random.random())
        return hashlib.md5(f"{timestamp}_{random_str}".encode()).hexdigest()[:12]
    
    @contextmanager
    def acquire_connection(self, timeout: Optional[float] = None) -> sqlite3.Connection:
        """
        è·å–æ•°æ®åº“è¿æ¥ï¼ˆä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼‰
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Yields:
            sqlite3.Connection: æ•°æ®åº“è¿æ¥
        """
        timeout = timeout or self.default_timeout
        connection_id = None
        
        try:
            # å°è¯•ä»æ± ä¸­è·å–è¿æ¥
            connection_id = self._get_connection_from_pool(timeout)
            
            if not connection_id:
                raise TimeoutError(f"æ— æ³•åœ¨ {timeout} ç§’å†…è·å–æ•°æ®åº“è¿æ¥")
                
            with self.manager_lock:
                conn_info = self.connection_pool[connection_id]
                conn_info.in_use = True
                conn_info.last_used = datetime.now()
                
            yield conn_info.connection
            
        finally:
            # é‡Šæ”¾è¿æ¥å›æ± 
            if connection_id:
                self._release_connection_to_pool(connection_id)
    
    def _get_connection_from_pool(self, timeout: float) -> Optional[str]:
        """ä»æ± ä¸­è·å–è¿æ¥"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            try:
                # å°è¯•è·å–å¯ç”¨è¿æ¥
                connection_id = self.available_connections.get(timeout=0.1)
                
                with self.manager_lock:
                    if connection_id in self.connection_pool:
                        conn_info = self.connection_pool[connection_id]
                        
                        # æ£€æŸ¥è¿æ¥æ˜¯å¦æœ‰æ•ˆ
                        try:
                            conn_info.connection.execute("SELECT 1")
                            return connection_id
                        except:
                            # è¿æ¥æ— æ•ˆï¼Œåˆ›å»ºæ–°è¿æ¥
                            logger.warning(f"ğŸ”’ è¿æ¥ {connection_id} æ— æ•ˆï¼Œåˆ›å»ºæ–°è¿æ¥")
                            self._remove_connection(connection_id)
                            new_conn = self._create_connection()
                            if new_conn:
                                self.connection_pool[new_conn.connection_id] = new_conn
                                return new_conn.connection_id
                                
            except queue.Empty:
                # æ£€æŸ¥æ˜¯å¦å¯ä»¥åˆ›å»ºæ–°è¿æ¥
                with self.manager_lock:
                    if len(self.connection_pool) < self.max_connections:
                        new_conn = self._create_connection()
                        if new_conn:
                            self.connection_pool[new_conn.connection_id] = new_conn
                            return new_conn.connection_id
                            
                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´å†é‡è¯•
                time.sleep(0.1)
                
        return None
    
    def _release_connection_to_pool(self, connection_id: str):
        """é‡Šæ”¾è¿æ¥å›æ± """
        with self.manager_lock:
            if connection_id in self.connection_pool:
                conn_info = self.connection_pool[connection_id]
                conn_info.in_use = False
                conn_info.last_used = datetime.now()
                
                # å›æ»šæœªæäº¤çš„äº‹åŠ¡
                if conn_info.transaction_active:
                    try:
                        conn_info.connection.rollback()
                        conn_info.transaction_active = False
                    except:
                        pass
                        
                self.available_connections.put(connection_id)
    
    def _remove_connection(self, connection_id: str):
        """ç§»é™¤è¿æ¥"""
        with self.manager_lock:
            if connection_id in self.connection_pool:
                conn_info = self.connection_pool[connection_id]
                try:
                    conn_info.connection.close()
                except:
                    pass
                del self.connection_pool[connection_id]
                logger.debug(f"ğŸ”’ ç§»é™¤è¿æ¥: {connection_id}")
    
    def execute_with_retry(self, func: Callable, *args, max_retries: int = 3, **kwargs) -> Any:
        """
        å¸¦é‡è¯•æœºåˆ¶æ‰§è¡Œæ•°æ®åº“æ“ä½œ
        
        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            *args, **kwargs: å‡½æ•°å‚æ•°
            
        Returns:
            å‡½æ•°æ‰§è¡Œç»“æœ
        """
        last_error = None
        
        for attempt in range(max_retries + 1):
            try:
                # è®¡ç®—é‡è¯•å»¶è¿Ÿ
                if attempt > 0:
                    delay = self._calculate_retry_delay(attempt)
                    logger.info(f"ğŸ”’ é‡è¯•ç¬¬ {attempt} æ¬¡ï¼Œå»¶è¿Ÿ {delay:.2f} ç§’")
                    time.sleep(delay)
                    self.statistics.retries += 1
                    
                # æ‰§è¡Œå‡½æ•°
                with self.acquire_connection() as conn:
                    result = func(conn, *args, **kwargs)
                    
                self.statistics.successful_acquisitions += 1
                return result
                
            except sqlite3.OperationalError as e:
                last_error = e
                error_msg = str(e).lower()
                
                if 'database is locked' in error_msg:
                    logger.warning(f"ğŸ”’ æ•°æ®åº“é”å®šï¼Œå°è¯• {attempt + 1}/{max_retries + 1}")
                    self.statistics.timeouts += 1
                    
                elif 'deadlock' in error_msg:
                    logger.error(f"ğŸ”’ æ£€æµ‹åˆ°æ­»é”: {e}")
                    self.statistics.deadlocks += 1
                    self._handle_deadlock()
                    
                else:
                    logger.error(f"ğŸ”’ æ•°æ®åº“æ“ä½œé”™è¯¯: {e}")
                    raise
                    
            except Exception as e:
                logger.error(f"ğŸ”’ æ‰§è¡Œå¤±è´¥: {e}")
                raise
                
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        logger.error(f"ğŸ”’ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥: {last_error}")
        raise last_error
    
    def _calculate_retry_delay(self, attempt: int) -> float:
        """è®¡ç®—é‡è¯•å»¶è¿Ÿï¼ˆæŒ‡æ•°é€€é¿ + æŠ–åŠ¨ï¼‰"""
        # æŒ‡æ•°é€€é¿
        delay = min(self.base_retry_delay * (2 ** (attempt - 1)), self.max_retry_delay)
        
        # æ·»åŠ éšæœºæŠ–åŠ¨
        jitter = delay * self.retry_jitter * (2 * random.random() - 1)
        final_delay = max(0, delay + jitter)
        
        return final_delay
    
    def _handle_deadlock(self):
        """å¤„ç†æ­»é”æƒ…å†µ"""
        logger.warning("ğŸ”’ å¼€å§‹æ­»é”å¤„ç†...")
        
        with self.manager_lock:
            # æ£€æµ‹æ­»é”ç¯
            deadlock_chains = self._detect_deadlock_cycles()
            
            if deadlock_chains:
                logger.warning(f"ğŸ”’ å‘ç° {len(deadlock_chains)} ä¸ªæ­»é”ç¯")
                
                # é€‰æ‹©ç‰ºç‰²è€…ï¼ˆä¼˜å…ˆçº§æœ€ä½çš„ï¼‰
                for chain in deadlock_chains:
                    victim = self._select_deadlock_victim(chain)
                    if victim:
                        logger.warning(f"ğŸ”’ é€‰æ‹©ç‰ºç‰²è€…: {victim}")
                        self._abort_lock_request(victim)
                        
            # æ¸…ç†ç­‰å¾…å›¾
            self.lock_wait_graph.clear()
    
    def _detect_deadlock_cycles(self) -> List[List[str]]:
        """æ£€æµ‹æ­»é”ç¯ï¼ˆä½¿ç”¨DFSï¼‰"""
        visited = set()
        rec_stack = set()
        cycles = []
        
        def dfs(node: str, path: List[str]) -> bool:
            visited.add(node)
            rec_stack.add(node)
            path.append(node)
            
            for neighbor in self.lock_wait_graph.get(node, []):
                if neighbor not in visited:
                    if dfs(neighbor, path.copy()):
                        return True
                elif neighbor in rec_stack:
                    # æ‰¾åˆ°ç¯
                    cycle_start = path.index(neighbor)
                    cycle = path[cycle_start:]
                    cycles.append(cycle)
                    return True
                    
            rec_stack.remove(node)
            return False
            
        for node in list(self.lock_wait_graph.keys()):
            if node not in visited:
                dfs(node, [])
                
        return cycles
    
    def _select_deadlock_victim(self, chain: List[str]) -> Optional[str]:
        """é€‰æ‹©æ­»é”ç‰ºç‰²è€…"""
        if not chain:
            return None
            
        # é€‰æ‹©ä¼˜å…ˆçº§æœ€ä½æˆ–ç­‰å¾…æ—¶é—´æœ€çŸ­çš„
        victim = None
        min_priority = float('inf')
        
        for request_id in chain:
            if request_id in self.active_locks:
                lock_request = self.active_locks[request_id]
                if lock_request.priority < min_priority:
                    min_priority = lock_request.priority
                    victim = request_id
                    
        return victim
    
    def _abort_lock_request(self, request_id: str):
        """ä¸­æ­¢é”è¯·æ±‚"""
        if request_id in self.active_locks:
            lock_request = self.active_locks[request_id]
            
            # é‡Šæ”¾ç›¸å…³èµ„æº
            if lock_request.connection_id in self.connection_pool:
                conn_info = self.connection_pool[lock_request.connection_id]
                try:
                    conn_info.connection.rollback()
                except:
                    pass
                    
            # ä»æ´»åŠ¨é”ä¸­ç§»é™¤
            del self.active_locks[request_id]
            
            # è®°å½•åˆ°å†å²
            self._record_lock_event(lock_request, LockState.DEADLOCK)
    
    def acquire_table_lock(self, table_name: str, lock_type: LockType = LockType.EXCLUSIVE,
                          timeout: Optional[float] = None, priority: int = 1) -> bool:
        """
        è·å–è¡¨çº§é”
        
        Args:
            table_name: è¡¨å
            lock_type: é”ç±»å‹
            timeout: è¶…æ—¶æ—¶é—´
            priority: ä¼˜å…ˆçº§
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸè·å–é”
        """
        timeout = timeout or self.default_timeout
        request_id = self._generate_request_id()
        
        lock_request = LockRequest(
            request_id=request_id,
            connection_id=threading.current_thread().ident,
            lock_type=lock_type,
            table_name=table_name,
            request_time=datetime.now(),
            timeout_seconds=timeout,
            priority=priority
        )
        
        with self.manager_lock:
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥ç«‹å³è·å–é”
            if self._can_acquire_lock(lock_request):
                self.active_locks[request_id] = lock_request
                self._record_lock_event(lock_request, LockState.ACQUIRED)
                self.statistics.successful_acquisitions += 1
                return True
                
            # åŠ å…¥ç­‰å¾…é˜Ÿåˆ—
            self.waiting_queue.put((priority, lock_request))
            self._update_wait_graph(lock_request)
            
        # ç­‰å¾…é”
        start_time = time.time()
        while time.time() - start_time < timeout:
            with self.condition:
                if request_id in self.active_locks:
                    return True
                    
                # ç­‰å¾…é€šçŸ¥
                self.condition.wait(timeout=0.5)
                
        # è¶…æ—¶
        self._record_lock_event(lock_request, LockState.TIMEOUT)
        self.statistics.timeouts += 1
        return False
    
    def _can_acquire_lock(self, request: LockRequest) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥è·å–é”"""
        if request.lock_type == LockType.SHARED:
            # å…±äº«é”ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ’ä»–é”
            for active_request in self.active_locks.values():
                if (active_request.table_name == request.table_name and
                    active_request.lock_type == LockType.EXCLUSIVE):
                    return False
        else:
            # æ’ä»–é”ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•é”
            for active_request in self.active_locks.values():
                if active_request.table_name == request.table_name:
                    return False
                    
        return True
    
    def _update_wait_graph(self, request: LockRequest):
        """æ›´æ–°ç­‰å¾…å›¾ï¼ˆç”¨äºæ­»é”æ£€æµ‹ï¼‰"""
        # æ‰¾å‡ºé˜»å¡å½“å‰è¯·æ±‚çš„é”
        blocking_requests = []
        
        for active_request in self.active_locks.values():
            if active_request.table_name == request.table_name:
                if (request.lock_type == LockType.EXCLUSIVE or
                    active_request.lock_type == LockType.EXCLUSIVE):
                    blocking_requests.append(active_request.request_id)
                    
        # æ›´æ–°ç­‰å¾…å›¾
        if blocking_requests:
            self.lock_wait_graph[request.request_id] = blocking_requests
    
    def release_table_lock(self, request_id: str):
        """é‡Šæ”¾è¡¨çº§é”"""
        with self.manager_lock:
            if request_id in self.active_locks:
                lock_request = self.active_locks[request_id]
                del self.active_locks[request_id]
                
                self._record_lock_event(lock_request, LockState.RELEASED)
                
                # é€šçŸ¥ç­‰å¾…çº¿ç¨‹
                self.condition.notify_all()
                
                # å¤„ç†ç­‰å¾…é˜Ÿåˆ—
                self._process_waiting_queue()
    
    def _process_waiting_queue(self):
        """å¤„ç†ç­‰å¾…é˜Ÿåˆ—"""
        processed = []
        
        while not self.waiting_queue.empty():
            priority, lock_request = self.waiting_queue.get()
            
            if self._can_acquire_lock(lock_request):
                self.active_locks[lock_request.request_id] = lock_request
                self._record_lock_event(lock_request, LockState.ACQUIRED)
                self.condition.notify_all()
            else:
                processed.append((priority, lock_request))
                
        # å°†æœªå¤„ç†çš„è¯·æ±‚æ”¾å›é˜Ÿåˆ—
        for item in processed:
            self.waiting_queue.put(item)
    
    def _generate_request_id(self) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„è¯·æ±‚ID"""
        timestamp = datetime.now().isoformat()
        thread_id = threading.current_thread().ident
        random_str = str(random.random())
        return hashlib.md5(f"{timestamp}_{thread_id}_{random_str}".encode()).hexdigest()[:16]
    
    def _record_lock_event(self, request: LockRequest, state: LockState):
        """è®°å½•é”äº‹ä»¶"""
        event = {
            'request_id': request.request_id,
            'table_name': request.table_name,
            'lock_type': request.lock_type.value,
            'state': state.value,
            'timestamp': datetime.now().isoformat(),
            'wait_time': (datetime.now() - request.request_time).total_seconds()
        }
        
        self.lock_history.append(event)
        
        # æ›´æ–°ç»Ÿè®¡
        self.statistics.total_requests += 1
        
        if request.lock_type.value not in self.statistics.lock_type_distribution:
            self.statistics.lock_type_distribution[request.lock_type.value] = 0
        self.statistics.lock_type_distribution[request.lock_type.value] += 1
        
        if request.table_name:
            if request.table_name not in self.statistics.table_lock_distribution:
                self.statistics.table_lock_distribution[request.table_name] = 0
            self.statistics.table_lock_distribution[request.table_name] += 1
            
        # æ›´æ–°å¹³å‡ç­‰å¾…æ—¶é—´
        if state == LockState.ACQUIRED:
            wait_time = (datetime.now() - request.request_time).total_seconds()
            current_avg = self.statistics.average_wait_time
            current_count = self.statistics.successful_acquisitions
            new_avg = (current_avg * current_count + wait_time) / (current_count + 1)
            self.statistics.average_wait_time = new_avg
            
        # æ›´æ–°å³°å€¼å¹¶å‘é”
        current_locks = len(self.active_locks)
        if current_locks > self.statistics.peak_concurrent_locks:
            self.statistics.peak_concurrent_locks = current_locks
    
    def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§çº¿ç¨‹"""
        if self.monitoring:
            return
            
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        
        logger.info("ğŸ”’ æ•°æ®åº“é”ç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
            
        logger.info("ğŸ”’ æ•°æ®åº“é”ç›‘æ§å·²åœæ­¢")
    
    def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        logger.info("ğŸ”’ æ•°æ®åº“é”ç›‘æ§å¾ªç¯å¯åŠ¨")
        
        while self.monitoring:
            try:
                # æ£€æµ‹æ­»é”
                with self.manager_lock:
                    if self.lock_wait_graph:
                        deadlock_chains = self._detect_deadlock_cycles()
                        if deadlock_chains:
                            logger.warning(f"ğŸ”’ æ£€æµ‹åˆ° {len(deadlock_chains)} ä¸ªæ½œåœ¨æ­»é”")
                            self._handle_deadlock()
                            
                # æ¸…ç†ç©ºé—²è¿æ¥
                self._cleanup_idle_connections()
                
                # å¤„ç†è¶…æ—¶è¯·æ±‚
                self._handle_timeout_requests()
                
                # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                if self.statistics.total_requests > 0 and self.statistics.total_requests % 100 == 0:
                    self._log_statistics()
                    
                time.sleep(self.deadlock_check_interval)
                
            except Exception as e:
                logger.error(f"ğŸ”’ ç›‘æ§å¾ªç¯å¼‚å¸¸: {e}")
                time.sleep(1)
                
        logger.info("ğŸ”’ æ•°æ®åº“é”ç›‘æ§å¾ªç¯ç»“æŸ")
    
    def _cleanup_idle_connections(self):
        """æ¸…ç†ç©ºé—²è¿æ¥"""
        with self.manager_lock:
            idle_connections = []
            
            for conn_id, conn_info in self.connection_pool.items():
                if conn_info.is_idle(self.connection_idle_timeout):
                    idle_connections.append(conn_id)
                    
            # ä¿ç•™æœ€å°è¿æ¥æ•°
            removable_count = len(self.connection_pool) - self.min_connections
            
            for conn_id in idle_connections[:removable_count]:
                self._remove_connection(conn_id)
                logger.debug(f"ğŸ”’ æ¸…ç†ç©ºé—²è¿æ¥: {conn_id}")
    
    def _handle_timeout_requests(self):
        """å¤„ç†è¶…æ—¶çš„é”è¯·æ±‚"""
        with self.manager_lock:
            timeout_requests = []
            
            # æ£€æŸ¥ç­‰å¾…é˜Ÿåˆ—ä¸­çš„è¶…æ—¶è¯·æ±‚
            temp_queue = []
            
            while not self.waiting_queue.empty():
                priority, request = self.waiting_queue.get()
                if request.is_expired():
                    timeout_requests.append(request)
                    self._record_lock_event(request, LockState.TIMEOUT)
                else:
                    temp_queue.append((priority, request))
                    
            # é‡æ–°å¡«å……é˜Ÿåˆ—
            for item in temp_queue:
                self.waiting_queue.put(item)
                
            if timeout_requests:
                logger.warning(f"ğŸ”’ å¤„ç†äº† {len(timeout_requests)} ä¸ªè¶…æ—¶è¯·æ±‚")
    
    def _log_statistics(self):
        """è®°å½•ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("ğŸ”’ æ•°æ®åº“é”ç»Ÿè®¡:")
        logger.info(f"  - æ€»è¯·æ±‚æ•°: {self.statistics.total_requests}")
        logger.info(f"  - æˆåŠŸè·å–: {self.statistics.successful_acquisitions}")
        logger.info(f"  - è¶…æ—¶: {self.statistics.timeouts}")
        logger.info(f"  - æ­»é”: {self.statistics.deadlocks}")
        logger.info(f"  - é‡è¯•: {self.statistics.retries}")
        logger.info(f"  - å¹³å‡ç­‰å¾…æ—¶é—´: {self.statistics.average_wait_time:.2f}ç§’")
        logger.info(f"  - å³°å€¼å¹¶å‘é”: {self.statistics.peak_concurrent_locks}")
        logger.info(f"  - å½“å‰æ´»åŠ¨é”: {len(self.active_locks)}")
        logger.info(f"  - è¿æ¥æ± å¤§å°: {len(self.connection_pool)}")
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.manager_lock:
            return {
                'total_requests': self.statistics.total_requests,
                'successful_acquisitions': self.statistics.successful_acquisitions,
                'success_rate': self.statistics.successful_acquisitions / self.statistics.total_requests 
                                if self.statistics.total_requests > 0 else 0,
                'timeouts': self.statistics.timeouts,
                'deadlocks': self.statistics.deadlocks,
                'retries': self.statistics.retries,
                'average_wait_time': self.statistics.average_wait_time,
                'peak_concurrent_locks': self.statistics.peak_concurrent_locks,
                'current_active_locks': len(self.active_locks),
                'waiting_requests': self.waiting_queue.qsize(),
                'connection_pool_size': len(self.connection_pool),
                'lock_type_distribution': dict(self.statistics.lock_type_distribution),
                'table_lock_distribution': dict(self.statistics.table_lock_distribution)
            }
    
    def optimize_for_performance(self):
        """ä¼˜åŒ–æ•°æ®åº“æ€§èƒ½è®¾ç½®"""
        try:
            with self.acquire_connection() as conn:
                # åˆ†ææ•°æ®åº“
                conn.execute("ANALYZE")
                
                # é‡å»ºç´¢å¼•
                conn.execute("REINDEX")
                
                # æ¸…ç†æ•°æ®åº“
                conn.execute("VACUUM")
                
                logger.info("ğŸ”’ æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–å®Œæˆ")
                
        except Exception as e:
            logger.error(f"ğŸ”’ æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {e}")
    
    def get_system_metrics(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸæŒ‡æ ‡"""
        try:
            # è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            # è·å–SQLiteç‰¹å®šæŒ‡æ ‡
            sqlite_metrics = {}
            
            with self.acquire_connection() as conn:
                # é¡µé¢ç¼“å­˜ç»Ÿè®¡
                result = conn.execute("PRAGMA page_count").fetchone()
                sqlite_metrics['page_count'] = result[0] if result else 0
                
                result = conn.execute("PRAGMA page_size").fetchone()
                sqlite_metrics['page_size'] = result[0] if result else 0
                
                result = conn.execute("PRAGMA cache_size").fetchone()
                sqlite_metrics['cache_size'] = result[0] if result else 0
                
                # WALæ¨¡å¼æ£€æŸ¥
                result = conn.execute("PRAGMA journal_mode").fetchone()
                sqlite_metrics['journal_mode'] = result[0] if result else 'unknown'
                
            return {
                'system': {
                    'cpu_percent': cpu_percent,
                    'memory_percent': memory.percent,
                    'memory_available_mb': memory.available / (1024 * 1024),
                    'disk_percent': disk.percent,
                    'disk_free_gb': disk.free / (1024 * 1024 * 1024)
                },
                'sqlite': sqlite_metrics,
                'lock_manager': self.get_statistics()
            }
            
        except Exception as e:
            logger.error(f"ğŸ”’ è·å–ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {e}")
            return {}

# å…¨å±€å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_database_lock_manager: Optional[DatabaseLockManager] = None

def get_database_lock_manager(db_path: str = "./data/twitter_publisher.db") -> DatabaseLockManager:
    """è·å–æ•°æ®åº“é”ç®¡ç†å™¨å®ä¾‹"""
    global _database_lock_manager
    
    if _database_lock_manager is None:
        _database_lock_manager = DatabaseLockManager(db_path)
        _database_lock_manager.start_monitoring()
        
    return _database_lock_manager

def execute_with_lock_protection(func: Callable, *args, **kwargs) -> Any:
    """
    ä¾¿æ·å‡½æ•°ï¼šä½¿ç”¨é”ä¿æŠ¤æ‰§è¡Œæ•°æ®åº“æ“ä½œ
    
    Args:
        func: è¦æ‰§è¡Œçš„å‡½æ•°
        *args, **kwargs: å‡½æ•°å‚æ•°
        
    Returns:
        å‡½æ•°æ‰§è¡Œç»“æœ
    """
    manager = get_database_lock_manager()
    return manager.execute_with_retry(func, *args, **kwargs)