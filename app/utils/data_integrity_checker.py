#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ” æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å™¨ - Phase 4.3
æ ¹æ®TWITTER_OPTIMIZATION_PLAN.mdå®ç°å…¨é¢çš„æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å’Œè‡ªåŠ¨ä¿®å¤æœºåˆ¶

ä¸»è¦åŠŸèƒ½:
1. æ•°æ®ä¸€è‡´æ€§éªŒè¯
2. å¤–é”®çº¦æŸæ£€æŸ¥
3. å­¤ç«‹è®°å½•æ£€æµ‹
4. æ•°æ®å†—ä½™è¯†åˆ«
5. è‡ªåŠ¨ä¿®å¤æœºåˆ¶
6. æ•°æ®å¤‡ä»½ä¸æ¢å¤
7. å¢é‡å®Œæ•´æ€§æ£€æŸ¥
8. å®æ—¶ç›‘æ§ä¸é¢„è­¦
"""

import sqlite3
import hashlib
import json
import os
import shutil
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Set, Callable
from dataclasses import dataclass, field
from enum import Enum
from collections import defaultdict
import threading
import time
import re

from app.utils.logger import get_logger

logger = get_logger(__name__)

class IntegrityIssueType(Enum):
    """å®Œæ•´æ€§é—®é¢˜ç±»å‹"""
    ORPHAN_RECORD = "orphan_record"              # å­¤ç«‹è®°å½•
    MISSING_REFERENCE = "missing_reference"      # ç¼ºå¤±å¼•ç”¨
    DUPLICATE_DATA = "duplicate_data"            # é‡å¤æ•°æ®
    INCONSISTENT_STATE = "inconsistent_state"    # çŠ¶æ€ä¸ä¸€è‡´
    INVALID_DATA = "invalid_data"                # æ— æ•ˆæ•°æ®
    CORRUPTED_DATA = "corrupted_data"            # æŸåæ•°æ®
    CONSTRAINT_VIOLATION = "constraint_violation" # çº¦æŸè¿å
    SCHEMA_MISMATCH = "schema_mismatch"          # æ¨¡å¼ä¸åŒ¹é…
    SEQUENCE_GAP = "sequence_gap"                # åºåˆ—é—´éš™
    TIMESTAMP_ANOMALY = "timestamp_anomaly"      # æ—¶é—´æˆ³å¼‚å¸¸

class RepairStrategy(Enum):
    """ä¿®å¤ç­–ç•¥"""
    AUTO_FIX = "auto_fix"                    # è‡ªåŠ¨ä¿®å¤
    MANUAL_REVIEW = "manual_review"          # äººå·¥å®¡æŸ¥
    BACKUP_RESTORE = "backup_restore"        # å¤‡ä»½æ¢å¤
    CASCADE_DELETE = "cascade_delete"        # çº§è”åˆ é™¤
    DEFAULT_VALUE = "default_value"          # é»˜è®¤å€¼å¡«å……
    RECALCULATE = "recalculate"             # é‡æ–°è®¡ç®—
    IGNORE = "ignore"                        # å¿½ç•¥

@dataclass
class IntegrityIssue:
    """å®Œæ•´æ€§é—®é¢˜è®°å½•"""
    issue_id: str
    issue_type: IntegrityIssueType
    table_name: str
    record_id: Optional[Any]
    column_name: Optional[str]
    description: str
    severity: int  # 1-5, 5æœ€ä¸¥é‡
    detected_at: datetime
    repair_strategy: RepairStrategy
    repair_attempted: bool = False
    repair_successful: bool = False
    repair_error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class TableSchema:
    """è¡¨ç»“æ„ä¿¡æ¯"""
    table_name: str
    columns: List[Dict[str, Any]]
    primary_keys: List[str]
    foreign_keys: List[Dict[str, Any]]
    indexes: List[Dict[str, Any]]
    constraints: List[Dict[str, Any]]
    row_count: int
    last_check: datetime

@dataclass
class IntegrityReport:
    """å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š"""
    check_id: str
    check_time: datetime
    duration_seconds: float
    tables_checked: int
    records_checked: int
    issues_found: List[IntegrityIssue]
    auto_fixed: int
    manual_required: int
    critical_issues: int
    recommendations: List[str]

class DataIntegrityChecker:
    """ğŸ” é«˜çº§æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å™¨"""
    
    def __init__(self, db_path: str, config: Optional[Dict[str, Any]] = None):
        self.db_path = db_path
        self.config = config or {}
        
        # é…ç½®å‚æ•°
        self.check_interval = self.config.get('check_interval', 3600)  # 1å°æ—¶
        self.auto_repair = self.config.get('auto_repair', True)
        self.backup_before_repair = self.config.get('backup_before_repair', True)
        self.max_repair_attempts = self.config.get('max_repair_attempts', 3)
        self.critical_severity_threshold = self.config.get('critical_severity_threshold', 4)
        
        # è¡¨ç»“æ„ç¼“å­˜
        self.schema_cache: Dict[str, TableSchema] = {}
        self.schema_cache_ttl = timedelta(hours=1)
        
        # é—®é¢˜è¿½è¸ª
        self.active_issues: Dict[str, IntegrityIssue] = {}
        self.issue_history: List[IntegrityIssue] = []
        self.repair_history: List[Dict[str, Any]] = []
        
        # æ£€æŸ¥è§„åˆ™
        self.integrity_rules = self._initialize_integrity_rules()
        
        # ä¿®å¤ç­–ç•¥æ˜ å°„
        self.repair_strategies = self._initialize_repair_strategies()
        
        # ç›‘æ§çŠ¶æ€
        self.monitoring = False
        self.monitor_thread = None
        self.last_check_time = None
        self.lock = threading.RLock()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.statistics = {
            'total_checks': 0,
            'total_issues': 0,
            'auto_fixed': 0,
            'manual_required': 0,
            'failed_repairs': 0,
            'check_duration_avg': 0.0
        }
        
        logger.info("ğŸ” æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å™¨å·²åˆå§‹åŒ–")
        logger.info(f"  - æ•°æ®åº“è·¯å¾„: {self.db_path}")
        logger.info(f"  - è‡ªåŠ¨ä¿®å¤: {self.auto_repair}")
        logger.info(f"  - æ£€æŸ¥é—´éš”: {self.check_interval}ç§’")
    
    def _initialize_integrity_rules(self) -> Dict[str, List[Callable]]:
        """åˆå§‹åŒ–å®Œæ•´æ€§æ£€æŸ¥è§„åˆ™"""
        return {
            'publishing_tasks': [
                self._check_task_references,
                self._check_task_states,
                self._check_task_timestamps,
                self._check_task_duplicates
            ],
            'publishing_logs': [
                self._check_log_references,
                self._check_log_sequence,
                self._check_log_timestamps
            ],
            'projects': [
                self._check_project_references,
                self._check_project_constraints
            ],
            'content_sources': [
                self._check_source_references,
                self._check_source_paths
            ],
            'api_keys': [
                self._check_api_key_validity,
                self._check_api_key_expiration
            ],
            'analytics_hourly': [
                self._check_analytics_gaps,
                self._check_analytics_consistency
            ]
        }
    
    def _initialize_repair_strategies(self) -> Dict[IntegrityIssueType, Callable]:
        """åˆå§‹åŒ–ä¿®å¤ç­–ç•¥"""
        return {
            IntegrityIssueType.ORPHAN_RECORD: self._repair_orphan_record,
            IntegrityIssueType.MISSING_REFERENCE: self._repair_missing_reference,
            IntegrityIssueType.DUPLICATE_DATA: self._repair_duplicate_data,
            IntegrityIssueType.INCONSISTENT_STATE: self._repair_inconsistent_state,
            IntegrityIssueType.INVALID_DATA: self._repair_invalid_data,
            IntegrityIssueType.CORRUPTED_DATA: self._repair_corrupted_data,
            IntegrityIssueType.CONSTRAINT_VIOLATION: self._repair_constraint_violation,
            IntegrityIssueType.SCHEMA_MISMATCH: self._repair_schema_mismatch,
            IntegrityIssueType.SEQUENCE_GAP: self._repair_sequence_gap,
            IntegrityIssueType.TIMESTAMP_ANOMALY: self._repair_timestamp_anomaly
        }
    
    def perform_full_check(self) -> IntegrityReport:
        """
        æ‰§è¡Œå…¨é¢çš„å®Œæ•´æ€§æ£€æŸ¥
        
        Returns:
            IntegrityReport: æ£€æŸ¥æŠ¥å‘Š
        """
        logger.info("ğŸ” å¼€å§‹æ‰§è¡Œå…¨é¢æ•°æ®å®Œæ•´æ€§æ£€æŸ¥...")
        
        check_id = self._generate_check_id()
        start_time = datetime.now()
        issues_found = []
        tables_checked = 0
        records_checked = 0
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.row_factory = sqlite3.Row
                
                # åˆ·æ–°è¡¨ç»“æ„ç¼“å­˜
                self._refresh_schema_cache(conn)
                
                # å¯¹æ¯ä¸ªè¡¨æ‰§è¡Œæ£€æŸ¥
                for table_name, schema in self.schema_cache.items():
                    logger.info(f"ğŸ” æ£€æŸ¥è¡¨: {table_name}")
                    tables_checked += 1
                    
                    # è·å–è¡¨è®°å½•æ•°
                    cursor = conn.execute(f"SELECT COUNT(*) FROM {table_name}")
                    row_count = cursor.fetchone()[0]
                    records_checked += row_count
                    
                    # æ‰§è¡Œè¡¨ç‰¹å®šçš„æ£€æŸ¥è§„åˆ™
                    if table_name in self.integrity_rules:
                        for rule_func in self.integrity_rules[table_name]:
                            try:
                                table_issues = rule_func(conn, table_name, schema)
                                issues_found.extend(table_issues)
                            except Exception as e:
                                logger.error(f"ğŸ” è§„åˆ™æ£€æŸ¥å¤±è´¥ {rule_func.__name__}: {e}")
                    
                    # æ‰§è¡Œé€šç”¨æ£€æŸ¥
                    generic_issues = self._perform_generic_checks(conn, table_name, schema)
                    issues_found.extend(generic_issues)
                
                # æ‰§è¡Œè·¨è¡¨æ£€æŸ¥
                cross_table_issues = self._perform_cross_table_checks(conn)
                issues_found.extend(cross_table_issues)
                
            # è®°å½•é—®é¢˜
            with self.lock:
                for issue in issues_found:
                    self.active_issues[issue.issue_id] = issue
                    self.issue_history.append(issue)
            
            # è‡ªåŠ¨ä¿®å¤
            auto_fixed = 0
            manual_required = 0
            critical_issues = 0
            
            if self.auto_repair and issues_found:
                logger.info(f"ğŸ” å‘ç° {len(issues_found)} ä¸ªé—®é¢˜ï¼Œå¼€å§‹è‡ªåŠ¨ä¿®å¤...")
                
                for issue in issues_found:
                    if issue.severity >= self.critical_severity_threshold:
                        critical_issues += 1
                    
                    if issue.repair_strategy == RepairStrategy.AUTO_FIX:
                        if self._attempt_auto_repair(issue):
                            auto_fixed += 1
                        else:
                            manual_required += 1
                    else:
                        manual_required += 1
            
            # ç”Ÿæˆå»ºè®®
            recommendations = self._generate_recommendations(issues_found)
            
            # åˆ›å»ºæŠ¥å‘Š
            duration = (datetime.now() - start_time).total_seconds()
            report = IntegrityReport(
                check_id=check_id,
                check_time=start_time,
                duration_seconds=duration,
                tables_checked=tables_checked,
                records_checked=records_checked,
                issues_found=issues_found,
                auto_fixed=auto_fixed,
                manual_required=manual_required,
                critical_issues=critical_issues,
                recommendations=recommendations
            )
            
            # æ›´æ–°ç»Ÿè®¡
            self._update_statistics(report)
            
            # è®°å½•ç»“æœ
            self._log_report(report)
            
            self.last_check_time = datetime.now()
            
            return report
            
        except Exception as e:
            logger.error(f"ğŸ” å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}")
            raise
    
    def _refresh_schema_cache(self, conn: sqlite3.Connection):
        """åˆ·æ–°è¡¨ç»“æ„ç¼“å­˜"""
        cursor = conn.cursor()
        
        # è·å–æ‰€æœ‰è¡¨
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = cursor.fetchall()
        
        for table_row in tables:
            table_name = table_row[0]
            
            if table_name.startswith('sqlite_'):
                continue
            
            # è·å–è¡¨ä¿¡æ¯
            cursor.execute(f"PRAGMA table_info({table_name})")
            columns = [dict(row) for row in cursor.fetchall()]
            
            # è·å–ä¸»é”®
            primary_keys = [col['name'] for col in columns if col['pk'] > 0]
            
            # è·å–å¤–é”®
            cursor.execute(f"PRAGMA foreign_key_list({table_name})")
            foreign_keys = [dict(row) for row in cursor.fetchall()]
            
            # è·å–ç´¢å¼•
            cursor.execute(f"PRAGMA index_list({table_name})")
            indexes = [dict(row) for row in cursor.fetchall()]
            
            # è·å–è¡Œæ•°
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            row_count = cursor.fetchone()[0]
            
            # åˆ›å»ºè¡¨ç»“æ„å¯¹è±¡
            schema = TableSchema(
                table_name=table_name,
                columns=columns,
                primary_keys=primary_keys,
                foreign_keys=foreign_keys,
                indexes=indexes,
                constraints=[],  # SQLiteä¸ç›´æ¥æä¾›çº¦æŸä¿¡æ¯
                row_count=row_count,
                last_check=datetime.now()
            )
            
            self.schema_cache[table_name] = schema
    
    def _check_task_references(self, conn: sqlite3.Connection, table_name: str, 
                              schema: TableSchema) -> List[IntegrityIssue]:
        """æ£€æŸ¥ä»»åŠ¡è¡¨å¼•ç”¨å®Œæ•´æ€§"""
        issues = []
        
        # æ£€æŸ¥project_idå¼•ç”¨
        cursor = conn.execute("""
            SELECT t.id, t.project_id 
            FROM publishing_tasks t
            LEFT JOIN projects p ON t.project_id = p.id
            WHERE p.id IS NULL AND t.project_id IS NOT NULL
        """)
        
        for row in cursor:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.MISSING_REFERENCE,
                table_name=table_name,
                record_id=row[0],
                column_name='project_id',
                description=f"ä»»åŠ¡ {row[0]} å¼•ç”¨äº†ä¸å­˜åœ¨çš„é¡¹ç›® {row[1]}",
                severity=4,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.CASCADE_DELETE,
                metadata={'task_id': row[0], 'project_id': row[1]}
            )
            issues.append(issue)
        
        # æ£€æŸ¥source_idå¼•ç”¨
        cursor = conn.execute("""
            SELECT t.id, t.source_id 
            FROM publishing_tasks t
            LEFT JOIN content_sources s ON t.source_id = s.id
            WHERE s.id IS NULL AND t.source_id IS NOT NULL
        """)
        
        for row in cursor:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.MISSING_REFERENCE,
                table_name=table_name,
                record_id=row[0],
                column_name='source_id',
                description=f"ä»»åŠ¡ {row[0]} å¼•ç”¨äº†ä¸å­˜åœ¨çš„å†…å®¹æº {row[1]}",
                severity=4,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.CASCADE_DELETE,
                metadata={'task_id': row[0], 'source_id': row[1]}
            )
            issues.append(issue)
        
        return issues
    
    def _check_task_states(self, conn: sqlite3.Connection, table_name: str,
                          schema: TableSchema) -> List[IntegrityIssue]:
        """æ£€æŸ¥ä»»åŠ¡çŠ¶æ€ä¸€è‡´æ€§"""
        issues = []
        
        # æ£€æŸ¥é•¿æ—¶é—´å¤„äºè¿è¡ŒçŠ¶æ€çš„ä»»åŠ¡
        cursor = conn.execute("""
            SELECT id, status, updated_at
            FROM publishing_tasks
            WHERE status IN ('running', 'processing')
            AND datetime(updated_at) < datetime('now', '-1 hour')
        """)
        
        for row in cursor:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.INCONSISTENT_STATE,
                table_name=table_name,
                record_id=row[0],
                column_name='status',
                description=f"ä»»åŠ¡ {row[0]} é•¿æ—¶é—´å¤„äº {row[1]} çŠ¶æ€",
                severity=3,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.AUTO_FIX,
                metadata={'task_id': row[0], 'status': row[1], 'updated_at': row[2]}
            )
            issues.append(issue)
        
        # æ£€æŸ¥æ— æ•ˆçš„çŠ¶æ€å€¼
        valid_states = ['pending', 'running', 'completed', 'failed', 'retry', 'cancelled']
        cursor = conn.execute(f"""
            SELECT id, status
            FROM publishing_tasks
            WHERE status NOT IN ({','.join(['?'] * len(valid_states))})
        """, valid_states)
        
        for row in cursor:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.INVALID_DATA,
                table_name=table_name,
                record_id=row[0],
                column_name='status',
                description=f"ä»»åŠ¡ {row[0]} çš„çŠ¶æ€å€¼æ— æ•ˆ: {row[1]}",
                severity=4,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.DEFAULT_VALUE,
                metadata={'task_id': row[0], 'invalid_status': row[1]}
            )
            issues.append(issue)
        
        return issues
    
    def _check_task_timestamps(self, conn: sqlite3.Connection, table_name: str,
                              schema: TableSchema) -> List[IntegrityIssue]:
        """æ£€æŸ¥ä»»åŠ¡æ—¶é—´æˆ³ä¸€è‡´æ€§"""
        issues = []
        
        # æ£€æŸ¥æ—¶é—´æˆ³å¼‚å¸¸ï¼ˆæ›´æ–°æ—¶é—´æ—©äºåˆ›å»ºæ—¶é—´ï¼‰
        cursor = conn.execute("""
            SELECT id, created_at, updated_at
            FROM publishing_tasks
            WHERE datetime(updated_at) < datetime(created_at)
        """)
        
        for row in cursor:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.TIMESTAMP_ANOMALY,
                table_name=table_name,
                record_id=row[0],
                column_name='updated_at',
                description=f"ä»»åŠ¡ {row[0]} çš„æ›´æ–°æ—¶é—´æ—©äºåˆ›å»ºæ—¶é—´",
                severity=2,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.AUTO_FIX,
                metadata={'task_id': row[0], 'created_at': row[1], 'updated_at': row[2]}
            )
            issues.append(issue)
        
        # æ£€æŸ¥æœªæ¥æ—¶é—´æˆ³
        cursor = conn.execute("""
            SELECT id, created_at
            FROM publishing_tasks
            WHERE datetime(created_at) > datetime('now')
        """)
        
        for row in cursor:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.TIMESTAMP_ANOMALY,
                table_name=table_name,
                record_id=row[0],
                column_name='created_at',
                description=f"ä»»åŠ¡ {row[0]} çš„åˆ›å»ºæ—¶é—´åœ¨æœªæ¥",
                severity=3,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.AUTO_FIX,
                metadata={'task_id': row[0], 'created_at': row[1]}
            )
            issues.append(issue)
        
        return issues
    
    def _check_task_duplicates(self, conn: sqlite3.Connection, table_name: str,
                              schema: TableSchema) -> List[IntegrityIssue]:
        """æ£€æŸ¥ä»»åŠ¡é‡å¤"""
        issues = []
        
        # æ£€æŸ¥ç›¸åŒåª’ä½“æ–‡ä»¶çš„é‡å¤ä»»åŠ¡
        cursor = conn.execute("""
            SELECT media_path, COUNT(*) as count, GROUP_CONCAT(id) as task_ids
            FROM publishing_tasks
            WHERE status IN ('pending', 'retry')
            GROUP BY media_path
            HAVING COUNT(*) > 1
        """)
        
        for row in cursor:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.DUPLICATE_DATA,
                table_name=table_name,
                record_id=None,
                column_name='media_path',
                description=f"å‘ç° {row[1]} ä¸ªé‡å¤çš„å¾…å¤„ç†ä»»åŠ¡ä½¿ç”¨ç›¸åŒåª’ä½“: {row[0]}",
                severity=3,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.AUTO_FIX,
                metadata={'media_path': row[0], 'count': row[1], 'task_ids': row[2].split(',')}
            )
            issues.append(issue)
        
        return issues
    
    def _check_log_references(self, conn: sqlite3.Connection, table_name: str,
                             schema: TableSchema) -> List[IntegrityIssue]:
        """æ£€æŸ¥æ—¥å¿—è¡¨å¼•ç”¨å®Œæ•´æ€§"""
        issues = []
        
        # æ£€æŸ¥task_idå¼•ç”¨
        cursor = conn.execute("""
            SELECT l.id, l.task_id
            FROM publishing_logs l
            LEFT JOIN publishing_tasks t ON l.task_id = t.id
            WHERE t.id IS NULL
        """)
        
        for row in cursor:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.ORPHAN_RECORD,
                table_name=table_name,
                record_id=row[0],
                column_name='task_id',
                description=f"æ—¥å¿— {row[0]} å¼•ç”¨äº†ä¸å­˜åœ¨çš„ä»»åŠ¡ {row[1]}",
                severity=2,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.CASCADE_DELETE,
                metadata={'log_id': row[0], 'task_id': row[1]}
            )
            issues.append(issue)
        
        return issues
    
    def _check_log_sequence(self, conn: sqlite3.Connection, table_name: str,
                           schema: TableSchema) -> List[IntegrityIssue]:
        """æ£€æŸ¥æ—¥å¿—åºåˆ—å®Œæ•´æ€§"""
        issues = []
        
        # æ£€æŸ¥IDåºåˆ—é—´éš™
        cursor = conn.execute("""
            SELECT id FROM publishing_logs ORDER BY id
        """)
        
        ids = [row[0] for row in cursor]
        if ids:
            expected_ids = set(range(min(ids), max(ids) + 1))
            actual_ids = set(ids)
            missing_ids = expected_ids - actual_ids
            
            if missing_ids:
                issue = IntegrityIssue(
                    issue_id=self._generate_issue_id(),
                    issue_type=IntegrityIssueType.SEQUENCE_GAP,
                    table_name=table_name,
                    record_id=None,
                    column_name='id',
                    description=f"æ—¥å¿—IDåºåˆ—å­˜åœ¨ {len(missing_ids)} ä¸ªé—´éš™",
                    severity=1,
                    detected_at=datetime.now(),
                    repair_strategy=RepairStrategy.IGNORE,
                    metadata={'missing_ids': list(missing_ids)[:10]}  # åªè®°å½•å‰10ä¸ª
                )
                issues.append(issue)
        
        return issues
    
    def _check_log_timestamps(self, conn: sqlite3.Connection, table_name: str,
                             schema: TableSchema) -> List[IntegrityIssue]:
        """æ£€æŸ¥æ—¥å¿—æ—¶é—´æˆ³"""
        issues = []
        
        # æ£€æŸ¥æ—¥å¿—æ—¶é—´æˆ³é¡ºåº
        cursor = conn.execute("""
            SELECT l1.id, l1.task_id, l1.published_at, l2.published_at
            FROM publishing_logs l1
            JOIN publishing_logs l2 ON l1.task_id = l2.task_id
            WHERE l1.id < l2.id 
            AND datetime(l1.published_at) > datetime(l2.published_at)
        """)
        
        for row in cursor:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.TIMESTAMP_ANOMALY,
                table_name=table_name,
                record_id=row[0],
                column_name='published_at',
                description=f"æ—¥å¿— {row[0]} çš„æ—¶é—´æˆ³é¡ºåºå¼‚å¸¸",
                severity=2,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.MANUAL_REVIEW,
                metadata={'log_id': row[0], 'task_id': row[1]}
            )
            issues.append(issue)
        
        return issues
    
    def _check_project_references(self, conn: sqlite3.Connection, table_name: str,
                                 schema: TableSchema) -> List[IntegrityIssue]:
        """æ£€æŸ¥é¡¹ç›®è¡¨å¼•ç”¨"""
        issues = []
        
        # æ£€æŸ¥user_idå¼•ç”¨
        cursor = conn.execute("""
            SELECT p.id, p.user_id
            FROM projects p
            LEFT JOIN users u ON p.user_id = u.id
            WHERE u.id IS NULL
        """)
        
        for row in cursor:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.MISSING_REFERENCE,
                table_name=table_name,
                record_id=row[0],
                column_name='user_id',
                description=f"é¡¹ç›® {row[0]} å¼•ç”¨äº†ä¸å­˜åœ¨çš„ç”¨æˆ· {row[1]}",
                severity=5,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.MANUAL_REVIEW,
                metadata={'project_id': row[0], 'user_id': row[1]}
            )
            issues.append(issue)
        
        return issues
    
    def _check_project_constraints(self, conn: sqlite3.Connection, table_name: str,
                                  schema: TableSchema) -> List[IntegrityIssue]:
        """æ£€æŸ¥é¡¹ç›®çº¦æŸ"""
        issues = []
        
        # æ£€æŸ¥é¡¹ç›®åç§°å”¯ä¸€æ€§ï¼ˆåŒä¸€ç”¨æˆ·ä¸‹ï¼‰
        cursor = conn.execute("""
            SELECT user_id, name, COUNT(*) as count, GROUP_CONCAT(id) as project_ids
            FROM projects
            WHERE is_active = 1
            GROUP BY user_id, name
            HAVING COUNT(*) > 1
        """)
        
        for row in cursor:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.CONSTRAINT_VIOLATION,
                table_name=table_name,
                record_id=None,
                column_name='name',
                description=f"ç”¨æˆ· {row[0]} ä¸‹å­˜åœ¨ {row[2]} ä¸ªåŒåé¡¹ç›®: {row[1]}",
                severity=3,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.MANUAL_REVIEW,
                metadata={'user_id': row[0], 'name': row[1], 'project_ids': row[3].split(',')}
            )
            issues.append(issue)
        
        return issues
    
    def _check_source_references(self, conn: sqlite3.Connection, table_name: str,
                                schema: TableSchema) -> List[IntegrityIssue]:
        """æ£€æŸ¥å†…å®¹æºå¼•ç”¨"""
        issues = []
        
        # æ£€æŸ¥project_idå¼•ç”¨
        cursor = conn.execute("""
            SELECT s.id, s.project_id
            FROM content_sources s
            LEFT JOIN projects p ON s.project_id = p.id
            WHERE p.id IS NULL
        """)
        
        for row in cursor:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.MISSING_REFERENCE,
                table_name=table_name,
                record_id=row[0],
                column_name='project_id',
                description=f"å†…å®¹æº {row[0]} å¼•ç”¨äº†ä¸å­˜åœ¨çš„é¡¹ç›® {row[1]}",
                severity=4,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.CASCADE_DELETE,
                metadata={'source_id': row[0], 'project_id': row[1]}
            )
            issues.append(issue)
        
        return issues
    
    def _check_source_paths(self, conn: sqlite3.Connection, table_name: str,
                           schema: TableSchema) -> List[IntegrityIssue]:
        """æ£€æŸ¥å†…å®¹æºè·¯å¾„æœ‰æ•ˆæ€§"""
        issues = []
        
        cursor = conn.execute("""
            SELECT id, folder_path
            FROM content_sources
            WHERE is_active = 1
        """)
        
        for row in cursor:
            source_id, folder_path = row
            
            # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
            if folder_path and not os.path.exists(folder_path):
                issue = IntegrityIssue(
                    issue_id=self._generate_issue_id(),
                    issue_type=IntegrityIssueType.INVALID_DATA,
                    table_name=table_name,
                    record_id=source_id,
                    column_name='folder_path',
                    description=f"å†…å®¹æº {source_id} çš„æ–‡ä»¶å¤¹è·¯å¾„ä¸å­˜åœ¨: {folder_path}",
                    severity=3,
                    detected_at=datetime.now(),
                    repair_strategy=RepairStrategy.MANUAL_REVIEW,
                    metadata={'source_id': source_id, 'folder_path': folder_path}
                )
                issues.append(issue)
        
        return issues
    
    def _check_api_key_validity(self, conn: sqlite3.Connection, table_name: str,
                               schema: TableSchema) -> List[IntegrityIssue]:
        """æ£€æŸ¥APIå¯†é’¥æœ‰æ•ˆæ€§"""
        issues = []
        
        # æ£€æŸ¥ç©ºçš„æˆ–æ— æ•ˆçš„APIå¯†é’¥
        cursor = conn.execute("""
            SELECT id, key_name, key_value
            FROM api_keys
            WHERE is_active = 1
            AND (key_value IS NULL OR key_value = '' OR LENGTH(key_value) < 10)
        """)
        
        for row in cursor:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.INVALID_DATA,
                table_name=table_name,
                record_id=row[0],
                column_name='key_value',
                description=f"APIå¯†é’¥ {row[1]} (ID: {row[0]}) æ— æ•ˆæˆ–ä¸ºç©º",
                severity=5,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.MANUAL_REVIEW,
                metadata={'key_id': row[0], 'key_name': row[1]}
            )
            issues.append(issue)
        
        return issues
    
    def _check_api_key_expiration(self, conn: sqlite3.Connection, table_name: str,
                                 schema: TableSchema) -> List[IntegrityIssue]:
        """æ£€æŸ¥APIå¯†é’¥è¿‡æœŸ"""
        issues = []
        
        # æ£€æŸ¥è¿‡æœŸçš„APIå¯†é’¥
        cursor = conn.execute("""
            SELECT id, key_name, expires_at
            FROM api_keys
            WHERE is_active = 1
            AND expires_at IS NOT NULL
            AND datetime(expires_at) < datetime('now')
        """)
        
        for row in cursor:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.INVALID_DATA,
                table_name=table_name,
                record_id=row[0],
                column_name='expires_at',
                description=f"APIå¯†é’¥ {row[1]} (ID: {row[0]}) å·²è¿‡æœŸ",
                severity=4,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.AUTO_FIX,
                metadata={'key_id': row[0], 'key_name': row[1], 'expires_at': row[2]}
            )
            issues.append(issue)
        
        return issues
    
    def _check_analytics_gaps(self, conn: sqlite3.Connection, table_name: str,
                             schema: TableSchema) -> List[IntegrityIssue]:
        """æ£€æŸ¥åˆ†ææ•°æ®é—´éš™"""
        issues = []
        
        # æ£€æŸ¥å°æ—¶ç»Ÿè®¡æ•°æ®çš„è¿ç»­æ€§
        cursor = conn.execute("""
            SELECT 
                datetime(hour_timestamp) as hour,
                datetime(hour_timestamp, '+1 hour') as next_hour
            FROM analytics_hourly a1
            WHERE NOT EXISTS (
                SELECT 1 FROM analytics_hourly a2
                WHERE datetime(a2.hour_timestamp) = datetime(a1.hour_timestamp, '+1 hour')
            )
            AND datetime(hour_timestamp) < datetime('now', '-1 hour')
            ORDER BY hour_timestamp DESC
            LIMIT 10
        """)
        
        gaps = cursor.fetchall()
        if gaps:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.SEQUENCE_GAP,
                table_name=table_name,
                record_id=None,
                column_name='hour_timestamp',
                description=f"åˆ†ææ•°æ®å­˜åœ¨ {len(gaps)} ä¸ªæ—¶é—´é—´éš™",
                severity=2,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.RECALCULATE,
                metadata={'gaps': [{'from': g[0], 'to': g[1]} for g in gaps[:5]]}
            )
            issues.append(issue)
        
        return issues
    
    def _check_analytics_consistency(self, conn: sqlite3.Connection, table_name: str,
                                   schema: TableSchema) -> List[IntegrityIssue]:
        """æ£€æŸ¥åˆ†ææ•°æ®ä¸€è‡´æ€§"""
        issues = []
        
        # æ£€æŸ¥è´Ÿå€¼ç»Ÿè®¡
        cursor = conn.execute("""
            SELECT id, hour_timestamp, project_id
            FROM analytics_hourly
            WHERE tasks_completed < 0 
            OR tasks_failed < 0 
            OR total_engagement < 0
        """)
        
        for row in cursor:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.INVALID_DATA,
                table_name=table_name,
                record_id=row[0],
                column_name=None,
                description=f"åˆ†æè®°å½• {row[0]} åŒ…å«è´Ÿå€¼ç»Ÿè®¡æ•°æ®",
                severity=3,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.RECALCULATE,
                metadata={'analytics_id': row[0], 'hour': row[1], 'project_id': row[2]}
            )
            issues.append(issue)
        
        return issues
    
    def _perform_generic_checks(self, conn: sqlite3.Connection, table_name: str,
                               schema: TableSchema) -> List[IntegrityIssue]:
        """æ‰§è¡Œé€šç”¨æ£€æŸ¥"""
        issues = []
        
        # æ£€æŸ¥NULLå€¼åœ¨NOT NULLåˆ—ä¸­
        for column in schema.columns:
            if column.get('notnull') and column['name'] != 'id':
                cursor = conn.execute(f"""
                    SELECT id FROM {table_name} 
                    WHERE {column['name']} IS NULL
                    LIMIT 10
                """)
                
                null_records = cursor.fetchall()
                if null_records:
                    issue = IntegrityIssue(
                        issue_id=self._generate_issue_id(),
                        issue_type=IntegrityIssueType.CONSTRAINT_VIOLATION,
                        table_name=table_name,
                        record_id=None,
                        column_name=column['name'],
                        description=f"è¡¨ {table_name} çš„åˆ— {column['name']} å­˜åœ¨ {len(null_records)} ä¸ªNULLå€¼",
                        severity=3,
                        detected_at=datetime.now(),
                        repair_strategy=RepairStrategy.DEFAULT_VALUE,
                        metadata={'record_ids': [r[0] for r in null_records]}
                    )
                    issues.append(issue)
        
        return issues
    
    def _perform_cross_table_checks(self, conn: sqlite3.Connection) -> List[IntegrityIssue]:
        """æ‰§è¡Œè·¨è¡¨æ£€æŸ¥"""
        issues = []
        
        # æ£€æŸ¥å­¤ç«‹çš„ä»»åŠ¡ï¼ˆæ²¡æœ‰å¯¹åº”æ—¥å¿—çš„å·²å®Œæˆä»»åŠ¡ï¼‰
        cursor = conn.execute("""
            SELECT t.id
            FROM publishing_tasks t
            WHERE t.status = 'completed'
            AND NOT EXISTS (
                SELECT 1 FROM publishing_logs l
                WHERE l.task_id = t.id AND l.status = 'success'
            )
        """)
        
        orphan_tasks = cursor.fetchall()
        if orphan_tasks:
            issue = IntegrityIssue(
                issue_id=self._generate_issue_id(),
                issue_type=IntegrityIssueType.INCONSISTENT_STATE,
                table_name='publishing_tasks',
                record_id=None,
                column_name='status',
                description=f"å‘ç° {len(orphan_tasks)} ä¸ªå·²å®Œæˆä½†æ— æˆåŠŸæ—¥å¿—çš„ä»»åŠ¡",
                severity=3,
                detected_at=datetime.now(),
                repair_strategy=RepairStrategy.AUTO_FIX,
                metadata={'task_ids': [t[0] for t in orphan_tasks[:10]]}
            )
            issues.append(issue)
        
        return issues
    
    def _attempt_auto_repair(self, issue: IntegrityIssue) -> bool:
        """å°è¯•è‡ªåŠ¨ä¿®å¤é—®é¢˜"""
        if not self.auto_repair:
            return False
        
        if issue.repair_strategy != RepairStrategy.AUTO_FIX:
            return False
        
        logger.info(f"ğŸ” å°è¯•è‡ªåŠ¨ä¿®å¤é—®é¢˜: {issue.issue_id}")
        
        # å¤‡ä»½æ•°æ®
        if self.backup_before_repair:
            self._create_backup()
        
        # æ ¹æ®é—®é¢˜ç±»å‹é€‰æ‹©ä¿®å¤ç­–ç•¥
        if issue.issue_type in self.repair_strategies:
            repair_func = self.repair_strategies[issue.issue_type]
            
            try:
                success = repair_func(issue)
                
                # è®°å½•ä¿®å¤ç»“æœ
                issue.repair_attempted = True
                issue.repair_successful = success
                
                self._record_repair(issue, success)
                
                if success:
                    logger.info(f"ğŸ” æˆåŠŸä¿®å¤é—®é¢˜: {issue.issue_id}")
                else:
                    logger.warning(f"ğŸ” ä¿®å¤å¤±è´¥: {issue.issue_id}")
                
                return success
                
            except Exception as e:
                logger.error(f"ğŸ” ä¿®å¤è¿‡ç¨‹å‡ºé”™: {e}")
                issue.repair_error = str(e)
                return False
        
        return False
    
    def _repair_orphan_record(self, issue: IntegrityIssue) -> bool:
        """ä¿®å¤å­¤ç«‹è®°å½•"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                if issue.repair_strategy == RepairStrategy.CASCADE_DELETE:
                    # åˆ é™¤å­¤ç«‹è®°å½•
                    conn.execute(f"""
                        DELETE FROM {issue.table_name}
                        WHERE id = ?
                    """, (issue.record_id,))
                    conn.commit()
                    return True
        except Exception as e:
            logger.error(f"ä¿®å¤å­¤ç«‹è®°å½•å¤±è´¥: {e}")
        return False
    
    def _repair_missing_reference(self, issue: IntegrityIssue) -> bool:
        """ä¿®å¤ç¼ºå¤±å¼•ç”¨"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                if issue.repair_strategy == RepairStrategy.CASCADE_DELETE:
                    # åˆ é™¤å¼•ç”¨æ— æ•ˆçš„è®°å½•
                    conn.execute(f"""
                        DELETE FROM {issue.table_name}
                        WHERE id = ?
                    """, (issue.record_id,))
                    conn.commit()
                    return True
                elif issue.repair_strategy == RepairStrategy.DEFAULT_VALUE:
                    # è®¾ç½®ä¸ºNULLæˆ–é»˜è®¤å€¼
                    conn.execute(f"""
                        UPDATE {issue.table_name}
                        SET {issue.column_name} = NULL
                        WHERE id = ?
                    """, (issue.record_id,))
                    conn.commit()
                    return True
        except Exception as e:
            logger.error(f"ä¿®å¤ç¼ºå¤±å¼•ç”¨å¤±è´¥: {e}")
        return False
    
    def _repair_duplicate_data(self, issue: IntegrityIssue) -> bool:
        """ä¿®å¤é‡å¤æ•°æ®"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                if 'task_ids' in issue.metadata:
                    # ä¿ç•™æœ€æ–°çš„ï¼Œåˆ é™¤å…¶ä»–çš„
                    task_ids = issue.metadata['task_ids']
                    if len(task_ids) > 1:
                        # ä¿ç•™ç¬¬ä¸€ä¸ª
                        tasks_to_delete = task_ids[1:]
                        placeholders = ','.join(['?'] * len(tasks_to_delete))
                        conn.execute(f"""
                            DELETE FROM {issue.table_name}
                            WHERE id IN ({placeholders})
                        """, tasks_to_delete)
                        conn.commit()
                        return True
        except Exception as e:
            logger.error(f"ä¿®å¤é‡å¤æ•°æ®å¤±è´¥: {e}")
        return False
    
    def _repair_inconsistent_state(self, issue: IntegrityIssue) -> bool:
        """ä¿®å¤ä¸ä¸€è‡´çŠ¶æ€"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                if issue.column_name == 'status':
                    # é‡ç½®ä¸ºpendingçŠ¶æ€
                    conn.execute(f"""
                        UPDATE {issue.table_name}
                        SET status = 'pending', updated_at = datetime('now')
                        WHERE id = ?
                    """, (issue.record_id,))
                    conn.commit()
                    return True
        except Exception as e:
            logger.error(f"ä¿®å¤ä¸ä¸€è‡´çŠ¶æ€å¤±è´¥: {e}")
        return False
    
    def _repair_invalid_data(self, issue: IntegrityIssue) -> bool:
        """ä¿®å¤æ— æ•ˆæ•°æ®"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                if issue.repair_strategy == RepairStrategy.DEFAULT_VALUE:
                    # æ ¹æ®åˆ—ç±»å‹è®¾ç½®é»˜è®¤å€¼
                    default_values = {
                        'status': 'pending',
                        'priority': 1,
                        'retry_count': 0,
                        'is_active': 1
                    }
                    
                    default_value = default_values.get(issue.column_name, '')
                    
                    conn.execute(f"""
                        UPDATE {issue.table_name}
                        SET {issue.column_name} = ?
                        WHERE id = ?
                    """, (default_value, issue.record_id))
                    conn.commit()
                    return True
        except Exception as e:
            logger.error(f"ä¿®å¤æ— æ•ˆæ•°æ®å¤±è´¥: {e}")
        return False
    
    def _repair_corrupted_data(self, issue: IntegrityIssue) -> bool:
        """ä¿®å¤æŸåæ•°æ®"""
        # é€šå¸¸éœ€è¦ä»å¤‡ä»½æ¢å¤
        logger.warning(f"æŸåæ•°æ®éœ€è¦ä»å¤‡ä»½æ¢å¤: {issue}")
        return False
    
    def _repair_constraint_violation(self, issue: IntegrityIssue) -> bool:
        """ä¿®å¤çº¦æŸè¿å"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                if issue.column_name and issue.repair_strategy == RepairStrategy.DEFAULT_VALUE:
                    # è®¾ç½®ä¸ºNULLæˆ–é»˜è®¤å€¼
                    conn.execute(f"""
                        UPDATE {issue.table_name}
                        SET {issue.column_name} = NULL
                        WHERE id = ?
                    """, (issue.record_id,))
                    conn.commit()
                    return True
        except Exception as e:
            logger.error(f"ä¿®å¤çº¦æŸè¿åå¤±è´¥: {e}")
        return False
    
    def _repair_schema_mismatch(self, issue: IntegrityIssue) -> bool:
        """ä¿®å¤æ¨¡å¼ä¸åŒ¹é…"""
        # é€šå¸¸éœ€è¦æ•°æ®åº“è¿ç§»
        logger.warning(f"æ¨¡å¼ä¸åŒ¹é…éœ€è¦æ•°æ®åº“è¿ç§»: {issue}")
        return False
    
    def _repair_sequence_gap(self, issue: IntegrityIssue) -> bool:
        """ä¿®å¤åºåˆ—é—´éš™"""
        # åºåˆ—é—´éš™é€šå¸¸å¯ä»¥å¿½ç•¥
        if issue.repair_strategy == RepairStrategy.IGNORE:
            return True
        return False
    
    def _repair_timestamp_anomaly(self, issue: IntegrityIssue) -> bool:
        """ä¿®å¤æ—¶é—´æˆ³å¼‚å¸¸"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                if 'created_at' in issue.metadata and 'updated_at' in issue.metadata:
                    # ä¿®æ­£æ›´æ–°æ—¶é—´
                    conn.execute(f"""
                        UPDATE {issue.table_name}
                        SET updated_at = created_at
                        WHERE id = ?
                    """, (issue.record_id,))
                    conn.commit()
                    return True
                elif issue.column_name == 'created_at':
                    # ä¿®æ­£æœªæ¥æ—¶é—´æˆ³
                    conn.execute(f"""
                        UPDATE {issue.table_name}
                        SET created_at = datetime('now')
                        WHERE id = ?
                    """, (issue.record_id,))
                    conn.commit()
                    return True
        except Exception as e:
            logger.error(f"ä¿®å¤æ—¶é—´æˆ³å¼‚å¸¸å¤±è´¥: {e}")
        return False
    
    def _create_backup(self):
        """åˆ›å»ºæ•°æ®åº“å¤‡ä»½"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            backup_dir = os.path.dirname(self.db_path)
            backup_path = os.path.join(backup_dir, f"backup_{timestamp}.db")
            
            shutil.copy2(self.db_path, backup_path)
            logger.info(f"ğŸ” åˆ›å»ºæ•°æ®åº“å¤‡ä»½: {backup_path}")
            
            # æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™æœ€è¿‘10ä¸ªï¼‰
            self._cleanup_old_backups(backup_dir)
            
        except Exception as e:
            logger.error(f"åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
    
    def _cleanup_old_backups(self, backup_dir: str):
        """æ¸…ç†æ—§å¤‡ä»½"""
        try:
            backup_files = [
                f for f in os.listdir(backup_dir)
                if f.startswith('backup_') and f.endswith('.db')
            ]
            
            if len(backup_files) > 10:
                backup_files.sort()
                for old_backup in backup_files[:-10]:
                    os.remove(os.path.join(backup_dir, old_backup))
                    logger.debug(f"åˆ é™¤æ—§å¤‡ä»½: {old_backup}")
                    
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§å¤‡ä»½å¤±è´¥: {e}")
    
    def _generate_recommendations(self, issues: List[IntegrityIssue]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # ç»Ÿè®¡é—®é¢˜ç±»å‹
        issue_types = defaultdict(int)
        severity_counts = defaultdict(int)
        
        for issue in issues:
            issue_types[issue.issue_type] += 1
            severity_counts[issue.severity] += 1
        
        # æ ¹æ®é—®é¢˜ç±»å‹ç”Ÿæˆå»ºè®®
        if issue_types[IntegrityIssueType.MISSING_REFERENCE] > 0:
            recommendations.append("å»ºè®®å¯ç”¨å¤–é”®çº¦æŸä»¥é˜²æ­¢å¼•ç”¨å®Œæ•´æ€§é—®é¢˜")
        
        if issue_types[IntegrityIssueType.DUPLICATE_DATA] > 0:
            recommendations.append("å»ºè®®æ·»åŠ å”¯ä¸€ç´¢å¼•ä»¥é˜²æ­¢æ•°æ®é‡å¤")
        
        if issue_types[IntegrityIssueType.TIMESTAMP_ANOMALY] > 0:
            recommendations.append("å»ºè®®ä½¿ç”¨è§¦å‘å™¨è‡ªåŠ¨ç»´æŠ¤æ—¶é—´æˆ³å­—æ®µ")
        
        if issue_types[IntegrityIssueType.ORPHAN_RECORD] > 0:
            recommendations.append("å»ºè®®å®šæœŸæ¸…ç†å­¤ç«‹è®°å½•")
        
        # æ ¹æ®ä¸¥é‡æ€§ç”Ÿæˆå»ºè®®
        if severity_counts[5] > 0:
            recommendations.append("å‘ç°ä¸¥é‡é—®é¢˜ï¼Œå»ºè®®ç«‹å³è¿›è¡Œäººå·¥å®¡æŸ¥")
        
        if severity_counts[4] > 5:
            recommendations.append("å‘ç°å¤šä¸ªé«˜ä¸¥é‡æ€§é—®é¢˜ï¼Œå»ºè®®è¿›è¡Œå…¨é¢çš„æ•°æ®å®¡è®¡")
        
        # é€šç”¨å»ºè®®
        if len(issues) > 50:
            recommendations.append("é—®é¢˜æ•°é‡è¾ƒå¤šï¼Œå»ºè®®å¢åŠ æ£€æŸ¥é¢‘ç‡")
        
        if self.statistics['failed_repairs'] > 10:
            recommendations.append("ä¿®å¤å¤±è´¥ç‡è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥ä¿®å¤ç­–ç•¥é…ç½®")
        
        return recommendations
    
    def _record_repair(self, issue: IntegrityIssue, success: bool):
        """è®°å½•ä¿®å¤æ“ä½œ"""
        repair_record = {
            'issue_id': issue.issue_id,
            'issue_type': issue.issue_type.value,
            'repair_time': datetime.now().isoformat(),
            'success': success,
            'repair_strategy': issue.repair_strategy.value,
            'metadata': issue.metadata
        }
        
        self.repair_history.append(repair_record)
        
        # æ›´æ–°ç»Ÿè®¡
        if success:
            self.statistics['auto_fixed'] += 1
        else:
            self.statistics['failed_repairs'] += 1
    
    def _update_statistics(self, report: IntegrityReport):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.statistics['total_checks'] += 1
        self.statistics['total_issues'] += len(report.issues_found)
        
        # æ›´æ–°å¹³å‡æ£€æŸ¥æ—¶é•¿
        current_avg = self.statistics['check_duration_avg']
        new_avg = (current_avg * (self.statistics['total_checks'] - 1) + 
                  report.duration_seconds) / self.statistics['total_checks']
        self.statistics['check_duration_avg'] = new_avg
    
    def _log_report(self, report: IntegrityReport):
        """è®°å½•æ£€æŸ¥æŠ¥å‘Š"""
        logger.info("ğŸ” æ•°æ®å®Œæ•´æ€§æ£€æŸ¥æŠ¥å‘Š:")
        logger.info(f"  - æ£€æŸ¥ID: {report.check_id}")
        logger.info(f"  - æ£€æŸ¥æ—¶é—´: {report.check_time}")
        logger.info(f"  - è€—æ—¶: {report.duration_seconds:.2f}ç§’")
        logger.info(f"  - æ£€æŸ¥è¡¨æ•°: {report.tables_checked}")
        logger.info(f"  - æ£€æŸ¥è®°å½•æ•°: {report.records_checked}")
        logger.info(f"  - å‘ç°é—®é¢˜: {len(report.issues_found)}")
        logger.info(f"  - è‡ªåŠ¨ä¿®å¤: {report.auto_fixed}")
        logger.info(f"  - éœ€è¦äººå·¥: {report.manual_required}")
        logger.info(f"  - ä¸¥é‡é—®é¢˜: {report.critical_issues}")
        
        if report.recommendations:
            logger.info("  å»ºè®®:")
            for rec in report.recommendations:
                logger.info(f"    - {rec}")
    
    def _generate_check_id(self) -> str:
        """ç”Ÿæˆæ£€æŸ¥ID"""
        timestamp = datetime.now().isoformat()
        return hashlib.md5(f"check_{timestamp}".encode()).hexdigest()[:12]
    
    def _generate_issue_id(self) -> str:
        """ç”Ÿæˆé—®é¢˜ID"""
        timestamp = datetime.now().isoformat()
        random_str = str(time.time())
        return hashlib.md5(f"issue_{timestamp}_{random_str}".encode()).hexdigest()[:16]
    
    def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§"""
        if self.monitoring:
            return
        
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        
        logger.info("ğŸ” æ•°æ®å®Œæ•´æ€§ç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        
        logger.info("ğŸ” æ•°æ®å®Œæ•´æ€§ç›‘æ§å·²åœæ­¢")
    
    def _monitoring_loop(self):
        """ç›‘æ§å¾ªç¯"""
        logger.info("ğŸ” æ•°æ®å®Œæ•´æ€§ç›‘æ§å¾ªç¯å¯åŠ¨")
        
        while self.monitoring:
            try:
                # æ‰§è¡Œå®šæœŸæ£€æŸ¥
                if self._should_perform_check():
                    report = self.perform_full_check()
                    
                    # å¦‚æœå‘ç°ä¸¥é‡é—®é¢˜ï¼Œå‘é€è­¦æŠ¥
                    if report.critical_issues > 0:
                        self._send_alert(report)
                
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ˜¯å¦éœ€è¦æ‰§è¡Œ
                
            except Exception as e:
                logger.error(f"ğŸ” ç›‘æ§å¾ªç¯å¼‚å¸¸: {e}")
                time.sleep(60)
        
        logger.info("ğŸ” æ•°æ®å®Œæ•´æ€§ç›‘æ§å¾ªç¯ç»“æŸ")
    
    def _should_perform_check(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰§è¡Œæ£€æŸ¥"""
        if not self.last_check_time:
            return True
        
        elapsed = (datetime.now() - self.last_check_time).total_seconds()
        return elapsed >= self.check_interval
    
    def _send_alert(self, report: IntegrityReport):
        """å‘é€è­¦æŠ¥"""
        logger.warning(f"ğŸš¨ æ•°æ®å®Œæ•´æ€§è­¦æŠ¥: å‘ç° {report.critical_issues} ä¸ªä¸¥é‡é—®é¢˜!")
        
        # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶ã€Slackç­‰é€šçŸ¥æœºåˆ¶
        
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_checks': self.statistics['total_checks'],
            'total_issues': self.statistics['total_issues'],
            'auto_fixed': self.statistics['auto_fixed'],
            'manual_required': self.statistics['manual_required'],
            'failed_repairs': self.statistics['failed_repairs'],
            'check_duration_avg': self.statistics['check_duration_avg'],
            'active_issues': len(self.active_issues),
            'monitoring': self.monitoring,
            'last_check_time': self.last_check_time.isoformat() if self.last_check_time else None
        }

# å…¨å±€å®ä¾‹
_data_integrity_checker: Optional[DataIntegrityChecker] = None

def get_data_integrity_checker(db_path: str = "./data/twitter_publisher.db") -> DataIntegrityChecker:
    """è·å–æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å™¨å®ä¾‹"""
    global _data_integrity_checker
    
    if _data_integrity_checker is None:
        _data_integrity_checker = DataIntegrityChecker(db_path)
    
    return _data_integrity_checker

def perform_integrity_check(auto_repair: bool = True) -> IntegrityReport:
    """
    ä¾¿æ·å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´æ€§æ£€æŸ¥
    
    Args:
        auto_repair: æ˜¯å¦è‡ªåŠ¨ä¿®å¤
        
    Returns:
        IntegrityReport: æ£€æŸ¥æŠ¥å‘Š
    """
    checker = get_data_integrity_checker()
    checker.auto_repair = auto_repair
    return checker.perform_full_check()