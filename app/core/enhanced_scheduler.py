#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºå‹ä»»åŠ¡è°ƒåº¦å™¨ - ä¼˜åŒ–çš„ä»»åŠ¡è°ƒåº¦å’Œæ‰§è¡Œç®¡ç†

ä¸»è¦æ”¹è¿›:
1. æ™ºèƒ½é‡è¯•æœºåˆ¶
2. ä»»åŠ¡ä¼˜å…ˆçº§ç®¡ç†
3. å¹¶å‘æ§åˆ¶
4. æ€§èƒ½ç›‘æ§
5. é”™è¯¯æ¢å¤
6. èµ„æºç®¡ç†
"""

import asyncio
import os
import time
import random
import signal
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any, Callable
from dataclasses import dataclass
from enum import Enum
import threading
from queue import PriorityQueue, Empty

from app.core.content_generator import ContentGenerator
from app.core.publisher import TwitterPublisher
from app.core.global_task_creator import GlobalTaskCreator
from app.database.repository import (
    ContentSourceRepository,
    ProjectRepository,
    PublishingLogRepository,
    PublishingTaskRepository,
)
from app.database.models import PublishingTask
from app.utils.logger import get_logger
from app.utils.enhanced_config import get_enhanced_config
from app.utils.path_manager import get_path_manager
from app.utils.dynamic_path_manager import get_dynamic_path_manager
from app.utils.performance_monitor import PerformanceMonitor
from app.utils.retry_handler import ErrorHandler
from app.utils.error_classifier import error_classifier, classify_and_handle_error
from app.utils.priority_calculator import priority_calculator, calculate_task_priority, get_priority_level
from app.utils.optimal_timing_predictor import optimal_timing_predictor, predict_best_publish_time
from app.utils.stuck_task_recovery import stuck_task_recovery_manager, detect_and_recover_stuck_tasks
from app.utils.database_lock_manager import get_database_lock_manager, execute_with_lock_protection
from app.utils.data_integrity_checker import get_data_integrity_checker, perform_integrity_check

logger = get_logger(__name__)


class TaskPriority(Enum):
    """ä»»åŠ¡ä¼˜å…ˆçº§"""
    LOW = 3
    NORMAL = 2
    HIGH = 1
    URGENT = 0


class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    RETRYING = "retrying"
    CANCELLED = "cancelled"


@dataclass
class TaskExecution:
    """ä»»åŠ¡æ‰§è¡Œä¿¡æ¯"""
    task_id: int
    priority: TaskPriority
    scheduled_time: datetime
    retry_count: int = 0
    last_error: Optional[str] = None
    
    def __lt__(self, other):
        """ç”¨äºä¼˜å…ˆé˜Ÿåˆ—æ’åº"""
        if self.priority.value != other.priority.value:
            return self.priority.value < other.priority.value
        return self.scheduled_time < other.scheduled_time


class EnhancedTaskScheduler:
    """å¢å¼ºå‹ä»»åŠ¡è°ƒåº¦å™¨"""
    
    def __init__(self, db_manager=None, content_generator=None, publisher=None):
        self.config = get_enhanced_config()
        self.path_manager = get_path_manager()
        self.dynamic_path_manager = get_dynamic_path_manager()
        
        # å¦‚æœæä¾›äº†æ•°æ®åº“ç®¡ç†å™¨ï¼Œä½¿ç”¨å®ƒæ¥è·å–session
        if db_manager:
            self.db_manager = db_manager
            # ä½¿ç”¨æ•°æ®åº“ç®¡ç†å™¨çš„session
            session = db_manager.get_session()
            self.task_repo = PublishingTaskRepository(session)
            self.log_repo = PublishingLogRepository(session)
            self.project_repo = ProjectRepository(session)
            self.content_source_repo = ContentSourceRepository(session)
        else:
            # å…¼å®¹æ€§ï¼šå¦‚æœæ²¡æœ‰æä¾›æ•°æ®åº“ç®¡ç†å™¨ï¼Œå°è¯•åˆ›å»ºé»˜è®¤çš„
            from app.database.db_manager import EnhancedDatabaseManager
            self.db_manager = EnhancedDatabaseManager()
            session = self.db_manager.get_session()
            self.task_repo = PublishingTaskRepository(session)
            self.log_repo = PublishingLogRepository(session)
            self.project_repo = ProjectRepository(session)
            self.content_source_repo = ContentSourceRepository(session)
        
        # ä½¿ç”¨æä¾›çš„ç»„ä»¶æˆ–åˆ›å»ºé»˜è®¤çš„
        self.content_generator = content_generator or ContentGenerator()
        self.publisher = publisher or TwitterPublisher()
        self.performance_monitor = PerformanceMonitor()
        self.error_handler = ErrorHandler()
        
        # ğŸ¯ Phase 3: é›†æˆæ™ºèƒ½é”™è¯¯åˆ†ç±»å™¨
        self.error_classifier = error_classifier
        logger.info("âœ… æ™ºèƒ½é”™è¯¯åˆ†ç±»å™¨å·²é›†æˆåˆ°è°ƒåº¦å™¨")
        
        # ğŸ¯ Phase 3.4: é›†æˆä¼˜å…ˆçº§æƒé‡ç®—æ³•
        self.priority_calculator = priority_calculator
        logger.info("âœ… ä¼˜å…ˆçº§æƒé‡ç®—æ³•å·²é›†æˆåˆ°è°ƒåº¦å™¨")
        
        # ğŸ“… Phase 3.5: é›†æˆæœ€ä½³å‘å¸ƒæ—¶é—´é¢„æµ‹å™¨
        self.timing_predictor = optimal_timing_predictor
        logger.info("âœ… æœ€ä½³å‘å¸ƒæ—¶é—´é¢„æµ‹å™¨å·²é›†æˆåˆ°è°ƒåº¦å™¨")
        
        # ğŸ›¡ï¸ Phase 4.1: é›†æˆå¡ä½ä»»åŠ¡è‡ªåŠ¨æ¢å¤ç®¡ç†å™¨
        self.stuck_recovery_manager = stuck_task_recovery_manager
        logger.info("âœ… å¡ä½ä»»åŠ¡è‡ªåŠ¨æ¢å¤ç®¡ç†å™¨å·²é›†æˆåˆ°è°ƒåº¦å™¨")
        
        # ğŸ”’ Phase 4.2: é›†æˆæ•°æ®åº“é”ç®¡ç†å™¨
        db_path = self.config.get('database', {}).get('path', './data/twitter_publisher.db')
        self.lock_manager = get_database_lock_manager(db_path)
        logger.info("âœ… æ•°æ®åº“é”ç®¡ç†å™¨å·²é›†æˆåˆ°è°ƒåº¦å™¨")
        
        # ğŸ” Phase 4.3: é›†æˆæ•°æ®å®Œæ•´æ€§æ£€æŸ¥å™¨
        self.integrity_checker = get_data_integrity_checker(db_path)
        logger.info("âœ… æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å™¨å·²é›†æˆåˆ°è°ƒåº¦å™¨")
        
        # åˆå§‹åŒ–å…¨å±€ä»»åŠ¡åˆ›å»ºå™¨
        self.global_task_creator = GlobalTaskCreator(self.db_manager)
        
        # è°ƒåº¦å™¨é…ç½®
        scheduler_config = self.config.get('scheduling', {})
        self.max_workers = scheduler_config.get('max_workers', 5)
        self.batch_size = scheduler_config.get('batch_size', 3)
        self.check_interval = scheduler_config.get('check_interval', 60)
        self.max_retries = scheduler_config.get('max_retries', 3)
        self.stuck_task_timeout = scheduler_config.get('stuck_task_timeout', 300)
        
        # â° Phase 3.3: ä»»åŠ¡è¶…æ—¶ä¿æŠ¤é…ç½®
        self.task_timeout_minutes = scheduler_config.get('task_timeout_minutes', 5)
        self.task_timeout_seconds = self.task_timeout_minutes * 60
        logger.info(f"â° ä»»åŠ¡è¶…æ—¶ä¿æŠ¤å·²å¯ç”¨: {self.task_timeout_minutes} åˆ†é’Ÿ")
        
        # æ¯æ—¥ä»»åŠ¡æ•°é‡é…ç½®
        self.daily_min_tasks = scheduler_config.get('daily_min_tasks', 5)
        self.daily_max_tasks = scheduler_config.get('daily_max_tasks', 6)
        
        # è¿è¡ŒçŠ¶æ€
        self.is_running = False
        self.executor = None
        self.task_queue = PriorityQueue()
        self.running_tasks = {}
        self.stats = {
            'total_processed': 0,
            'successful': 0,
            'failed': 0,
            'retried': 0,
            'start_time': None
        }
        
        # çº¿ç¨‹é”
        self.lock = threading.Lock() # ç”¨äºä¿æŠ¤è°ƒåº¦å™¨å†…éƒ¨çŠ¶æ€
        self.db_write_lock = threading.Lock() # ç”¨äºä¿æŠ¤æ•°æ®åº“å†™å…¥
        
    def start(self) -> Dict[str, Any]:
        """
        å¯åŠ¨è°ƒåº¦å™¨
        
        Returns:
            å¯åŠ¨ç»“æœä¿¡æ¯
        """
        if self.is_running:
            return {
                'success': False,
                'message': 'è°ƒåº¦å™¨å·²åœ¨è¿è¡Œä¸­'
            }
            
        try:
            self.is_running = True
            self.stats['start_time'] = datetime.now()
            self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
            
            # å¯åŠ¨ä¸»è°ƒåº¦å¾ªç¯
            threading.Thread(target=self._scheduler_loop, daemon=True).start()
            
            # å¯åŠ¨ç›‘æ§çº¿ç¨‹
            threading.Thread(target=self._monitor_loop, daemon=True).start()
            
            # ğŸ›¡ï¸ Phase 4.1: å¯åŠ¨å¡ä½ä»»åŠ¡æ¢å¤ç›‘æ§
            self.stuck_recovery_manager.start_monitoring(self.db_manager)
            
            logger.info(f"å¢å¼ºå‹ä»»åŠ¡è°ƒåº¦å™¨å¯åŠ¨æˆåŠŸï¼Œæœ€å¤§å·¥ä½œçº¿ç¨‹: {self.max_workers}")
            
            return {
                'success': True,
                'message': 'è°ƒåº¦å™¨å¯åŠ¨æˆåŠŸ',
                'config': {
                    'max_workers': self.max_workers,
                    'batch_size': self.batch_size,
                    'check_interval': self.check_interval
                }
            }
            
        except Exception as e:
            self.is_running = False
            logger.error(f"è°ƒåº¦å™¨å¯åŠ¨å¤±è´¥: {e}", exc_info=True)
            return {
                'success': False,
                'message': f'è°ƒåº¦å™¨å¯åŠ¨å¤±è´¥: {str(e)}'
            }
            
    def stop(self) -> Dict[str, Any]:
        """
        åœæ­¢è°ƒåº¦å™¨
        
        Returns:
            åœæ­¢ç»“æœä¿¡æ¯
        """
        if not self.is_running:
            return {
                'success': False,
                'message': 'è°ƒåº¦å™¨æœªåœ¨è¿è¡Œ'
            }
            
        try:
            self.is_running = False
            
            # ğŸ›¡ï¸ Phase 4.1: åœæ­¢å¡ä½ä»»åŠ¡æ¢å¤ç›‘æ§
            self.stuck_recovery_manager.stop_monitoring()
            
            # ç­‰å¾…æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡å®Œæˆ
            if self.executor:
                self.executor.shutdown(wait=True, timeout=60)
                
            # æ¸…ç†è¿è¡ŒçŠ¶æ€
            self._cleanup_running_tasks()
            
            logger.info("å¢å¼ºå‹ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢")
            
            return {
                'success': True,
                'message': 'è°ƒåº¦å™¨å·²åœæ­¢',
                'stats': self.get_stats()
            }
            
        except Exception as e:
            logger.error(f"è°ƒåº¦å™¨åœæ­¢å¤±è´¥: {e}", exc_info=True)
            return {
                'success': False,
                'message': f'è°ƒåº¦å™¨åœæ­¢å¤±è´¥: {str(e)}'
            }
            
    def schedule_task(self, task_id: int, priority: TaskPriority = TaskPriority.NORMAL,
                     delay_seconds: int = 0) -> bool:
        """
        è°ƒåº¦å•ä¸ªä»»åŠ¡
        
        Args:
            task_id: ä»»åŠ¡ID
            priority: ä»»åŠ¡ä¼˜å…ˆçº§
            delay_seconds: å»¶è¿Ÿæ‰§è¡Œç§’æ•°
            
        Returns:
            æ˜¯å¦æˆåŠŸè°ƒåº¦
        """
        try:
            scheduled_time = datetime.now() + timedelta(seconds=delay_seconds)
            
            task_execution = TaskExecution(
                task_id=task_id,
                priority=priority,
                scheduled_time=scheduled_time
            )
            
            self.task_queue.put(task_execution)
            logger.info(f"ä»»åŠ¡ {task_id} å·²è°ƒåº¦ï¼Œä¼˜å…ˆçº§: {priority.name}")
            
            return True
            
        except Exception as e:
            logger.error(f"è°ƒåº¦ä»»åŠ¡ {task_id} å¤±è´¥: {e}")
            return False
            
    def schedule_batch(self, limit: int = None) -> Dict[str, Any]:
        """
        æ‰¹é‡è°ƒåº¦å¾…å¤„ç†ä»»åŠ¡
        
        Args:
            limit: æœ€å¤§è°ƒåº¦æ•°é‡
            
        Returns:
            è°ƒåº¦ç»“æœä¿¡æ¯
        """
        try:
            # è·å–å¾…å¤„ç†ä»»åŠ¡
            pending_tasks = self.task_repo.get_pending_tasks(
                limit=limit or self.batch_size
            )
            
            scheduled_count = 0
            for task in pending_tasks:
                # æ ¹æ®ä»»åŠ¡å±æ€§ç¡®å®šä¼˜å…ˆçº§
                priority = self._determine_task_priority(task)
                
                if self.schedule_task(task.id, priority):
                    scheduled_count += 1
                    
            logger.info(f"æ‰¹é‡è°ƒåº¦å®Œæˆï¼Œè°ƒåº¦äº† {scheduled_count} ä¸ªä»»åŠ¡")
            
            return {
                'success': True,
                'scheduled_count': scheduled_count,
                'total_pending': len(pending_tasks)
            }
            
        except Exception as e:
            logger.error(f"æ‰¹é‡è°ƒåº¦å¤±è´¥: {e}", exc_info=True)
            return {
                'success': False,
                'message': f'æ‰¹é‡è°ƒåº¦å¤±è´¥: {str(e)}'
            }
            
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–è°ƒåº¦å™¨ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        stats = self.stats.copy()
        
        # æ·»åŠ è¿è¡Œæ—¶ä¿¡æ¯
        stats['is_running'] = self.is_running
        stats['queue_size'] = self.task_queue.qsize()
        stats['running_tasks_count'] = len(self.running_tasks)
        
        if stats['start_time']:
            stats['uptime_seconds'] = (datetime.now() - stats['start_time']).total_seconds()
            
        # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
        stats['performance'] = self.performance_monitor.get_metrics()
        
        return stats
        
    def _scheduler_loop(self):
        """ä¸»è°ƒåº¦å¾ªç¯"""
        logger.info("è°ƒåº¦å™¨ä¸»å¾ªç¯å¯åŠ¨")
        
        while self.is_running:
            try:
                # å¤„ç†é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡
                self._process_task_queue()
                
                # æ£€æŸ¥å¡ä½çš„ä»»åŠ¡
                self._check_stuck_tasks()
                
                # è‡ªåŠ¨è°ƒåº¦æ–°ä»»åŠ¡ - ä½¿ç”¨å…¨å±€ä»»åŠ¡åˆ›å»ºå™¨
                if self.task_queue.qsize() < self.batch_size:
                    try:
                        # ä½¿ç”¨å…¨å±€ä»»åŠ¡åˆ›å»ºå™¨åˆ›å»ºæ¯æ—¥ä»»åŠ¡
                        result = self.global_task_creator.create_daily_tasks()
                        if result.get('success'):
                            logger.info(f"å…¨å±€ä»»åŠ¡åˆ›å»ºå™¨åˆ›å»ºäº† {result.get('created_count', 0)} ä¸ªä»»åŠ¡")
                            # è°ƒåº¦æ–°åˆ›å»ºçš„ä»»åŠ¡
                            self.schedule_batch()
                        else:
                            logger.warning(f"å…¨å±€ä»»åŠ¡åˆ›å»ºå™¨æœªåˆ›å»ºæ–°ä»»åŠ¡: {result.get('message', 'æœªçŸ¥åŸå› ')}")
                    except Exception as e:
                        logger.error(f"å…¨å±€ä»»åŠ¡åˆ›å»ºå™¨æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
                        # å¦‚æœå…¨å±€ä»»åŠ¡åˆ›å»ºå™¨å¤±è´¥ï¼Œå›é€€åˆ°åŸæœ‰çš„è°ƒåº¦æ–¹å¼
                        self.schedule_batch()
                    
                time.sleep(self.check_interval)
                
            except Exception as e:
                logger.error(f"è°ƒåº¦å¾ªç¯å¼‚å¸¸: {e}", exc_info=True)
                time.sleep(5)  # å¼‚å¸¸æ—¶çŸ­æš‚ä¼‘çœ 
                
        logger.info("è°ƒåº¦å™¨ä¸»å¾ªç¯ç»“æŸ")
        
    def _process_task_queue(self):
        """â° Phase 3.3: å¤„ç†ä»»åŠ¡é˜Ÿåˆ—ï¼ˆåŒ…å«è¶…æ—¶ä¿æŠ¤ï¼‰"""
        available_workers = self.max_workers - len(self.running_tasks)
        
        # æ£€æŸ¥è¿è¡Œä¸­ä»»åŠ¡çš„è¶…æ—¶æƒ…å†µ
        self._check_task_timeouts()
        
        for _ in range(min(available_workers, self.task_queue.qsize())):
            try:
                # è·å–ä¸‹ä¸€ä¸ªä»»åŠ¡ï¼ˆéé˜»å¡ï¼‰
                task_execution = self.task_queue.get_nowait()
                
                # æ£€æŸ¥æ˜¯å¦åˆ°äº†æ‰§è¡Œæ—¶é—´
                if datetime.now() < task_execution.scheduled_time:
                    # é‡æ–°æ”¾å›é˜Ÿåˆ—
                    self.task_queue.put(task_execution)
                    break
                    
                # æäº¤ä»»åŠ¡æ‰§è¡Œï¼ˆå¸¦è¶…æ—¶ä¿æŠ¤ï¼‰
                future = self.executor.submit(self._execute_task_with_timeout, task_execution)
                
                with self.lock:
                    self.running_tasks[task_execution.task_id] = {
                        'future': future,
                        'start_time': datetime.now(),
                        'task_execution': task_execution,
                        'timeout_seconds': self.task_timeout_seconds
                    }
                    
            except Empty:
                break
            except Exception as e:
                logger.error(f"å¤„ç†ä»»åŠ¡é˜Ÿåˆ—å¼‚å¸¸: {e}")
    
    def _check_task_timeouts(self):
        """â° æ£€æŸ¥ä»»åŠ¡è¶…æ—¶æƒ…å†µ"""
        current_time = datetime.now()
        timed_out_tasks = []
        
        with self.lock:
            for task_id, task_info in self.running_tasks.items():
                start_time = task_info['start_time']
                timeout_seconds = task_info.get('timeout_seconds', self.task_timeout_seconds)
                
                elapsed_time = (current_time - start_time).total_seconds()
                
                if elapsed_time > timeout_seconds:
                    logger.warning(f"â° ä»»åŠ¡ {task_id} æ‰§è¡Œè¶…æ—¶: {elapsed_time:.1f}s > {timeout_seconds}s")
                    timed_out_tasks.append((task_id, task_info))
                    
        # å¤„ç†è¶…æ—¶ä»»åŠ¡
        for task_id, task_info in timed_out_tasks:
            self._handle_task_timeout(task_id, task_info)
    
    def _handle_task_timeout(self, task_id: int, task_info: Dict[str, Any]):
        """â° å¤„ç†ä»»åŠ¡è¶…æ—¶"""
        try:
            future = task_info['future']
            task_execution = task_info['task_execution']
            elapsed_time = (datetime.now() - task_info['start_time']).total_seconds()
            
            logger.error(f"â° å¼ºåˆ¶å–æ¶ˆè¶…æ—¶ä»»åŠ¡ {task_id} (è¿è¡Œæ—¶é—´: {elapsed_time:.1f}s)")
            
            # å°è¯•å–æ¶ˆä»»åŠ¡
            future.cancel()
            
            # ä»è¿è¡Œä»»åŠ¡åˆ—è¡¨ä¸­ç§»é™¤
            with self.lock:
                self.running_tasks.pop(task_id, None)
                
            # åˆ›å»ºè¶…æ—¶é”™è¯¯æ¶ˆæ¯
            timeout_error = f"ä»»åŠ¡æ‰§è¡Œè¶…æ—¶ ({elapsed_time:.1f}s > {self.task_timeout_seconds}s)"
            
            # ä½¿ç”¨é”™è¯¯åˆ†ç±»å™¨å¤„ç†è¶…æ—¶é”™è¯¯
            session = self.db_manager.get_session()
            try:
                task_repo = PublishingTaskRepository(session)
                log_repo = PublishingLogRepository(session)
                
                # å¤„ç†è¶…æ—¶å¤±è´¥
                self._handle_task_failure(task_execution, timeout_error, task_repo, log_repo)
                session.commit()
                
            except Exception as db_error:
                logger.error(f"å¤„ç†è¶…æ—¶ä»»åŠ¡æ•°æ®åº“æ“ä½œå¤±è´¥: {db_error}")
                session.rollback()
            finally:
                self.db_manager.remove_session()
                
        except Exception as e:
            logger.error(f"å¤„ç†ä»»åŠ¡è¶…æ—¶å¼‚å¸¸: {e}")
    
    def _execute_task_with_timeout(self, task_execution: TaskExecution) -> Dict[str, Any]:
        """â° å¸¦è¶…æ—¶ä¿æŠ¤çš„ä»»åŠ¡æ‰§è¡ŒåŒ…è£…å™¨"""
        task_id = task_execution.task_id
        
        try:
            # ä½¿ç”¨è¶…æ—¶æ‰§è¡Œä»»åŠ¡
            logger.info(f"â° å¼€å§‹æ‰§è¡Œä»»åŠ¡ {task_id} (è¶…æ—¶é™åˆ¶: {self.task_timeout_seconds}s)")
            result = self._execute_task(task_execution)
            
            # æ¸…ç†è¿è¡ŒçŠ¶æ€
            with self.lock:
                self.running_tasks.pop(task_id, None)
                
            return result
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"â° ä»»åŠ¡ {task_id} æ‰§è¡Œå¼‚å¸¸: {error_msg}")
            
            # æ¸…ç†è¿è¡ŒçŠ¶æ€
            with self.lock:
                self.running_tasks.pop(task_id, None)
                
            return {
                'success': False,
                'task_id': task_id,
                'error': error_msg,
                'timeout_protected': True
            }
                
    def _execute_task(self, task_execution: TaskExecution, task_repo: PublishingTaskRepository, 
                      log_repo: PublishingLogRepository, content_source_repo: ContentSourceRepository) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªä»»åŠ¡ï¼ˆä½¿ç”¨æä¾›çš„ä»“åº“å®ä¾‹ï¼‰
        
        Args:
            task_execution: ä»»åŠ¡æ‰§è¡Œä¿¡æ¯
            task_repo: ä»»åŠ¡ä»“åº“å®ä¾‹
            log_repo: æ—¥å¿—ä»“åº“å®ä¾‹
            content_source_repo: å†…å®¹æºä»“åº“å®ä¾‹
            
        Returns:
            æ‰§è¡Œç»“æœ
        """
        task_id = task_execution.task_id
        start_time = time.time()
        
        try:
            # è·å–ä»»åŠ¡è¯¦æƒ…
            task = task_repo.get_by_id(task_id)
            
            if not task:
                raise ValueError(f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
            
            # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€
            if task.status not in ['pending', 'retry']:
                raise ValueError(f"ä»»åŠ¡ {task_id} çŠ¶æ€ä¸æ­£ç¡®: {task.status}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
            task_repo.update(task_id, {'status': TaskStatus.RUNNING.value})
            
            # è®°å½•å¼€å§‹æ‰§è¡Œæ—¥å¿—
            log_repo.create_log(
                task_id=task_id,
                status="running"
            )
            
            # è·å–å†…å®¹æºä¿¡æ¯
            logger.info(f"[SCHEDULER_DEBUG] è·å–å†…å®¹æºä¿¡æ¯ï¼Œsource_id: {task.source_id}")
            content_source = content_source_repo.get_source_by_id(task.source_id)
            logger.info(f"[SCHEDULER_DEBUG] å†…å®¹æºä¿¡æ¯: {content_source}")
            if not content_source:
                logger.error(f"[SCHEDULER_DEBUG] å†…å®¹æº {task.source_id} ä¸å­˜åœ¨")
                raise ValueError(f"å†…å®¹æº {task.source_id} ä¸å­˜åœ¨")
            
            # ä½¿ç”¨åŠ¨æ€è·¯å¾„ç®¡ç†å™¨å¤„ç†åª’ä½“æ–‡ä»¶è·¯å¾„
            import os
            logger.info(f"[SCHEDULER_DEBUG] åŸå§‹åª’ä½“è·¯å¾„: {task.media_path}")
            
            # éªŒè¯åª’ä½“æ–‡ä»¶
            validation_result = self.dynamic_path_manager.validate_media_file(task.media_path)
            logger.info(f"[SCHEDULER_DEBUG] åª’ä½“æ–‡ä»¶éªŒè¯ç»“æœ: {validation_result}")
            
            if validation_result['is_hardcoded']:
                logger.warning(f"[SCHEDULER_DEBUG] æ£€æµ‹åˆ°ç¡¬ç¼–ç è·¯å¾„: {task.media_path}")
                logger.info(f"[SCHEDULER_DEBUG] è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„: {validation_result['converted_path']}")
            
            if not validation_result['exists']:
                error_msg = f"åª’ä½“æ–‡ä»¶ä¸å­˜åœ¨: {validation_result['resolved_path']} (åŸè·¯å¾„: {task.media_path})"
                if validation_result['error']:
                    error_msg += f", é”™è¯¯: {validation_result['error']}"
                
                logger.error(f"[SCHEDULER_DEBUG] {error_msg}")
                
                # å°è¯•é€šè¿‡æ–‡ä»¶åæŸ¥æ‰¾åª’ä½“æ–‡ä»¶
                filename = os.path.basename(task.media_path)
                found_file = self.dynamic_path_manager.find_media_file(filename)
                
                if found_file:
                    logger.info(f"[SCHEDULER_DEBUG] é€šè¿‡æ–‡ä»¶åæ‰¾åˆ°åª’ä½“æ–‡ä»¶: {found_file}")
                    media_file_path = found_file
                    media_file = str(media_file_path)
                else:
                    # ç«‹å³åœæ­¢ä»»åŠ¡æµç¨‹å¹¶è®°å½•ä¸ºå¤±è´¥
                    task_repo.update(task_id, {
                        'status': TaskStatus.FAILED.value,
                        'updated_at': datetime.now()
                    })
                    
                    # è®°å½•å¤±è´¥æ—¥å¿—
                    log_repo.create_log(
                        task_id=task_id,
                        status="failed",
                        error_message=error_msg,
                        duration_seconds=time.time() - start_time
                    )
                    
                    raise FileNotFoundError(error_msg)
            else:
                # ä½¿ç”¨éªŒè¯é€šè¿‡çš„è·¯å¾„
                media_file_path = self.dynamic_path_manager.resolve_media_path(task.media_path)
                media_file = str(media_file_path)
                logger.info(f"[SCHEDULER_DEBUG] è§£æååª’ä½“è·¯å¾„: {media_file}")
            
            # æŸ¥æ‰¾å¯¹åº”çš„JSONå…ƒæ•°æ®æ–‡ä»¶
            media_dir = os.path.dirname(media_file)
            media_name = os.path.splitext(os.path.basename(media_file))[0]
            metadata_file = os.path.join(media_dir, f"{media_name}.json")
            logger.info(f"[SCHEDULER_DEBUG] æŸ¥æ‰¾å…ƒæ•°æ®æ–‡ä»¶: {metadata_file}")
            
            # å¦‚æœå•ç‹¬çš„JSONæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾outputsç›®å½•ä¸­çš„ç»¼åˆæŠ¥å‘Šæ–‡ä»¶
            if not os.path.exists(metadata_file):
                logger.info(f"[SCHEDULER_DEBUG] å•ç‹¬JSONæ–‡ä»¶ä¸å­˜åœ¨ï¼ŒæŸ¥æ‰¾outputsç›®å½•")
                outputs_dir = os.path.join(media_dir, "outputs")
                logger.info(f"[SCHEDULER_DEBUG] outputsç›®å½•: {outputs_dir}")
                if os.path.exists(outputs_dir):
                    # æŸ¥æ‰¾stage3_final_reportæ–‡ä»¶
                    for file in os.listdir(outputs_dir):
                        if file.startswith("stage3_final_report") and file.endswith(".json"):
                            metadata_file = os.path.join(outputs_dir, file)
                            logger.info(f"[SCHEDULER_DEBUG] æ‰¾åˆ°stage3æŠ¥å‘Šæ–‡ä»¶: {metadata_file}")
                            break
            
            # å¦‚æœoutputsç›®å½•ä¸­ä¹Ÿæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾uploader_jsonç›®å½•ä¸­çš„en_prompt_resultsæ–‡ä»¶
            if not os.path.exists(metadata_file):
                logger.info(f"[SCHEDULER_DEBUG] outputsç›®å½•ä¸­æœªæ‰¾åˆ°ï¼ŒæŸ¥æ‰¾uploader_jsonç›®å½•")
                # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆmedia_dirçš„ä¸Šçº§ç›®å½•ï¼‰
                project_dir = os.path.dirname(media_dir)
                uploader_json_dir = os.path.join(project_dir, "uploader_json")
                logger.info(f"[SCHEDULER_DEBUG] uploader_jsonç›®å½•: {uploader_json_dir}")
                if os.path.exists(uploader_json_dir):
                    # æŸ¥æ‰¾en_prompt_resultsæ–‡ä»¶
                    for file in os.listdir(uploader_json_dir):
                        if file.startswith("en_prompt_results") and file.endswith(".json"):
                            metadata_file = os.path.join(uploader_json_dir, file)
                            logger.info(f"[SCHEDULER_DEBUG] æ‰¾åˆ°en_prompt_resultsæ–‡ä»¶: {metadata_file}")
                            break
            
            if not os.path.exists(metadata_file):
                logger.error(f"[SCHEDULER_DEBUG] å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
                raise FileNotFoundError(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
            
            logger.info(f"[SCHEDULER_DEBUG] æœ€ç»ˆä½¿ç”¨çš„å…ƒæ•°æ®æ–‡ä»¶: {metadata_file}")
            
            # ç”Ÿæˆå†…å®¹
            logger.info(f"[SCHEDULER_DEBUG] ä»»åŠ¡ {task_id} å¼€å§‹ç”Ÿæˆå†…å®¹")
            logger.info(f"[SCHEDULER_DEBUG] åª’ä½“è·¯å¾„: {task.media_path}")
            logger.info(f"[SCHEDULER_DEBUG] å…ƒæ•°æ®æ–‡ä»¶: {metadata_file}")
            logger.info(f"[SCHEDULER_DEBUG] è§†é¢‘æ–‡ä»¶å: {os.path.basename(media_file)}")
            
            content_result = self.content_generator.generate_content(
                video_filename=os.path.basename(media_file),
                metadata_path=metadata_file,
                language='en'  # æ˜ç¡®æŒ‡å®šä½¿ç”¨è‹±æ–‡
            )
            
            logger.info(f"[SCHEDULER_DEBUG] å†…å®¹ç”Ÿæˆå™¨è¿”å›ç»“æœ: {content_result}")
            
            # åŒ…è£…è¿”å›ç»“æœä»¥ä¿æŒå…¼å®¹æ€§
            if content_result:
                content_result = {
                    'success': True,
                    'content': content_result,
                    'message': 'å†…å®¹ç”ŸæˆæˆåŠŸ'
                }
            else:
                content_result = {
                    'success': False,
                    'content': None,
                    'message': 'å†…å®¹ç”Ÿæˆå¤±è´¥'
                }
            
            logger.info(f"[SCHEDULER_DEBUG] ä»»åŠ¡ {task_id} å†…å®¹ç”Ÿæˆå®Œæˆï¼Œç»“æœ: {content_result.get('success', False)}")
            logger.info(f"[SCHEDULER_DEBUG] ç”Ÿæˆçš„å†…å®¹: {content_result.get('content')}")
            
            if not content_result['success']:
                logger.error(f"[SCHEDULER_DEBUG] å†…å®¹ç”Ÿæˆå¤±è´¥: {content_result['message']}")
                raise Exception(f"å†…å®¹ç”Ÿæˆå¤±è´¥: {content_result['message']}")
                
            # å‘å¸ƒåˆ°Twitter
            logger.info(f"[SCHEDULER_DEBUG] ä»»åŠ¡ {task_id} å¼€å§‹å‘å¸ƒåˆ°Twitter")
            logger.info(f"[SCHEDULER_DEBUG] å‘å¸ƒå†…å®¹: {content_result['content']}")
            logger.info(f"[SCHEDULER_DEBUG] å‘å¸ƒåª’ä½“è·¯å¾„: {task.media_path}")
            
            publish_result = self._publish_to_twitter(
                content_result['content'],
                task.media_path
            )
            
            logger.info(f"[SCHEDULER_DEBUG] ä»»åŠ¡ {task_id} å‘å¸ƒå®Œæˆï¼Œç»“æœ: {publish_result.get('success', False)}")
            logger.info(f"[SCHEDULER_DEBUG] å‘å¸ƒç»“æœè¯¦æƒ…: {publish_result}")
            
            if not publish_result['success']:
                logger.error(f"[SCHEDULER_DEBUG] å‘å¸ƒå¤±è´¥: {publish_result['message']}")
                raise Exception(f"å‘å¸ƒå¤±è´¥: {publish_result['message']}")
                
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            task_repo.update(task_id, {
                'status': TaskStatus.COMPLETED.value,
                'updated_at': datetime.now()
            })
            
            # è®°å½•æˆåŠŸæ—¥å¿—
            log_repo.create_log(
                task_id=task_id,
                status="success",
                tweet_id=publish_result.get('tweet_id'),
                duration_seconds=time.time() - start_time
            )
            
            # æ›´æ–°ç»Ÿè®¡
            with self.lock:
                self.stats['total_processed'] += 1
                self.stats['successful'] += 1
                
            logger.info(f"ä»»åŠ¡ {task_id} æ‰§è¡ŒæˆåŠŸ")
            
            return {
                'success': True,
                'task_id': task_id,
                'execution_time': time.time() - start_time
            }
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {error_msg}")
            
            # å¤„ç†é‡è¯•é€»è¾‘
            should_retry = self._handle_task_failure(
                task_execution, error_msg, task_repo, log_repo
            )
            
            # æ›´æ–°ç»Ÿè®¡
            with self.lock:
                self.stats['total_processed'] += 1
                if should_retry:
                    self.stats['retried'] += 1
                else:
                    self.stats['failed'] += 1
                    
            return {
                'success': False,
                'task_id': task_id,
                'error': error_msg,
                'retry_scheduled': should_retry
            }
            
        finally:
            # æ¸…ç†è¿è¡ŒçŠ¶æ€
            with self.lock:
                self.running_tasks.pop(task_id, None)
                
    def _handle_task_failure(self, task_execution: TaskExecution, error_msg: str, 
                           task_repo: PublishingTaskRepository, log_repo: PublishingLogRepository) -> bool:
        """
        ğŸ¯ Phase 3: æ™ºèƒ½é”™è¯¯å¤„ç† - ä½¿ç”¨é”™è¯¯åˆ†ç±»å™¨å†³å®šé‡è¯•ç­–ç•¥
        
        Args:
            task_execution: ä»»åŠ¡æ‰§è¡Œä¿¡æ¯
            error_msg: é”™è¯¯æ¶ˆæ¯
            task_repo: ä»»åŠ¡ä»“åº“å®ä¾‹
            log_repo: æ—¥å¿—ä»“åº“å®ä¾‹
            
        Returns:
            æ˜¯å¦å®‰æ’äº†é‡è¯•
        """
        task_id = task_execution.task_id
        
        try:
            # ğŸ§  ä½¿ç”¨æ™ºèƒ½é”™è¯¯åˆ†ç±»å™¨åˆ†æé”™è¯¯
            error_analysis = classify_and_handle_error(error_msg, task_execution.retry_count + 1)
            error_type = error_analysis['error_type']
            should_retry = error_analysis['should_retry']
            retry_delay = error_analysis['retry_delay']
            needs_human = error_analysis['needs_human_intervention']
            
            logger.info(f"ğŸ” ä»»åŠ¡ {task_id} é”™è¯¯åˆ†æç»“æœ:")
            logger.info(f"  - é”™è¯¯ç±»å‹: {error_type}")
            logger.info(f"  - åº”è¯¥é‡è¯•: {should_retry}")
            logger.info(f"  - é‡è¯•å»¶è¿Ÿ: {retry_delay}ç§’")
            logger.info(f"  - éœ€è¦äººå·¥ä»‹å…¥: {needs_human}")
            
            if needs_human:
                logger.warning(f"âš ï¸ ä»»åŠ¡ {task_id} éœ€è¦äººå·¥ä»‹å…¥: {error_type}")
                # æ ‡è®°ä¸ºéœ€è¦äººå·¥ä»‹å…¥çš„å¤±è´¥
                task_repo.update(task_id, {
                    'status': 'failed',
                    'error_type': error_type,
                    'updated_at': datetime.now()
                })
                
                log_repo.create_log(
                    task_id=task_id,
                    status="failed",
                    error_message=f"[{error_type}] {error_msg} - éœ€è¦äººå·¥ä»‹å…¥"
                )
                
                return False
                
            if not should_retry or retry_delay is None:
                logger.info(f"âŒ ä»»åŠ¡ {task_id} è¾¾åˆ°é‡è¯•é™åˆ¶æˆ–ä¸é€‚åˆé‡è¯•")
                # æ ‡è®°ä¸ºæœ€ç»ˆå¤±è´¥
                task_repo.update(task_id, {
                    'status': 'failed',
                    'error_type': error_type,
                    'updated_at': datetime.now()
                })
                
                log_repo.create_log(
                    task_id=task_id,
                    status="failed",
                    error_message=f"[{error_type}] {error_msg}"
                )
                
                return False
                
            # ğŸ”„ å®‰æ’æ™ºèƒ½é‡è¯•
            logger.info(f"ğŸ”„ ä»»åŠ¡ {task_id} å°†åœ¨ {retry_delay} ç§’åé‡è¯• (ç±»å‹: {error_type})")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºé‡è¯•ä¸­
            task_repo.update(task_id, {
                'status': 'retry',
                'error_type': error_type,
                'updated_at': datetime.now()
            })
            
            # è®°å½•æ™ºèƒ½é‡è¯•æ—¥å¿—
            log_repo.create_log(
                task_id=task_id,
                status="retry",
                error_message=f"[{error_type}] æ™ºèƒ½é‡è¯• #{task_execution.retry_count + 1}: {error_msg}"
            )
            
            # å®‰æ’é‡è¯•ä»»åŠ¡
            retry_execution = TaskExecution(
                task_id=task_id,
                priority=task_execution.priority,
                scheduled_time=datetime.now() + timedelta(seconds=retry_delay),
                retry_count=task_execution.retry_count + 1,
                last_error=error_msg
            )
            
            self.task_queue.put(retry_execution)
            
            return True
            
        except Exception as e:
            logger.error(f"ğŸ”¥ æ™ºèƒ½é”™è¯¯å¤„ç†å¼‚å¸¸: {e}")
            # å›é€€åˆ°ç®€å•é‡è¯•é€»è¾‘
            return self._fallback_retry_logic(task_execution, error_msg, task_repo, log_repo)
    
    def optimize_task_timing(self, task: PublishingTask) -> datetime:
        """
        ğŸ“… Phase 3.5: ä½¿ç”¨æ—¶é—´é¢„æµ‹å™¨ä¼˜åŒ–ä»»åŠ¡æ‰§è¡Œæ—¶é—´
        
        Args:
            task: å‘å¸ƒä»»åŠ¡
            
        Returns:
            datetime: ä¼˜åŒ–åçš„æ‰§è¡Œæ—¶é—´
        """
        try:
            # è·å–ä»»åŠ¡å±æ€§
            content_type = getattr(task, 'content_type', 'normal')
            project_priority = getattr(task, 'project_priority', 3)
            
            # è®¡ç®—æœ€å°å»¶è¿Ÿï¼ˆé¿å…è¿‡äºé¢‘ç¹å‘å¸ƒï¼‰
            last_publish_time = self._get_last_publish_time(task.project_id)
            min_delay_minutes = 30  # é»˜è®¤30åˆ†é’Ÿé—´éš”
            
            if last_publish_time:
                time_since_last = (datetime.now() - last_publish_time).total_seconds() / 60
                if time_since_last < 180:  # 3å°æ—¶å†…æœ‰å‘å¸ƒ
                    min_delay_minutes = max(30, 180 - int(time_since_last))
                    
            # ä½¿ç”¨æ—¶é—´é¢„æµ‹å™¨é¢„æµ‹æœ€ä½³æ—¶é—´
            prediction = self.timing_predictor.predict_optimal_time(
                content_type=content_type,
                project_priority=project_priority,
                min_delay_minutes=min_delay_minutes,
                max_delay_hours=24  # æœ€å¤šå»¶è¿Ÿ24å°æ—¶
            )
            
            logger.info(f"ğŸ“… ä»»åŠ¡ {task.id} æ—¶é—´ä¼˜åŒ–:")
            logger.info(f"  - åŸè®¡åˆ’æ—¶é—´: {task.scheduled_at}")
            logger.info(f"  - ä¼˜åŒ–åæ—¶é—´: {prediction.recommended_time}")
            logger.info(f"  - ç½®ä¿¡åº¦: {prediction.confidence_score:.2f}")
            logger.info(f"  - æ¨èç†ç”±: {prediction.reasoning}")
            
            # å¦‚æœæ˜¯é«˜ä¼˜å…ˆçº§ä»»åŠ¡ä¸”æ—¶é—´ç´§æ€¥ï¼Œè·³è¿‡ä¼˜åŒ–
            if project_priority >= 4 and task.scheduled_at:
                time_diff = (datetime.now() - task.scheduled_at).total_seconds() / 3600
                if abs(time_diff) <= 1:  # 1å°æ—¶å†…çš„ç´§æ€¥ä»»åŠ¡
                    logger.info(f"ğŸ“… ä»»åŠ¡ {task.id} ä¸ºç´§æ€¥ä»»åŠ¡ï¼Œè·³è¿‡æ—¶é—´ä¼˜åŒ–")
                    return task.scheduled_at or datetime.now()
                    
            return prediction.recommended_time
            
        except Exception as e:
            logger.error(f"ğŸ“… ä»»åŠ¡ {task.id} æ—¶é—´ä¼˜åŒ–å¤±è´¥: {e}")
            # å›é€€åˆ°åŸæ—¶é—´æˆ–å½“å‰æ—¶é—´
            return task.scheduled_at or datetime.now()
    
    def _get_last_publish_time(self, project_id: int) -> Optional[datetime]:
        """è·å–é¡¹ç›®æœ€åå‘å¸ƒæ—¶é—´"""
        try:
            session = self.db_manager.get_session()
            log_repo = PublishingLogRepository(session)
            
            # æŸ¥è¯¢æœ€è¿‘çš„æˆåŠŸå‘å¸ƒè®°å½•
            recent_log = log_repo.get_recent_successful_publish(project_id)
            if recent_log:
                return recent_log.published_at
                
        except Exception as e:
            logger.error(f"è·å–æœ€åå‘å¸ƒæ—¶é—´å¤±è´¥: {e}")
        finally:
            self.db_manager.remove_session()
            
        return None
    
    def schedule_task_with_timing_optimization(self, task_id: int, 
                                             priority: TaskPriority = TaskPriority.NORMAL) -> bool:
        """
        ğŸ“… å¸¦æ—¶é—´ä¼˜åŒ–çš„ä»»åŠ¡è°ƒåº¦
        
        Args:
            task_id: ä»»åŠ¡ID
            priority: ä»»åŠ¡ä¼˜å…ˆçº§
            
        Returns:
            æ˜¯å¦æˆåŠŸè°ƒåº¦
        """
        try:
            # è·å–ä»»åŠ¡ä¿¡æ¯
            session = self.db_manager.get_session()
            task_repo = PublishingTaskRepository(session)
            task = task_repo.get_by_id(task_id)
            
            if not task:
                logger.error(f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
                return False
                
            # ä¼˜åŒ–æ‰§è¡Œæ—¶é—´
            optimized_time = self.optimize_task_timing(task)
            
            # æ›´æ–°ä»»åŠ¡çš„è°ƒåº¦æ—¶é—´
            task_repo.update(task_id, {'scheduled_at': optimized_time})
            session.commit()
            
            # åˆ›å»ºä»»åŠ¡æ‰§è¡Œå¯¹è±¡
            task_execution = TaskExecution(
                task_id=task_id,
                priority=priority,
                scheduled_time=optimized_time
            )
            
            self.task_queue.put(task_execution)
            logger.info(f"ğŸ“… ä»»åŠ¡ {task_id} å·²ä¼˜åŒ–è°ƒåº¦è‡³ {optimized_time.strftime('%Y-%m-%d %H:%M')}")
            
            return True
            
        except Exception as e:
            logger.error(f"ğŸ“… å¸¦æ—¶é—´ä¼˜åŒ–çš„ä»»åŠ¡è°ƒåº¦å¤±è´¥: {e}")
            return False
        finally:
            self.db_manager.remove_session()
    
    def _fallback_retry_logic(self, task_execution: TaskExecution, error_msg: str,
                             task_repo: PublishingTaskRepository, log_repo: PublishingLogRepository) -> bool:
        """
        å›é€€é‡è¯•é€»è¾‘ - å½“é”™è¯¯åˆ†ç±»å™¨å¤±è´¥æ—¶ä½¿ç”¨
        
        Args:
            task_execution: ä»»åŠ¡æ‰§è¡Œä¿¡æ¯
            error_msg: é”™è¯¯æ¶ˆæ¯
            task_repo: ä»»åŠ¡ä»“åº“å®ä¾‹
            log_repo: æ—¥å¿—ä»“åº“å®ä¾‹
            
        Returns:
            æ˜¯å¦å®‰æ’äº†é‡è¯•
        """
        task_id = task_execution.task_id
        
        logger.warning(f"ğŸ”„ ä»»åŠ¡ {task_id} ä½¿ç”¨ç®€å•é‡è¯•é€»è¾‘")
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
        if task_execution.retry_count >= self.max_retries:
            # æ ‡è®°ä¸ºæœ€ç»ˆå¤±è´¥
            task_repo.update(task_id, {
                'status': 'failed',
                'updated_at': datetime.now()
            })
            
            log_repo.create_log(
                task_id=task_id,
                status="failed",
                error_message=error_msg
            )
            
            return False
            
        # è®¡ç®—é‡è¯•å»¶è¿Ÿ - ç®€å•æŒ‡æ•°é€€é¿
        retry_delay = min(60 * (2 ** task_execution.retry_count), 300)  # æœ€å¤§5åˆ†é’Ÿ
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºé‡è¯•ä¸­
        task_repo.update(task_id, {
            'status': 'retry',
            'updated_at': datetime.now()
        })
        
        # è®°å½•é‡è¯•æ—¥å¿—
        log_repo.create_log(
            task_id=task_id,
            status="retry",
            error_message=f"ç®€å•é‡è¯• #{task_execution.retry_count + 1}: {error_msg}"
        )
        
        # å®‰æ’é‡è¯•
        retry_execution = TaskExecution(
            task_id=task_id,
            priority=task_execution.priority,
            scheduled_time=datetime.now() + timedelta(seconds=retry_delay),
            retry_count=task_execution.retry_count + 1,
            last_error=error_msg
        )
        
        self.task_queue.put(retry_execution)
        
        return True
            
    def _determine_task_priority(self, task: PublishingTask) -> TaskPriority:
        """
        ğŸ¯ Phase 3.4: ä½¿ç”¨æ™ºèƒ½ä¼˜å…ˆçº§æƒé‡ç®—æ³•ç¡®å®šä»»åŠ¡ä¼˜å…ˆçº§
        
        Args:
            task: å‘å¸ƒä»»åŠ¡
            
        Returns:
            ä»»åŠ¡ä¼˜å…ˆçº§
        """
        try:
            # æ„å»ºä»»åŠ¡æ•°æ®å­—å…¸
            task_data = {
                'created_at': task.created_at,
                'scheduled_at': task.scheduled_at,
                'retry_count': task.retry_count or 0,
                'project_id': task.project_id,
                'project_priority': getattr(task, 'project_priority', 3),  # é»˜è®¤ä¸­ç­‰ä¼˜å…ˆçº§
                'content_type': getattr(task, 'content_type', 'normal')
            }
            
            # ä½¿ç”¨ä¼˜å…ˆçº§è®¡ç®—å™¨è®¡ç®—å¾—åˆ†
            priority_score = self.priority_calculator.calculate_priority_score(task_data)
            priority_level = get_priority_level(priority_score)
            
            # æ ¹æ®å¾—åˆ†æ˜ å°„åˆ°TaskPriorityæšä¸¾
            if priority_score >= 80:
                task_priority = TaskPriority.URGENT
            elif priority_score >= 60:
                task_priority = TaskPriority.HIGH
            elif priority_score >= 40:
                task_priority = TaskPriority.NORMAL
            else:
                task_priority = TaskPriority.LOW
                
            logger.debug(f"ğŸ¯ ä»»åŠ¡ {task.id} ä¼˜å…ˆçº§è®¡ç®—: å¾—åˆ†={priority_score:.2f}, çº§åˆ«={priority_level}, æšä¸¾={task_priority.name}")
            
            return task_priority
            
        except Exception as e:
            logger.error(f"ğŸ¯ ä»»åŠ¡ {task.id} ä¼˜å…ˆçº§è®¡ç®—å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•é€»è¾‘
            return self._simple_priority_fallback(task)
    
    def _simple_priority_fallback(self, task: PublishingTask) -> TaskPriority:
        """ç®€å•ä¼˜å…ˆçº§å›é€€é€»è¾‘"""
        # æ ¹æ®é‡è¯•æ¬¡æ•°è°ƒæ•´ä¼˜å…ˆçº§
        if task.retry_count > 2:
            return TaskPriority.LOW
        elif task.retry_count > 0:
            return TaskPriority.NORMAL
            
        # æ ¹æ®åˆ›å»ºæ—¶é—´è°ƒæ•´ä¼˜å…ˆçº§
        age_hours = (datetime.now() - task.created_at).total_seconds() / 3600
        if age_hours > 24:
            return TaskPriority.HIGH
        elif age_hours > 12:
            return TaskPriority.NORMAL
            
        return TaskPriority.NORMAL
        
    def _check_stuck_tasks(self):
        """æ£€æŸ¥å¹¶æ¢å¤å¡ä½çš„ä»»åŠ¡ï¼ˆåœ¨ç‹¬ç«‹çš„ä¼šè¯ä¸­è¿è¡Œï¼‰"""
        session = self.db_manager.get_session()
        try:
            task_repo = PublishingTaskRepository(session)
            log_repo = PublishingLogRepository(session)

            stuck_tasks = task_repo.get_stuck_tasks(
                self.stuck_task_timeout
            )

            if stuck_tasks:
                with self.db_write_lock:
                    for task in stuck_tasks:
                        logger.warning(f"å‘ç°å¡ä½çš„ä»»åŠ¡ {task.id}ï¼Œæ­£åœ¨æ¢å¤...")
                        task_repo.update(task.id, {'status': TaskStatus.PENDING.value})
                        log_repo.create_log(
                            task_id=task.id,
                            status="stuck_recovered"
                        )
                    session.commit()
        except Exception as e:
            logger.error(f"æ£€æŸ¥å¡ä½ä»»åŠ¡æ—¶å‡ºé”™: {e}", exc_info=True)
            with self.db_write_lock:
                session.rollback()
        finally:
            self.db_manager.remove_session()

    def _execute_task(self, task_execution: TaskExecution) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•ä¸ªä»»åŠ¡
        
        Args:
            task_execution: ä»»åŠ¡æ‰§è¡Œä¿¡æ¯
            
        Returns:
            æ‰§è¡Œç»“æœ
        """
        task_id = task_execution.task_id
        start_time = time.time()
        
        # è¯¦ç»†è°ƒè¯•æ—¥å¿—
        logger.info(f"[SCHEDULER_DEBUG] å¼€å§‹æ‰§è¡Œä»»åŠ¡ {task_id}")
        logger.info(f"[SCHEDULER_DEBUG] ä»»åŠ¡æ‰§è¡Œä¿¡æ¯: {task_execution}")
        
        session = self.db_manager.get_session()
        task_repo = PublishingTaskRepository(session)
        log_repo = PublishingLogRepository(session)
        content_source_repo = ContentSourceRepository(session)

        try:
            with self.db_write_lock:
                # è·å–ä»»åŠ¡è¯¦æƒ…
                task = task_repo.get_by_id(task_id)
                logger.info(f"[SCHEDULER_DEBUG] è·å–åˆ°ä»»åŠ¡: {task}")
                
                if not task:
                    logger.error(f"[SCHEDULER_DEBUG] ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
                    raise ValueError(f"ä»»åŠ¡ {task_id} ä¸å­˜åœ¨")
                
                logger.info(f"[SCHEDULER_DEBUG] ä»»åŠ¡çŠ¶æ€: {task.status}")
                logger.info(f"[SCHEDULER_DEBUG] ä»»åŠ¡å†…å®¹æ•°æ®: {task.content_data}")
                logger.info(f"[SCHEDULER_DEBUG] ä»»åŠ¡åª’ä½“è·¯å¾„: {task.media_path}")
                
                # ğŸ› ï¸ æ™ºèƒ½ä»»åŠ¡çŠ¶æ€æ£€æŸ¥å’Œæ¢å¤
                if task.status not in ['pending', 'retry']:
                    if task.status == 'running':
                        # æ£€æŸ¥æ˜¯å¦æ˜¯å¡ä½çš„ä»»åŠ¡ - å¦‚æœè¿è¡Œæ—¶é—´è¶…è¿‡é˜ˆå€¼ï¼Œé‡ç½®çŠ¶æ€
                        task_stuck_timeout = self.config.get('task.stuck_timeout', 300)  # 5åˆ†é’Ÿ
                        
                        if task.updated_at:
                            time_since_update = (datetime.now() - task.updated_at).total_seconds()
                            if time_since_update > task_stuck_timeout:
                                logger.warning(f"[SCHEDULER_DEBUG] ä»»åŠ¡ {task_id} å·²è¿è¡Œ{time_since_update:.0f}ç§’ï¼Œè¶…è¿‡é˜ˆå€¼{task_stuck_timeout}ç§’ï¼Œé‡ç½®ä¸ºå¾…æ‰§è¡ŒçŠ¶æ€")
                                task_repo.update(task_id, {'status': 'pending'})
                                session.commit()
                            else:
                                logger.info(f"[SCHEDULER_DEBUG] ä»»åŠ¡ {task_id} æ­£åœ¨è¿è¡Œä¸­(å·²è¿è¡Œ{time_since_update:.0f}ç§’)ï¼Œè·³è¿‡æ‰§è¡Œ")
                                return {'success': False, 'reason': 'task_already_running', 'running_time': time_since_update}
                        else:
                            # å¦‚æœæ²¡æœ‰æ›´æ–°æ—¶é—´ï¼Œç›´æ¥é‡ç½®ä¸ºå¾…æ‰§è¡Œ
                            logger.warning(f"[SCHEDULER_DEBUG] ä»»åŠ¡ {task_id} çŠ¶æ€ä¸ºrunningä½†æ— æ›´æ–°æ—¶é—´ï¼Œé‡ç½®ä¸ºå¾…æ‰§è¡ŒçŠ¶æ€")
                            task_repo.update(task_id, {'status': 'pending'})
                            session.commit()
                    else:
                        logger.error(f"[SCHEDULER_DEBUG] ä»»åŠ¡ {task_id} çŠ¶æ€ä¸æ­£ç¡®: {task.status}")
                        raise ValueError(f"ä»»åŠ¡ {task_id} çŠ¶æ€ä¸æ­£ç¡®: {task.status}")
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
                logger.info(f"[SCHEDULER_DEBUG] æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­")
                task_repo.update(task_id, {'status': 'running'})
                session.commit()

            # ... aqiure lock ...

            # è®°å½•å¼€å§‹æ‰§è¡Œæ—¥å¿—
            # log_repo.create_log(
            #     task_id=task_id,
            #     status="running"
            # )
            
            # è·å–å†…å®¹æºä¿¡æ¯
            content_source = content_source_repo.get_source_by_id(task.source_id)
            if not content_source:
                raise ValueError(f"å†…å®¹æº {task.source_id} ä¸å­˜åœ¨")
            
            # æ„é€ å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
            import os
            # ä½¿ç”¨è·¯å¾„ç®¡ç†å™¨æ ‡å‡†åŒ–åª’ä½“æ–‡ä»¶è·¯å¾„
            media_file_path = self.path_manager.normalize_path(task.media_path)
            media_file = str(media_file_path)
            
            if not media_file_path.exists():
                raise FileNotFoundError(f"åª’ä½“æ–‡ä»¶ä¸å­˜åœ¨: {media_file} (åŸè·¯å¾„: {task.media_path})")
            
            # æŸ¥æ‰¾å¯¹åº”çš„JSONå…ƒæ•°æ®æ–‡ä»¶
            media_dir = os.path.dirname(media_file)
            media_name = os.path.splitext(os.path.basename(media_file))[0]
            metadata_file = os.path.join(media_dir, f"{media_name}.json")
            
            # å¦‚æœå•ç‹¬çš„JSONæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾outputsç›®å½•ä¸­çš„ç»¼åˆæŠ¥å‘Šæ–‡ä»¶
            if not os.path.exists(metadata_file):
                outputs_dir = os.path.join(media_dir, "outputs")
                if os.path.exists(outputs_dir):
                    # æŸ¥æ‰¾stage3_final_reportæ–‡ä»¶
                    for file in os.listdir(outputs_dir):
                        if file.startswith("stage3_final_report") and file.endswith(".json"):
                            metadata_file = os.path.join(outputs_dir, file)
                            break
            
            # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾uploader_jsonç›®å½•ä¸­çš„en_prompt_resultsæ–‡ä»¶
            if not os.path.exists(metadata_file):
                # è·å–é¡¹ç›®æ ¹ç›®å½•
                project_root = os.path.dirname(media_dir)
                uploader_json_dir = os.path.join(project_root, "uploader_json")
                if os.path.exists(uploader_json_dir):
                    # æŸ¥æ‰¾en_prompt_resultsæ–‡ä»¶
                    for file in os.listdir(uploader_json_dir):
                        if file.startswith("en_prompt_results") and file.endswith(".json"):
                            metadata_file = os.path.join(uploader_json_dir, file)
                            break
            
            if not os.path.exists(metadata_file):
                raise FileNotFoundError(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_file}")
            
            # ç”Ÿæˆå†…å®¹
            logger.info(f"ä»»åŠ¡ {task_id} å¼€å§‹ç”Ÿæˆå†…å®¹ï¼Œåª’ä½“è·¯å¾„: {task.media_path}ï¼Œå…ƒæ•°æ®: {metadata_file}")
            content_result = self.content_generator.generate_content(
                video_filename=os.path.basename(media_file),
                metadata_path=metadata_file,
                language='en'  # æ˜ç¡®æŒ‡å®šä½¿ç”¨è‹±æ–‡
            )
            
            # åŒ…è£…è¿”å›ç»“æœä»¥ä¿æŒå…¼å®¹æ€§
            if content_result:
                content_result = {
                    'success': True,
                    'content': content_result,
                    'message': 'å†…å®¹ç”ŸæˆæˆåŠŸ'
                }
            else:
                content_result = {
                    'success': False,
                    'content': None,
                    'message': 'å†…å®¹ç”Ÿæˆå¤±è´¥'
                }
            
            logger.info(f"ä»»åŠ¡ {task_id} å†…å®¹ç”Ÿæˆå®Œæˆï¼Œç»“æœ: {content_result.get('success', False)}")
            
            if not content_result['success']:
                raise Exception(f"å†…å®¹ç”Ÿæˆå¤±è´¥: {content_result['message']}")
                
            # å‘å¸ƒåˆ°Twitter
            logger.info(f"ä»»åŠ¡ {task_id} å¼€å§‹å‘å¸ƒåˆ°Twitter")
            publish_result = self._publish_to_twitter(
                content_result['content'],
                task.media_path
            )
            logger.info(f"ä»»åŠ¡ {task_id} å‘å¸ƒå®Œæˆï¼Œç»“æœ: {publish_result.get('success', False)}")
            
            if not publish_result['success']:
                raise Exception(f"å‘å¸ƒå¤±è´¥: {publish_result['message']}")
            
            with self.db_write_lock:
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
                task_repo.update(task_id, {
                    'status': 'completed',
                    'updated_at': datetime.now()
                })
                
                # è®°å½•æˆåŠŸæ—¥å¿—
                log_repo.create_log(
                    task_id=task_id,
                    status="success",
                    tweet_id=publish_result.get('tweet_id'),
                    tweet_content=publish_result.get('tweet_text'),
                    duration_seconds=time.time() - start_time
                )
                session.commit()

            # æ›´æ–°ç»Ÿè®¡
            with self.lock:
                self.stats['total_processed'] += 1
                self.stats['successful'] += 1
                
            logger.info(f"ä»»åŠ¡ {task_id} æ‰§è¡ŒæˆåŠŸ")
            
            return {
                'success': True,
                'task_id': task_id,
                'execution_time': time.time() - start_time
            }
            
        except Exception as e:
            error_msg = str(e)
            logger.error(f"ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {error_msg}")
            
            try:
                with self.db_write_lock:
                    session.rollback()
                    # å¤„ç†é‡è¯•é€»è¾‘
                    task = task_repo.get_by_id(task_id)
                    if task and task.retry_count < self.max_retries:
                        # æ ‡è®°ä¸ºé‡è¯•
                        task_repo.update(task_id, {
                            'status': 'retry',
                            'retry_count': task.retry_count + 1
                        })
                        logger.info(f"ä»»åŠ¡ {task_id} å°†åœ¨ç¨åé‡è¯• (å°è¯•æ¬¡æ•°: {task.retry_count + 1})")
                    else:
                        # æ ‡è®°ä¸ºå¤±è´¥
                        task_repo.update(task_id, {'status': 'failed'})
                        logger.error(f"ä»»åŠ¡ {task_id} å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ ‡è®°ä¸ºå¤±è´¥")
                    
                    # è®°å½•å¤±è´¥æ—¥å¿—
                    log_repo.create_log(
                        task_id=task_id,
                        status="failed",
                        error_message=error_msg,
                        duration_seconds=time.time() - start_time
                    )
                    session.commit()
            except Exception as db_error:
                logger.error(f"æ›´æ–°ä»»åŠ¡ {task_id} å¤±è´¥çŠ¶æ€æ—¶å‡ºé”™: {db_error}")
                # å³ä½¿æ•°æ®åº“æ›´æ–°å¤±è´¥ï¼Œä¹Ÿè¦ç»§ç»­æ‰§è¡Œç»Ÿè®¡æ›´æ–°

            # æ›´æ–°ç»Ÿè®¡
            with self.lock:
                self.stats['total_processed'] += 1
                self.stats['failed'] += 1
            
            return {
                'success': False,
                'task_id': task_id,
                'error': error_msg
            }
        finally:
            self.db_manager.remove_session()

                
    def _publish_to_twitter(self, content, media_path: str) -> dict:
        """å‘å¸ƒå†…å®¹åˆ°Twitter"""
        try:
            logger.info(f"[SCHEDULER_PUBLISH_DEBUG] å¼€å§‹å‘å¸ƒåˆ°Twitter")
            logger.info(f"[SCHEDULER_PUBLISH_DEBUG] åŸå§‹å†…å®¹: {content}")
            logger.info(f"[SCHEDULER_PUBLISH_DEBUG] åª’ä½“è·¯å¾„: {media_path}")
            
            # è·å–æ¨æ–‡æ–‡æœ¬
            if isinstance(content, dict):
                tweet_text = content.get('text', '')
                logger.info(f"[SCHEDULER_PUBLISH_DEBUG] ä»å­—å…¸æå–æ¨æ–‡æ–‡æœ¬: {tweet_text}")
            elif isinstance(content, str):
                tweet_text = content
                logger.info(f"[SCHEDULER_PUBLISH_DEBUG] ç›´æ¥ä½¿ç”¨å­—ç¬¦ä¸²ä½œä¸ºæ¨æ–‡æ–‡æœ¬: {tweet_text}")
            else:
                tweet_text = str(content) if content else ''
                logger.info(f"[SCHEDULER_PUBLISH_DEBUG] è½¬æ¢ä¸ºå­—ç¬¦ä¸²çš„æ¨æ–‡æ–‡æœ¬: {tweet_text}")
                
            if not tweet_text:
                logger.error(f"[SCHEDULER_PUBLISH_DEBUG] æ¨æ–‡å†…å®¹ä¸ºç©º")
                raise ValueError("æ¨æ–‡å†…å®¹ä¸ºç©º")
                
            # ä¸¥æ ¼æ£€æŸ¥åª’ä½“æ–‡ä»¶å­˜åœ¨æ€§å’Œç±»å‹
            if not media_path or not os.path.exists(media_path):
                logger.error(f"[SCHEDULER_PUBLISH_DEBUG] åª’ä½“æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾„ä¸ºç©º: {media_path}")
                raise FileNotFoundError(f"åª’ä½“æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾„ä¸ºç©º: {media_path}")
                
            file_ext = os.path.splitext(media_path)[1].lower()
            logger.info(f"[SCHEDULER_PUBLISH_DEBUG] åª’ä½“æ–‡ä»¶æ‰©å±•å: {file_ext}")
            
            if file_ext in ['.mp4', '.mov', '.avi', '.mkv']:
                # è§†é¢‘æ–‡ä»¶ - è¿™æ˜¯æˆ‘ä»¬æœŸæœ›çš„åª’ä½“ç±»å‹
                logger.info(f"[SCHEDULER_PUBLISH_DEBUG] å‘å¸ƒè§†é¢‘æ¨æ–‡")
                tweet_info, upload_time = self.publisher.post_tweet_with_video(
                    tweet_text, media_path
                )
            elif file_ext in ['.jpg', '.jpeg', '.png', '.gif']:
                # å›¾ç‰‡æ–‡ä»¶
                logger.info(f"[SCHEDULER_PUBLISH_DEBUG] å‘å¸ƒå›¾ç‰‡æ¨æ–‡")
                tweet_info, upload_time = self.publisher.post_tweet_with_images(
                    tweet_text, [media_path]
                )
            else:
                # ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œç›´æ¥å¤±è´¥è€Œä¸æ˜¯é™çº§
                logger.error(f"[SCHEDULER_PUBLISH_DEBUG] ä¸æ”¯æŒçš„åª’ä½“æ–‡ä»¶ç±»å‹: {file_ext}")
                raise ValueError(f"ä¸æ”¯æŒçš„åª’ä½“æ–‡ä»¶ç±»å‹: {file_ext}ï¼ŒæœŸæœ›çš„ç±»å‹: .mp4, .mov, .avi, .mkv, .jpg, .jpeg, .png, .gif")
            
            logger.info(f"[SCHEDULER_PUBLISH_DEBUG] å‘å¸ƒæˆåŠŸï¼Œæ¨æ–‡ä¿¡æ¯: {tweet_info}")
            logger.info(f"[SCHEDULER_PUBLISH_DEBUG] ä¸Šä¼ æ—¶é—´: {upload_time}")
                
            return {
                'success': True,
                'tweet_id': tweet_info.get('tweet_id'),
                'tweet_url': tweet_info.get('tweet_url'),
                'tweet_text': tweet_text,
                'upload_time': upload_time,
                'message': 'å‘å¸ƒæˆåŠŸ'
            }
            
        except Exception as e:
            logger.error(f"[SCHEDULER_PUBLISH_DEBUG] Twitterå‘å¸ƒå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'message': f'å‘å¸ƒå¤±è´¥: {e}'
            }
            
    def _cleanup_running_tasks(self):
        """æ¸…ç†è¿è¡Œä¸­çš„ä»»åŠ¡"""
        with self.lock:
            for task_id, task_info in self.running_tasks.items():
                try:
                    task_info['future'].cancel()
                    self.task_repo.update(task_id, {'status': TaskStatus.PENDING.value})
                except Exception as e:
                    logger.error(f"æ¸…ç†ä»»åŠ¡ {task_id} å¤±è´¥: {e}")
                    
            self.running_tasks.clear()
            
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        logger.info("è°ƒåº¦å™¨ç›‘æ§å¾ªç¯å¯åŠ¨")
        
        while self.is_running:
            try:
                # è®°å½•æ€§èƒ½æŒ‡æ ‡
                self.performance_monitor.record_metric(
                    'scheduler_queue_size', 
                    self.task_queue.qsize()
                )
                
                self.performance_monitor.record_metric(
                    'scheduler_running_tasks', 
                    len(self.running_tasks)
                )
                
                # æ£€æŸ¥ç³»ç»Ÿèµ„æº
                self._check_system_resources()
                
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"ç›‘æ§å¾ªç¯å¼‚å¸¸: {e}")
                time.sleep(10)
                
        logger.info("è°ƒåº¦å™¨ç›‘æ§å¾ªç¯ç»“æŸ")
        
    def _check_system_resources(self):
        """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
        try:
            import psutil
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            memory_percent = psutil.virtual_memory().percent
            if memory_percent > 90:
                logger.warning(f"ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_percent}%")
                
            # æ£€æŸ¥CPUä½¿ç”¨
            cpu_percent = psutil.cpu_percent(interval=1)
            if cpu_percent > 90:
                logger.warning(f"ç³»ç»ŸCPUä½¿ç”¨ç‡è¿‡é«˜: {cpu_percent}%")
                
        except ImportError:
            # psutilæœªå®‰è£…ï¼Œè·³è¿‡èµ„æºæ£€æŸ¥
            pass
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç³»ç»Ÿèµ„æºå¤±è´¥: {e}")
            
    def _execute_task_wrapper(self, task_execution: TaskExecution):
        """
        åŒ…è£… _execute_taskï¼Œä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“ä¼šè¯å’Œäº‹åŠ¡ã€‚
        """
        try:
            # ç›´æ¥è°ƒç”¨æ–°çš„ _execute_task æ–¹æ³•ï¼Œå®ƒå·²ç»åŒ…å«äº†å®Œæ•´çš„æ•°æ®åº“ä¼šè¯ç®¡ç†
            result = self._execute_task(task_execution)
            return result

        except Exception as e:
            logger.error(f"ä»»åŠ¡ {task_execution.task_id} åœ¨åŒ…è£…å™¨ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
            return {
                'success': False,
                'task_id': task_execution.task_id,
                'error': f"Wrapper exception: {str(e)}"
            }

    def run_batch(self, limit: int = 10, project_filter: str = None, 
                  language_filter: str = None) -> Dict[str, Any]:
        """ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè¿è¡Œä¸€ä¸ªæ‰¹æ¬¡çš„ä»»åŠ¡ã€‚

        Args:
            limit: æœ¬æ‰¹æ¬¡æœ€å¤§ä»»åŠ¡æ•°
            project_filter: é¡¹ç›®è¿‡æ»¤å™¨
            language_filter: è¯­è¨€è¿‡æ»¤å™¨

        Returns:
            æ‰¹æ¬¡å¤„ç†ç»“æœæ€»ç»“
        """
        logger.info(f"å¼€å§‹è¿è¡Œæ‰¹å¤„ç†ï¼Œæœ€å¤šå¤„ç† {limit} ä¸ªä»»åŠ¡...")

        session = self.db_manager.get_session()
        try:
            task_repo = PublishingTaskRepository(session)
            filters = {'status': ['pending', 'retry']}
            if project_filter:
                # å‡è®¾ project_filter æ˜¯é¡¹ç›®åç§°ï¼Œéœ€è¦å…ˆæŸ¥è¯¢åˆ°é¡¹ç›®ID
                project_repo = ProjectRepository(session)
                # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ç”¨æˆ·IDä¸º1ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦åŠ¨æ€è·å–
                project = project_repo.get_by_name_and_user(name=project_filter, user_id=1)
                if project:
                    filters['project_id'] = project.id
                else:
                    logger.warning(f"æœªæ‰¾åˆ°é¡¹ç›® '{project_filter}'ï¼Œå°†å¿½ç•¥æ­¤è¿‡æ»¤å™¨ã€‚")
            if language_filter:
                filters['language'] = language_filter

            pending_tasks = task_repo.get_ready_tasks(filters=filters, limit=limit)

            if not pending_tasks:
                logger.info("æ²¡æœ‰å¾…å¤„ç†çš„ä»»åŠ¡ã€‚")
                return {
                    'success': True,
                    'processed': 0, 
                    'successful': 0, 
                    'failed': 0, 
                    'message': 'æ²¡æœ‰å¾…å¤„ç†çš„ä»»åŠ¡'
                }

            successful_count = 0
            failed_count = 0

            with ThreadPoolExecutor(max_workers=1) as executor:
                task_executions = [
                    TaskExecution(
                        task_id=task.id, 
                        priority=self._determine_task_priority(task), 
                        scheduled_time=datetime.now()
                    ) for task in pending_tasks
                ]
                
                future_to_task = {executor.submit(self._execute_task_wrapper, te): te for te in task_executions}
                
                for future in as_completed(future_to_task):
                    task_execution = future_to_task[future]
                    try:
                        result = future.result()
                        if result and result.get('success'):
                            successful_count += 1
                        else:
                            failed_count += 1
                    except Exception as exc:
                        logger.error(f'ä»»åŠ¡ {task_execution.task_id} ç”Ÿæˆäº†å¼‚å¸¸: {exc}', exc_info=True)
                        failed_count += 1

            processed_count = len(pending_tasks)
            logger.info(f"æ‰¹å¤„ç†å®Œæˆã€‚å…±å¤„ç† {processed_count} ä¸ªä»»åŠ¡ï¼ŒæˆåŠŸ {successful_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ªã€‚")

            return {
                'success': True,
                'processed': processed_count,
                'successful': successful_count,
                'failed': failed_count,
                'message': f'æ‰¹å¤„ç†å®Œæˆ: å¤„ç† {processed_count} ä¸ªä»»åŠ¡ï¼ŒæˆåŠŸ {successful_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª'
            }
        finally:
            self.db_manager.remove_session()