#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“è¿ç§»å·¥å…·
æä¾›æ•°æ®åº“æ¶æ„è¿ç§»ã€æ•°æ®å¤‡ä»½å’Œæ¢å¤åŠŸèƒ½
"""

import sys
import os
import json
import shutil
import sqlite3
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

try:
    from app.database.models import Base, User, Project, ContentSource, PublishingTask, PublishingLog
    from app.database.database import DatabaseManager
    from sqlalchemy import create_engine, inspect, text
    from sqlalchemy.orm import sessionmaker
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿é¡¹ç›®ä¾èµ–å·²æ­£ç¡®å®‰è£…")
    sys.exit(1)


class DatabaseMigrator:
    """æ•°æ®åº“è¿ç§»å™¨"""
    
    def __init__(self, db_path: str, backup_dir: Optional[str] = None):
        self.db_path = Path(db_path)
        self.backup_dir = Path(backup_dir) if backup_dir else self.db_path.parent / "backups"
        self.backup_dir.mkdir(exist_ok=True)
        
        # æ•°æ®åº“è¿æ¥
        self.db_url = f"sqlite:///{self.db_path}"
        self.engine = None
        self.session = None
        
        # è¿ç§»å†å²
        self.migration_log = []
        
    def initialize_connection(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        try:
            self.engine = create_engine(self.db_url, echo=False)
            Session = sessionmaker(bind=self.engine)
            self.session = Session()
            print(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ: {self.db_path}")
        except Exception as e:
            raise Exception(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            
    def cleanup_connection(self):
        """æ¸…ç†æ•°æ®åº“è¿æ¥"""
        if self.session:
            self.session.close()
        if self.engine:
            self.engine.dispose()
            
    def create_backup(self, backup_name: Optional[str] = None) -> str:
        """åˆ›å»ºæ•°æ®åº“å¤‡ä»½"""
        if not self.db_path.exists():
            raise FileNotFoundError(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {self.db_path}")
            
        # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
        if not backup_name:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"backup_{timestamp}.db"
            
        backup_path = self.backup_dir / backup_name
        
        print(f"ğŸ“¦ åˆ›å»ºæ•°æ®åº“å¤‡ä»½...")
        print(f"   æºæ–‡ä»¶: {self.db_path}")
        print(f"   å¤‡ä»½æ–‡ä»¶: {backup_path}")
        
        try:
            # ä½¿ç”¨SQLiteçš„å¤‡ä»½APIè¿›è¡Œåœ¨çº¿å¤‡ä»½
            source_conn = sqlite3.connect(str(self.db_path))
            backup_conn = sqlite3.connect(str(backup_path))
            
            source_conn.backup(backup_conn)
            
            source_conn.close()
            backup_conn.close()
            
            # éªŒè¯å¤‡ä»½æ–‡ä»¶
            backup_size = backup_path.stat().st_size
            original_size = self.db_path.stat().st_size
            
            print(f"   âœ… å¤‡ä»½å®Œæˆ")
            print(f"   åŸå§‹å¤§å°: {original_size:,} bytes")
            print(f"   å¤‡ä»½å¤§å°: {backup_size:,} bytes")
            
            # è®°å½•å¤‡ä»½ä¿¡æ¯
            backup_info = {
                "timestamp": datetime.now().isoformat(),
                "backup_path": str(backup_path),
                "original_path": str(self.db_path),
                "original_size": original_size,
                "backup_size": backup_size,
                "status": "success"
            }
            
            self.migration_log.append({
                "action": "backup",
                "details": backup_info
            })
            
            return str(backup_path)
            
        except Exception as e:
            if backup_path.exists():
                backup_path.unlink()  # åˆ é™¤å¤±è´¥çš„å¤‡ä»½æ–‡ä»¶
            raise Exception(f"å¤‡ä»½åˆ›å»ºå¤±è´¥: {e}")
            
    def restore_backup(self, backup_path: str, confirm: bool = False) -> bool:
        """ä»å¤‡ä»½æ¢å¤æ•°æ®åº“"""
        backup_file = Path(backup_path)
        
        if not backup_file.exists():
            raise FileNotFoundError(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_path}")
            
        if not confirm:
            print("âš ï¸  è­¦å‘Š: æ­¤æ“ä½œå°†è¦†ç›–å½“å‰æ•°æ®åº“ï¼")
            print(f"   å½“å‰æ•°æ®åº“: {self.db_path}")
            print(f"   å¤‡ä»½æ–‡ä»¶: {backup_file}")
            response = input("ç¡®è®¤æ¢å¤ï¼Ÿ(yes/no): ")
            if response.lower() != 'yes':
                print("âŒ æ¢å¤æ“ä½œå·²å–æ¶ˆ")
                return False
                
        print(f"ğŸ”„ ä»å¤‡ä»½æ¢å¤æ•°æ®åº“...")
        
        try:
            # å…³é—­ç°æœ‰è¿æ¥
            self.cleanup_connection()
            
            # åˆ›å»ºå½“å‰æ•°æ®åº“çš„ä¸´æ—¶å¤‡ä»½
            if self.db_path.exists():
                temp_backup = self.db_path.with_suffix('.temp_backup')
                shutil.copy2(self.db_path, temp_backup)
                print(f"   ğŸ“¦ åˆ›å»ºä¸´æ—¶å¤‡ä»½: {temp_backup}")
            else:
                temp_backup = None
                
            # æ¢å¤å¤‡ä»½
            shutil.copy2(backup_file, self.db_path)
            print(f"   âœ… æ•°æ®åº“æ¢å¤å®Œæˆ")
            
            # éªŒè¯æ¢å¤çš„æ•°æ®åº“
            self.initialize_connection()
            tables = self.get_table_info()
            print(f"   ğŸ“Š æ¢å¤åè¡¨æ•°é‡: {len(tables)}")
            
            # åˆ é™¤ä¸´æ—¶å¤‡ä»½
            if temp_backup and temp_backup.exists():
                temp_backup.unlink()
                print(f"   ğŸ—‘ï¸  åˆ é™¤ä¸´æ—¶å¤‡ä»½")
                
            # è®°å½•æ¢å¤ä¿¡æ¯
            self.migration_log.append({
                "action": "restore",
                "details": {
                    "timestamp": datetime.now().isoformat(),
                    "backup_source": str(backup_file),
                    "target_path": str(self.db_path),
                    "tables_count": len(tables),
                    "status": "success"
                }
            })
            
            return True
            
        except Exception as e:
            # å°è¯•æ¢å¤ä¸´æ—¶å¤‡ä»½
            if temp_backup and temp_backup.exists():
                try:
                    shutil.copy2(temp_backup, self.db_path)
                    temp_backup.unlink()
                    print(f"   ğŸ”„ å·²æ¢å¤åˆ°åŸå§‹çŠ¶æ€")
                except:
                    pass
                    
            raise Exception(f"æ•°æ®åº“æ¢å¤å¤±è´¥: {e}")
            
    def get_table_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“è¡¨ä¿¡æ¯"""
        if not self.engine:
            self.initialize_connection()
            
        inspector = inspect(self.engine)
        tables = {}
        
        for table_name in inspector.get_table_names():
            columns = inspector.get_columns(table_name)
            indexes = inspector.get_indexes(table_name)
            foreign_keys = inspector.get_foreign_keys(table_name)
            
            # è·å–è¡Œæ•°
            try:
                result = self.session.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                row_count = result.scalar()
            except:
                row_count = 0
                
            tables[table_name] = {
                "columns": len(columns),
                "indexes": len(indexes),
                "foreign_keys": len(foreign_keys),
                "row_count": row_count,
                "column_details": [
                    {
                        "name": col["name"],
                        "type": str(col["type"]),
                        "nullable": col["nullable"],
                        "primary_key": col.get("primary_key", False)
                    }
                    for col in columns
                ]
            }
            
        return tables
        
    def check_schema_compatibility(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ¶æ„å…¼å®¹æ€§"""
        print("ğŸ” æ£€æŸ¥æ•°æ®åº“æ¶æ„å…¼å®¹æ€§...")
        
        try:
            # è·å–å½“å‰æ•°æ®åº“æ¶æ„
            current_tables = self.get_table_info()
            
            # è·å–æ¨¡å‹å®šä¹‰çš„æ¶æ„
            expected_tables = {
                "users": ["id", "username", "email", "role", "created_at", "updated_at"],
                "projects": ["id", "user_id", "name", "description", "folder_path", "is_active", "created_at", "updated_at"],
                "content_sources": ["id", "project_id", "source_type", "path_or_identifier", "total_items", "used_items", "last_scanned", "created_at", "updated_at"],
                "publishing_tasks": ["id", "project_id", "source_id", "media_path", "content_data", "scheduled_at", "status", "priority", "retry_count", "created_at", "updated_at"],
                "publishing_logs": ["id", "task_id", "status", "tweet_id", "tweet_content", "published_at", "error_message", "duration_seconds", "created_at"]
            }
            
            compatibility_report = {
                "compatible": True,
                "missing_tables": [],
                "extra_tables": [],
                "column_issues": {},
                "summary": {}
            }
            
            # æ£€æŸ¥ç¼ºå¤±çš„è¡¨
            for expected_table in expected_tables:
                if expected_table not in current_tables:
                    compatibility_report["missing_tables"].append(expected_table)
                    compatibility_report["compatible"] = False
                    
            # æ£€æŸ¥å¤šä½™çš„è¡¨
            for current_table in current_tables:
                if current_table not in expected_tables and not current_table.startswith("sqlite_"):
                    compatibility_report["extra_tables"].append(current_table)
                    
            # æ£€æŸ¥åˆ—
            for table_name, expected_columns in expected_tables.items():
                if table_name in current_tables:
                    current_columns = [col["name"] for col in current_tables[table_name]["column_details"]]
                    missing_columns = [col for col in expected_columns if col not in current_columns]
                    extra_columns = [col for col in current_columns if col not in expected_columns]
                    
                    if missing_columns or extra_columns:
                        compatibility_report["column_issues"][table_name] = {
                            "missing": missing_columns,
                            "extra": extra_columns
                        }
                        if missing_columns:
                            compatibility_report["compatible"] = False
                            
            # ç”Ÿæˆæ‘˜è¦
            compatibility_report["summary"] = {
                "total_tables": len(current_tables),
                "expected_tables": len(expected_tables),
                "missing_tables_count": len(compatibility_report["missing_tables"]),
                "extra_tables_count": len(compatibility_report["extra_tables"]),
                "tables_with_column_issues": len(compatibility_report["column_issues"])
            }
            
            # æ‰“å°æŠ¥å‘Š
            if compatibility_report["compatible"]:
                print("   âœ… æ•°æ®åº“æ¶æ„å…¼å®¹")
            else:
                print("   âš ï¸  å‘ç°æ¶æ„å…¼å®¹æ€§é—®é¢˜")
                
            if compatibility_report["missing_tables"]:
                print(f"   âŒ ç¼ºå¤±è¡¨: {', '.join(compatibility_report['missing_tables'])}")
                
            if compatibility_report["extra_tables"]:
                print(f"   â„¹ï¸  é¢å¤–è¡¨: {', '.join(compatibility_report['extra_tables'])}")
                
            for table, issues in compatibility_report["column_issues"].items():
                if issues["missing"]:
                    print(f"   âŒ {table} ç¼ºå¤±åˆ—: {', '.join(issues['missing'])}")
                if issues["extra"]:
                    print(f"   â„¹ï¸  {table} é¢å¤–åˆ—: {', '.join(issues['extra'])}")
                    
            return compatibility_report
            
        except Exception as e:
            raise Exception(f"æ¶æ„å…¼å®¹æ€§æ£€æŸ¥å¤±è´¥: {e}")
            
    def migrate_to_latest(self, create_backup: bool = True) -> bool:
        """è¿ç§»åˆ°æœ€æ–°æ¶æ„"""
        print("ğŸš€ å¼€å§‹æ•°æ®åº“è¿ç§»...")
        
        try:
            # åˆ›å»ºå¤‡ä»½
            if create_backup:
                backup_path = self.create_backup()
                print(f"   ğŸ“¦ å¤‡ä»½å·²åˆ›å»º: {backup_path}")
                
            # æ£€æŸ¥å½“å‰æ¶æ„
            compatibility = self.check_schema_compatibility()
            
            if compatibility["compatible"]:
                print("   âœ… æ•°æ®åº“æ¶æ„å·²æ˜¯æœ€æ–°ç‰ˆæœ¬")
                return True
                
            # æ‰§è¡Œè¿ç§»
            print("   ğŸ”„ æ‰§è¡Œæ¶æ„è¿ç§»...")
            
            # åˆ›å»ºæ‰€æœ‰è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            Base.metadata.create_all(self.engine)
            
            # å†æ¬¡æ£€æŸ¥å…¼å®¹æ€§
            post_migration_compatibility = self.check_schema_compatibility()
            
            if post_migration_compatibility["compatible"]:
                print("   âœ… è¿ç§»å®Œæˆï¼Œæ¶æ„å·²æ›´æ–°")
                
                # è®°å½•è¿ç§»ä¿¡æ¯
                self.migration_log.append({
                    "action": "migrate",
                    "details": {
                        "timestamp": datetime.now().isoformat(),
                        "pre_migration": compatibility["summary"],
                        "post_migration": post_migration_compatibility["summary"],
                        "backup_created": create_backup,
                        "status": "success"
                    }
                })
                
                return True
            else:
                raise Exception("è¿ç§»åæ¶æ„ä»ä¸å…¼å®¹")
                
        except Exception as e:
            print(f"   âŒ è¿ç§»å¤±è´¥: {e}")
            
            # è®°å½•å¤±è´¥ä¿¡æ¯
            self.migration_log.append({
                "action": "migrate",
                "details": {
                    "timestamp": datetime.now().isoformat(),
                    "error": str(e),
                    "status": "failed"
                }
            })
            
            raise
            
    def list_backups(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½æ–‡ä»¶"""
        backups = []
        
        if not self.backup_dir.exists():
            return backups
            
        for backup_file in self.backup_dir.glob("*.db"):
            try:
                stat = backup_file.stat()
                backups.append({
                    "name": backup_file.name,
                    "path": str(backup_file),
                    "size": stat.st_size,
                    "created": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat()
                })
            except Exception:
                continue
                
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
        backups.sort(key=lambda x: x["created"], reverse=True)
        return backups
        
    def cleanup_old_backups(self, keep_count: int = 10) -> int:
        """æ¸…ç†æ—§å¤‡ä»½æ–‡ä»¶"""
        backups = self.list_backups()
        
        if len(backups) <= keep_count:
            return 0
            
        to_delete = backups[keep_count:]
        deleted_count = 0
        
        print(f"ğŸ—‘ï¸  æ¸…ç†æ—§å¤‡ä»½æ–‡ä»¶ (ä¿ç•™æœ€æ–° {keep_count} ä¸ª)...")
        
        for backup in to_delete:
            try:
                Path(backup["path"]).unlink()
                print(f"   åˆ é™¤: {backup['name']}")
                deleted_count += 1
            except Exception as e:
                print(f"   åˆ é™¤å¤±è´¥ {backup['name']}: {e}")
                
        return deleted_count
        
    def export_migration_log(self, output_path: Optional[str] = None) -> str:
        """å¯¼å‡ºè¿ç§»æ—¥å¿—"""
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = str(self.backup_dir / f"migration_log_{timestamp}.json")
            
        log_data = {
            "database_path": str(self.db_path),
            "migration_timestamp": datetime.now().isoformat(),
            "operations": self.migration_log
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)
            
        return output_path


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='æ•°æ®åº“è¿ç§»å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
    # æ£€æŸ¥æ•°æ®åº“æ¶æ„å…¼å®¹æ€§
    python database_migrator.py --check
    
    # è¿ç§»åˆ°æœ€æ–°æ¶æ„
    python database_migrator.py --migrate
    
    # åˆ›å»ºå¤‡ä»½
    python database_migrator.py --backup
    
    # ä»å¤‡ä»½æ¢å¤
    python database_migrator.py --restore backup_20241201_120000.db
    
    # åˆ—å‡ºæ‰€æœ‰å¤‡ä»½
    python database_migrator.py --list-backups
    
    # æ¸…ç†æ—§å¤‡ä»½
    python database_migrator.py --cleanup-backups --keep 5
        """
    )
    
    parser.add_argument(
        '--db-path',
        help='æ•°æ®åº“æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--backup-dir',
        help='å¤‡ä»½ç›®å½•è·¯å¾„'
    )
    
    parser.add_argument(
        '--check',
        action='store_true',
        help='æ£€æŸ¥æ•°æ®åº“æ¶æ„å…¼å®¹æ€§'
    )
    
    parser.add_argument(
        '--migrate',
        action='store_true',
        help='è¿ç§»åˆ°æœ€æ–°æ¶æ„'
    )
    
    parser.add_argument(
        '--backup',
        action='store_true',
        help='åˆ›å»ºæ•°æ®åº“å¤‡ä»½'
    )
    
    parser.add_argument(
        '--restore',
        metavar='BACKUP_FILE',
        help='ä»å¤‡ä»½æ–‡ä»¶æ¢å¤æ•°æ®åº“'
    )
    
    parser.add_argument(
        '--list-backups',
        action='store_true',
        help='åˆ—å‡ºæ‰€æœ‰å¤‡ä»½æ–‡ä»¶'
    )
    
    parser.add_argument(
        '--cleanup-backups',
        action='store_true',
        help='æ¸…ç†æ—§å¤‡ä»½æ–‡ä»¶'
    )
    
    parser.add_argument(
        '--keep',
        type=int,
        default=10,
        help='æ¸…ç†å¤‡ä»½æ—¶ä¿ç•™çš„æ–‡ä»¶æ•°é‡ï¼ˆé»˜è®¤: 10ï¼‰'
    )
    
    parser.add_argument(
        '--no-backup',
        action='store_true',
        help='è¿ç§»æ—¶ä¸åˆ›å»ºå¤‡ä»½'
    )
    
    parser.add_argument(
        '--force',
        action='store_true',
        help='å¼ºåˆ¶æ‰§è¡Œæ“ä½œï¼ˆè·³è¿‡ç¡®è®¤ï¼‰'
    )
    
    parser.add_argument(
        '--json',
        action='store_true',
        help='ä»¥JSONæ ¼å¼è¾“å‡ºç»“æœ'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†æ“ä½œ
    operations = [args.check, args.migrate, args.backup, args.restore, args.list_backups, args.cleanup_backups]
    if not any(operations):
        parser.print_help()
        sys.exit(1)
        
    try:
        # åˆå§‹åŒ–è¿ç§»å™¨
        db_path = args.db_path or str(project_root / "data" / "twitter_publisher.db")
        migrator = DatabaseMigrator(db_path, args.backup_dir)
        
        # æ‰§è¡Œæ“ä½œ
        if args.check:
            migrator.initialize_connection()
            compatibility = migrator.check_schema_compatibility()
            
            if args.json:
                print(json.dumps(compatibility, indent=2, ensure_ascii=False))
            else:
                print("\nğŸ“Š æ¶æ„å…¼å®¹æ€§æŠ¥å‘Š:")
                print(f"   å…¼å®¹æ€§: {'âœ… å…¼å®¹' if compatibility['compatible'] else 'âŒ ä¸å…¼å®¹'}")
                print(f"   æ€»è¡¨æ•°: {compatibility['summary']['total_tables']}")
                print(f"   é¢„æœŸè¡¨æ•°: {compatibility['summary']['expected_tables']}")
                
        elif args.migrate:
            migrator.initialize_connection()
            success = migrator.migrate_to_latest(create_backup=not args.no_backup)
            
            if args.json:
                print(json.dumps({"success": success}, indent=2))
                
        elif args.backup:
            backup_path = migrator.create_backup()
            
            if args.json:
                print(json.dumps({"backup_path": backup_path}, indent=2))
                
        elif args.restore:
            migrator.initialize_connection()
            success = migrator.restore_backup(args.restore, confirm=args.force)
            
            if args.json:
                print(json.dumps({"success": success}, indent=2))
                
        elif args.list_backups:
            backups = migrator.list_backups()
            
            if args.json:
                print(json.dumps(backups, indent=2, ensure_ascii=False))
            else:
                print("\nğŸ“¦ å¤‡ä»½æ–‡ä»¶åˆ—è¡¨:")
                if not backups:
                    print("   æ— å¤‡ä»½æ–‡ä»¶")
                else:
                    for backup in backups:
                        size_mb = backup["size"] / (1024 * 1024)
                        print(f"   {backup['name']} ({size_mb:.1f}MB) - {backup['created']}")
                        
        elif args.cleanup_backups:
            deleted_count = migrator.cleanup_old_backups(args.keep)
            
            if args.json:
                print(json.dumps({"deleted_count": deleted_count}, indent=2))
            else:
                print(f"\nğŸ—‘ï¸  å·²åˆ é™¤ {deleted_count} ä¸ªæ—§å¤‡ä»½æ–‡ä»¶")
                
        # å¯¼å‡ºè¿ç§»æ—¥å¿—
        if migrator.migration_log:
            log_path = migrator.export_migration_log()
            if not args.json:
                print(f"\nğŸ“‹ è¿ç§»æ—¥å¿—å·²ä¿å­˜: {log_path}")
                
    except KeyboardInterrupt:
        print("\nâš ï¸  æ“ä½œè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æ“ä½œå¤±è´¥: {e}")
        sys.exit(1)
    finally:
        if 'migrator' in locals():
            migrator.cleanup_connection()


if __name__ == "__main__":
    main()