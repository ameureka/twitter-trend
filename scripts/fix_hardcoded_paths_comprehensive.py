#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆè·¯å¾„ä¿®å¤è„šæœ¬

è§£å†³é—®é¢˜ä¸€ï¼šè‡´å‘½çš„ç¯å¢ƒä¸åŒ¹é…ä¸ç¡¬ç¼–ç è·¯å¾„

ä¸»è¦åŠŸèƒ½ï¼š
1. åˆ†ææ•°æ®åº“ä¸­çš„ç¡¬ç¼–ç ç»å¯¹è·¯å¾„é—®é¢˜
2. å°†ç»å¯¹è·¯å¾„è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
3. æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„åŸºç¡€è·¯å¾„è®¾ç½®
4. éªŒè¯ä¿®å¤åçš„è·¯å¾„å¯ç”¨æ€§

ä½¿ç”¨æ–¹æ³•ï¼š
1. åˆ†ææ¨¡å¼ï¼špython scripts/fix_hardcoded_paths_comprehensive.py --analyze
2. ä¿®å¤æ¨¡å¼ï¼špython scripts/fix_hardcoded_paths_comprehensive.py --fix
3. éªŒè¯æ¨¡å¼ï¼špython scripts/fix_hardcoded_paths_comprehensive.py --verify
"""

import os
import sys
import json
import shutil
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from app.database.db_manager import EnhancedDatabaseManager
from app.database.models import PublishingTask, ContentSource
from app.utils.enhanced_config import get_enhanced_config
from app.utils.logger import get_logger
from app.utils.path_manager import get_path_manager

logger = get_logger(__name__)

class ComprehensivePathFixer:
    """ç»¼åˆè·¯å¾„ä¿®å¤å™¨"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.config = get_enhanced_config()
        self.db_manager = EnhancedDatabaseManager()
        self.session = self.db_manager.get_session()
        self.path_manager = get_path_manager()
        
        logger.info(f"é¡¹ç›®æ ¹ç›®å½•: {self.project_root}")
        logger.info(f"å½“å‰é…ç½®çš„project_base_path: {self.config.get('project_base_path', 'æœªè®¾ç½®')}")
    
    def analyze_path_issues(self) -> Dict[str, Any]:
        """åˆ†æè·¯å¾„é—®é¢˜"""
        logger.info("å¼€å§‹åˆ†ææ•°æ®åº“ä¸­çš„è·¯å¾„é—®é¢˜...")
        
        analysis = {
            'publishing_tasks': {
                'total': 0,
                'absolute_paths': 0,
                'relative_paths': 0,
                'invalid_paths': 0,
                'hardcoded_patterns': {},
                'samples': []
            },
            'content_sources': {
                'total': 0,
                'absolute_paths': 0,
                'relative_paths': 0,
                'invalid_paths': 0,
                'hardcoded_patterns': {},
                'samples': []
            },
            'config_issues': [],
            'recommendations': []
        }
        
        # åˆ†æ publishing_tasks è¡¨
        tasks = self.session.query(PublishingTask).all()
        analysis['publishing_tasks']['total'] = len(tasks)
        
        for task in tasks:
            if task.media_path:
                self._analyze_path(task.media_path, analysis['publishing_tasks'], 
                                 {'id': task.id, 'field': 'media_path'})
            
            # åˆ†æ content_data ä¸­çš„è·¯å¾„
            if task.content_data:
                try:
                    content_data = json.loads(task.content_data)
                    for key in ['metadata_path', 'file_path', 'video_path', 'audio_path']:
                        if key in content_data and content_data[key]:
                            self._analyze_path(content_data[key], analysis['publishing_tasks'],
                                             {'id': task.id, 'field': f'content_data.{key}'})
                except json.JSONDecodeError:
                    pass
        
        # åˆ†æ content_sources è¡¨
        sources = self.session.query(ContentSource).all()
        analysis['content_sources']['total'] = len(sources)
        
        for source in sources:
            if source.path_or_identifier:
                self._analyze_path(source.path_or_identifier, analysis['content_sources'],
                                 {'id': source.id, 'field': 'path_or_identifier'})
        
        # åˆ†æé…ç½®é—®é¢˜
        self._analyze_config_issues(analysis)
        
        # ç”Ÿæˆå»ºè®®
        self._generate_recommendations(analysis)
        
        return analysis
    
    def _analyze_path(self, path: str, analysis_section: Dict, context: Dict):
        """åˆ†æå•ä¸ªè·¯å¾„"""
        if not path:
            analysis_section['invalid_paths'] += 1
            return
        
        path_obj = Path(path)
        
        if path_obj.is_absolute():
            analysis_section['absolute_paths'] += 1
            
            # æ£€æµ‹ç¡¬ç¼–ç æ¨¡å¼
            if '/Users/ameureka/Desktop/twitter-trend' in path:
                pattern = 'macos_dev_hardcoded'
            elif '/home/twitter-trend' in path:
                pattern = 'linux_server_hardcoded'
            elif '/tmp/' in path:
                pattern = 'temp_path'
            else:
                pattern = 'other_absolute'
            
            analysis_section['hardcoded_patterns'][pattern] = \
                analysis_section['hardcoded_patterns'].get(pattern, 0) + 1
        else:
            analysis_section['relative_paths'] += 1
        
        # æ”¶é›†æ ·æœ¬
        if len(analysis_section['samples']) < 10:
            analysis_section['samples'].append({
                'context': context,
                'path': path,
                'type': 'absolute' if path_obj.is_absolute() else 'relative'
            })
    
    def _analyze_config_issues(self, analysis: Dict):
        """åˆ†æé…ç½®é—®é¢˜"""
        config_issues = []
        
        # æ£€æŸ¥ project_base_path è®¾ç½®
        project_base_path = self.config.get('project_base_path')
        if not project_base_path:
            config_issues.append("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ project_base_path è®¾ç½®")
        elif project_base_path == '.':
            config_issues.append("project_base_path è®¾ç½®ä¸º '.'ï¼Œå»ºè®®è®¾ç½®ä¸ºå…·ä½“çš„é¡¹ç›®è·¯å¾„")
        
        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        if not os.environ.get('TWITTER_TREND_BASE_PATH'):
            config_issues.append("ç¼ºå°‘ç¯å¢ƒå˜é‡ TWITTER_TREND_BASE_PATH")
        
        analysis['config_issues'] = config_issues
    
    def _generate_recommendations(self, analysis: Dict):
        """ç”Ÿæˆä¿®å¤å»ºè®®"""
        recommendations = []
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®
        if analysis['publishing_tasks']['absolute_paths'] > 0:
            recommendations.append(
                f"å‘ç° {analysis['publishing_tasks']['absolute_paths']} ä¸ªå‘å¸ƒä»»åŠ¡åŒ…å«ç¡¬ç¼–ç ç»å¯¹è·¯å¾„ï¼Œéœ€è¦è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„"
            )
        
        if analysis['content_sources']['absolute_paths'] > 0:
            recommendations.append(
                f"å‘ç° {analysis['content_sources']['absolute_paths']} ä¸ªå†…å®¹æºåŒ…å«ç¡¬ç¼–ç ç»å¯¹è·¯å¾„ï¼Œéœ€è¦è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„"
            )
        
        if analysis['config_issues']:
            recommendations.append("éœ€è¦ä¿®å¤é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„è®¾ç½®é—®é¢˜")
        
        # ç¯å¢ƒé€‚é…å»ºè®®
        recommendations.extend([
            "å»ºè®®åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®åŠ¨æ€çš„ project_base_path",
            "å»ºè®®ä½¿ç”¨ç¯å¢ƒå˜é‡åŒºåˆ†å¼€å‘ç¯å¢ƒå’Œç”Ÿäº§ç¯å¢ƒ",
            "å»ºè®®å®æ–½è·¯å¾„æ ‡å‡†åŒ–ç­–ç•¥ï¼Œç»Ÿä¸€ä½¿ç”¨ç›¸å¯¹è·¯å¾„å­˜å‚¨"
        ])
        
        analysis['recommendations'] = recommendations
    
    def fix_hardcoded_paths(self, dry_run: bool = True) -> Dict[str, Any]:
        """ä¿®å¤ç¡¬ç¼–ç è·¯å¾„"""
        logger.info(f"å¼€å§‹ä¿®å¤ç¡¬ç¼–ç è·¯å¾„ (dry_run={dry_run})...")
        
        result = {
            'publishing_tasks_fixed': 0,
            'content_sources_fixed': 0,
            'config_updated': False,
            'backup_created': False,
            'errors': []
        }
        
        try:
            # 1. åˆ›å»ºæ•°æ®åº“å¤‡ä»½
            if not dry_run:
                self._create_database_backup()
                result['backup_created'] = True
            
            # 2. ä¿®å¤ publishing_tasks è¡¨
            result['publishing_tasks_fixed'] = self._fix_publishing_tasks(dry_run)
            
            # 3. ä¿®å¤ content_sources è¡¨
            result['content_sources_fixed'] = self._fix_content_sources(dry_run)
            
            # 4. æ›´æ–°é…ç½®æ–‡ä»¶
            if not dry_run:
                self._update_config_file()
                result['config_updated'] = True
            
            # 5. æäº¤æ•°æ®åº“æ›´æ”¹
            if not dry_run:
                self.session.commit()
                logger.info("æ•°æ®åº“æ›´æ”¹å·²æäº¤")
            
        except Exception as e:
            logger.error(f"ä¿®å¤è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            if not dry_run:
                self.session.rollback()
            result['errors'].append(str(e))
            raise
        
        return result
    
    def _create_database_backup(self):
        """åˆ›å»ºæ•°æ®åº“å¤‡ä»½"""
        db_path = self.path_manager.get_database_path()
        backup_path = db_path.parent / f"twitter_publisher_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db"
        
        shutil.copy2(db_path, backup_path)
        logger.info(f"æ•°æ®åº“å¤‡ä»½å·²åˆ›å»º: {backup_path}")
    
    def _fix_publishing_tasks(self, dry_run: bool) -> int:
        """ä¿®å¤å‘å¸ƒä»»åŠ¡è¡¨ä¸­çš„è·¯å¾„"""
        fixed_count = 0
        
        tasks = self.session.query(PublishingTask).all()
        
        for task in tasks:
            modified = False
            
            # ä¿®å¤ media_path
            if task.media_path and self._is_hardcoded_path(task.media_path):
                new_path = self._convert_to_relative_path(task.media_path)
                if new_path != task.media_path:
                    logger.info(f"ä»»åŠ¡ {task.id} media_path: {task.media_path} -> {new_path}")
                    if not dry_run:
                        task.media_path = new_path
                    modified = True
            
            # ä¿®å¤ content_data ä¸­çš„è·¯å¾„
            if task.content_data:
                try:
                    content_data = json.loads(task.content_data)
                    content_modified = False
                    
                    for key in ['metadata_path', 'file_path', 'video_path', 'audio_path']:
                        if key in content_data and content_data[key] and self._is_hardcoded_path(content_data[key]):
                            new_path = self._convert_to_relative_path(content_data[key])
                            if new_path != content_data[key]:
                                logger.info(f"ä»»åŠ¡ {task.id} {key}: {content_data[key]} -> {new_path}")
                                if not dry_run:
                                    content_data[key] = new_path
                                content_modified = True
                    
                    if content_modified:
                        if not dry_run:
                            task.content_data = json.dumps(content_data, ensure_ascii=False)
                        modified = True
                        
                except json.JSONDecodeError:
                    logger.warning(f"ä»»åŠ¡ {task.id} content_data ä¸æ˜¯æœ‰æ•ˆçš„ JSON")
            
            if modified:
                fixed_count += 1
        
        return fixed_count
    
    def _fix_content_sources(self, dry_run: bool) -> int:
        """ä¿®å¤å†…å®¹æºè¡¨ä¸­çš„è·¯å¾„"""
        fixed_count = 0
        
        sources = self.session.query(ContentSource).all()
        
        for source in sources:
            if source.path_or_identifier and self._is_hardcoded_path(source.path_or_identifier):
                new_path = self._convert_to_relative_path(source.path_or_identifier)
                if new_path != source.path_or_identifier:
                    logger.info(f"å†…å®¹æº {source.id}: {source.path_or_identifier} -> {new_path}")
                    if not dry_run:
                        source.path_or_identifier = new_path
                    fixed_count += 1
        
        return fixed_count
    
    def _is_hardcoded_path(self, path: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç¡¬ç¼–ç è·¯å¾„"""
        hardcoded_patterns = [
            '/Users/ameureka/Desktop/twitter-trend',
            '/home/twitter-trend',
            '/data2/twitter-trend'
        ]
        
        return any(pattern in path for pattern in hardcoded_patterns)
    
    def _convert_to_relative_path(self, absolute_path: str) -> str:
        """å°†ç»å¯¹è·¯å¾„è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„"""
        path_obj = Path(absolute_path)
        
        # å¦‚æœå·²ç»æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œç›´æ¥è¿”å›
        if not path_obj.is_absolute():
            return absolute_path
        
        # æŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•æ ‡è¯†
        path_str = str(path_obj)
        
        # å¤„ç†ä¸åŒçš„ç¡¬ç¼–ç æ¨¡å¼
        patterns = [
            '/Users/ameureka/Desktop/twitter-trend/',
            '/home/twitter-trend/',
            '/data2/twitter-trend/',
            '/Users/ameureka/Desktop/twitter-trend',
            '/home/twitter-trend',
            '/data2/twitter-trend'
        ]
        
        for pattern in patterns:
            if pattern in path_str:
                relative_part = path_str.replace(pattern, '').lstrip('/')
                if relative_part:
                    return relative_part
                else:
                    return '.'
        
        # å¦‚æœæ— æ³•è½¬æ¢ï¼Œå°è¯•æŸ¥æ‰¾ 'project' ç›®å½•
        parts = path_obj.parts
        try:
            project_index = parts.index('project')
            relative_parts = parts[project_index:]
            return str(Path(*relative_parts))
        except ValueError:
            pass
        
        # æœ€åå°è¯•ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•è®¡ç®—
        try:
            relative_path = path_obj.relative_to(self.project_root)
            return str(relative_path)
        except ValueError:
            logger.warning(f"æ— æ³•è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„: {absolute_path}")
            return absolute_path
    
    def _update_config_file(self):
        """æ›´æ–°é…ç½®æ–‡ä»¶"""
        config_path = self.project_root / 'config' / 'enhanced_config.yaml'
        
        # è¯»å–é…ç½®æ–‡ä»¶å†…å®¹
        with open(config_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æ›´æ–° project_base_path
        if 'project_base_path: .' in content:
            content = content.replace('project_base_path: .', 'project_base_path: ./project')
            
            # åˆ›å»ºå¤‡ä»½
            backup_path = config_path.with_suffix('.yaml.backup')
            shutil.copy2(config_path, backup_path)
            
            # å†™å…¥æ›´æ–°åçš„å†…å®¹
            with open(config_path, 'w', encoding='utf-8') as f:
                f.write(content)
            
            logger.info(f"é…ç½®æ–‡ä»¶å·²æ›´æ–°: {config_path}")
            logger.info(f"é…ç½®æ–‡ä»¶å¤‡ä»½: {backup_path}")
    
    def verify_paths(self) -> Dict[str, Any]:
        """éªŒè¯ä¿®å¤åçš„è·¯å¾„"""
        logger.info("éªŒè¯ä¿®å¤åçš„è·¯å¾„...")
        
        verification = {
            'total_paths_checked': 0,
            'valid_paths': 0,
            'invalid_paths': 0,
            'missing_files': [],
            'success_rate': 0.0,
            'recommendations': []
        }
        
        # è·å–å½“å‰çš„åŸºç¡€è·¯å¾„
        project_base_path = self.config.get('project_base_path', './project')
        if not Path(project_base_path).is_absolute():
            base_path = self.project_root / project_base_path
        else:
            base_path = Path(project_base_path)
        
        logger.info(f"ä½¿ç”¨åŸºç¡€è·¯å¾„: {base_path}")
        
        # éªŒè¯å‘å¸ƒä»»åŠ¡ä¸­çš„åª’ä½“è·¯å¾„
        tasks = self.session.query(PublishingTask).limit(20).all()  # éªŒè¯å‰20ä¸ªä»»åŠ¡
        
        for task in tasks:
            if task.media_path:
                verification['total_paths_checked'] += 1
                
                # æ„å»ºå®Œæ•´è·¯å¾„
                if Path(task.media_path).is_absolute():
                    full_path = Path(task.media_path)
                else:
                    full_path = base_path / task.media_path
                
                if full_path.exists():
                    verification['valid_paths'] += 1
                else:
                    verification['invalid_paths'] += 1
                    verification['missing_files'].append({
                        'task_id': task.id,
                        'media_path': task.media_path,
                        'full_path': str(full_path)
                    })
        
        # è®¡ç®—æˆåŠŸç‡
        if verification['total_paths_checked'] > 0:
            verification['success_rate'] = (verification['valid_paths'] / verification['total_paths_checked']) * 100
        
        # ç”Ÿæˆå»ºè®®
        if verification['invalid_paths'] > 0:
            verification['recommendations'].append(
                f"å‘ç° {verification['invalid_paths']} ä¸ªæ— æ•ˆè·¯å¾„ï¼Œå»ºè®®æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨æˆ–è·¯å¾„é…ç½®æ˜¯å¦æ­£ç¡®"
            )
        
        if verification['success_rate'] < 80:
            verification['recommendations'].append(
                "è·¯å¾„éªŒè¯æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥ project_base_path é…ç½®å’Œæ–‡ä»¶ç»“æ„"
            )
        
        return verification
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.session:
            self.session.close()

def print_analysis_report(analysis: Dict[str, Any]):
    """æ‰“å°åˆ†ææŠ¥å‘Š"""
    print("\n" + "="*60)
    print("è·¯å¾„é—®é¢˜åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    # å‘å¸ƒä»»åŠ¡åˆ†æ
    pt = analysis['publishing_tasks']
    print(f"\nğŸ“‹ å‘å¸ƒä»»åŠ¡ (publishing_tasks):")
    print(f"  æ€»æ•°: {pt['total']}")
    print(f"  ç»å¯¹è·¯å¾„: {pt['absolute_paths']}")
    print(f"  ç›¸å¯¹è·¯å¾„: {pt['relative_paths']}")
    print(f"  æ— æ•ˆè·¯å¾„: {pt['invalid_paths']}")
    
    if pt['hardcoded_patterns']:
        print(f"  ç¡¬ç¼–ç æ¨¡å¼:")
        for pattern, count in pt['hardcoded_patterns'].items():
            print(f"    {pattern}: {count}")
    
    # å†…å®¹æºåˆ†æ
    cs = analysis['content_sources']
    print(f"\nğŸ“ å†…å®¹æº (content_sources):")
    print(f"  æ€»æ•°: {cs['total']}")
    print(f"  ç»å¯¹è·¯å¾„: {cs['absolute_paths']}")
    print(f"  ç›¸å¯¹è·¯å¾„: {cs['relative_paths']}")
    print(f"  æ— æ•ˆè·¯å¾„: {cs['invalid_paths']}")
    
    if cs['hardcoded_patterns']:
        print(f"  ç¡¬ç¼–ç æ¨¡å¼:")
        for pattern, count in cs['hardcoded_patterns'].items():
            print(f"    {pattern}: {count}")
    
    # é…ç½®é—®é¢˜
    if analysis['config_issues']:
        print(f"\nâš ï¸  é…ç½®é—®é¢˜:")
        for issue in analysis['config_issues']:
            print(f"  - {issue}")
    
    # å»ºè®®
    if analysis['recommendations']:
        print(f"\nğŸ’¡ ä¿®å¤å»ºè®®:")
        for i, rec in enumerate(analysis['recommendations'], 1):
            print(f"  {i}. {rec}")
    
    # æ ·æœ¬è·¯å¾„
    if pt['samples']:
        print(f"\nğŸ“ æ ·æœ¬è·¯å¾„ (å‘å¸ƒä»»åŠ¡):")
        for sample in pt['samples'][:5]:
            print(f"  {sample['context']['field']} (ID:{sample['context']['id']}): {sample['path']}")

def print_fix_report(result: Dict[str, Any]):
    """æ‰“å°ä¿®å¤æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("è·¯å¾„ä¿®å¤æŠ¥å‘Š")
    print("="*60)
    
    print(f"\nâœ… ä¿®å¤ç»“æœ:")
    print(f"  å‘å¸ƒä»»åŠ¡ä¿®å¤æ•°é‡: {result['publishing_tasks_fixed']}")
    print(f"  å†…å®¹æºä¿®å¤æ•°é‡: {result['content_sources_fixed']}")
    print(f"  é…ç½®æ–‡ä»¶å·²æ›´æ–°: {'æ˜¯' if result['config_updated'] else 'å¦'}")
    print(f"  æ•°æ®åº“å¤‡ä»½å·²åˆ›å»º: {'æ˜¯' if result['backup_created'] else 'å¦'}")
    
    if result['errors']:
        print(f"\nâŒ é”™è¯¯:")
        for error in result['errors']:
            print(f"  - {error}")

def print_verification_report(verification: Dict[str, Any]):
    """æ‰“å°éªŒè¯æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("è·¯å¾„éªŒè¯æŠ¥å‘Š")
    print("="*60)
    
    print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
    print(f"  æ£€æŸ¥è·¯å¾„æ€»æ•°: {verification['total_paths_checked']}")
    print(f"  æœ‰æ•ˆè·¯å¾„: {verification['valid_paths']}")
    print(f"  æ— æ•ˆè·¯å¾„: {verification['invalid_paths']}")
    print(f"  æˆåŠŸç‡: {verification['success_rate']:.1f}%")
    
    if verification['missing_files']:
        print(f"\nâŒ ç¼ºå¤±æ–‡ä»¶ (å‰5ä¸ª):")
        for missing in verification['missing_files'][:5]:
            print(f"  ä»»åŠ¡ {missing['task_id']}: {missing['full_path']}")
    
    if verification['recommendations']:
        print(f"\nğŸ’¡ å»ºè®®:")
        for rec in verification['recommendations']:
            print(f"  - {rec}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç»¼åˆè·¯å¾„ä¿®å¤è„šæœ¬')
    parser.add_argument('--analyze', action='store_true', help='åˆ†æè·¯å¾„é—®é¢˜')
    parser.add_argument('--fix', action='store_true', help='ä¿®å¤ç¡¬ç¼–ç è·¯å¾„')
    parser.add_argument('--verify', action='store_true', help='éªŒè¯ä¿®å¤åçš„è·¯å¾„')
    parser.add_argument('--dry-run', action='store_true', default=True, help='é¢„è§ˆæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰')
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ“ä½œï¼Œé»˜è®¤æ‰§è¡Œåˆ†æ
    if not any([args.analyze, args.fix, args.verify]):
        args.analyze = True
    
    fixer = ComprehensivePathFixer()
    
    try:
        if args.analyze:
            analysis = fixer.analyze_path_issues()
            print_analysis_report(analysis)
        
        if args.fix:
            dry_run = args.dry_run and not args.fix
            result = fixer.fix_hardcoded_paths(dry_run=dry_run)
            print_fix_report(result)
        
        if args.verify:
            verification = fixer.verify_paths()
            print_verification_report(verification)
        
        print("\n" + "="*60)
        print("ä½¿ç”¨è¯´æ˜")
        print("="*60)
        print("1. åˆ†æé—®é¢˜: python scripts/fix_hardcoded_paths_comprehensive.py --analyze")
        print("2. é¢„è§ˆä¿®å¤: python scripts/fix_hardcoded_paths_comprehensive.py --fix --dry-run")
        print("3. æ‰§è¡Œä¿®å¤: python scripts/fix_hardcoded_paths_comprehensive.py --fix")
        print("4. éªŒè¯ç»“æœ: python scripts/fix_hardcoded_paths_comprehensive.py --verify")
        
    except Exception as e:
        logger.error(f"è„šæœ¬æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise
    finally:
        fixer.close()

if __name__ == '__main__':
    main()